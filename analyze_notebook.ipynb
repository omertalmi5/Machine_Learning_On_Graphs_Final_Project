{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8942367,"sourceType":"datasetVersion","datasetId":5380713},{"sourceId":9676491,"sourceType":"datasetVersion","datasetId":5914052}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-10-20T19:17:51.980348Z","iopub.execute_input":"2024-10-20T19:17:51.980650Z","iopub.status.idle":"2024-10-20T19:17:53.041753Z","shell.execute_reply.started":"2024-10-20T19:17:51.980611Z","shell.execute_reply":"2024-10-20T19:17:53.040662Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/model-molhiv/99.ckpt\n/kaggle/input/pytorch-geometric-packages/torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl\n/kaggle/input/pytorch-geometric-packages/torch_geometric-2.5.3-py3-none-any.whl\n/kaggle/input/pytorch-geometric-packages/torch_cluster-1.6.3-cp310-cp310-linux_x86_64.whl\n/kaggle/input/pytorch-geometric-packages/torch_spline_conv-1.2.2-cp310-cp310-linux_x86_64.whl\n/kaggle/input/pytorch-geometric-packages/torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/rampasek/GraphGPS.git","metadata":{"execution":{"iopub.status.busy":"2024-10-20T16:45:04.413044Z","iopub.execute_input":"2024-10-20T16:45:04.413471Z","iopub.status.idle":"2024-10-20T16:45:07.167139Z","shell.execute_reply.started":"2024-10-20T16:45:04.413435Z","shell.execute_reply":"2024-10-20T16:45:07.165520Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'GraphGPS'...\nremote: Enumerating objects: 526, done.\u001b[K\nremote: Counting objects: 100% (350/350), done.\u001b[K\nremote: Compressing objects: 100% (109/109), done.\u001b[K\nremote: Total 526 (delta 271), reused 241 (delta 241), pack-reused 176 (from 1)\u001b[K\nReceiving objects: 100% (526/526), 12.93 MiB | 26.26 MiB/s, done.\nResolving deltas: 100% (337/337), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/GraphGPS","metadata":{"execution":{"iopub.status.busy":"2024-10-17T22:01:27.457629Z","iopub.execute_input":"2024-10-17T22:01:27.457964Z","iopub.status.idle":"2024-10-17T22:01:27.464878Z","shell.execute_reply.started":"2024-10-17T22:01:27.457928Z","shell.execute_reply":"2024-10-17T22:01:27.463950Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/GraphGPS\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##  installation for slurm if needed using conda ","metadata":{}},{"cell_type":"code","source":"# # for slurm if needed using conda \n# conda create -n graphgps python=3.10\n# conda activate graphgps\n\n# conda install pytorch=1.13 torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n# conda install pyg=2.2 -c pyg -c conda-forge\n# pip install pyg-lib -f https://data.pyg.org/whl/torch-1.13.0+cu117.html\n\n# # RDKit is required for OGB-LSC PCQM4Mv2 and datasets derived from it.  \n# conda install openbabel fsspec rdkit -c conda-forge\n\n# pip install pytorch-lightning yacs torchmetrics\n# pip install performer-pytorch\n# pip install tensorboardX\n# pip install ogb\n# pip install wandb\n\n# conda clean --all","metadata":{"execution":{"iopub.status.busy":"2024-10-20T19:17:53.043752Z","iopub.execute_input":"2024-10-20T19:17:53.044280Z","iopub.status.idle":"2024-10-20T19:17:53.051753Z","shell.execute_reply.started":"2024-10-20T19:17:53.044234Z","shell.execute_reply":"2024-10-20T19:17:53.050697Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# installation for kaggle","metadata":{}},{"cell_type":"code","source":"# Check existing environment\nimport sys\nprint(\"Python version:\", sys.version)\nimport torch\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\n\n!pip uninstall -y torch torchvision torchaudio\n!pip install torch==2.4.0+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n!pip install torch==2.4.0+cu118 torch_geometric\n\n# Optional dependencies:\n#17.10 23:24 - !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n!pip install torch==2.4.0+cu118  pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu118.html\n# Install other required packages\n!pip install torch==2.4.0+cu118  pytorch-lightning yacs torchmetrics\n!pip install torch==2.4.0+cu118  performer-pytorch\n!pip install tensorboardX\n!pip install ogb\n!pip install wandb\n\n# Attempt to install RDKit (may have limitations)\n!pip install rdkit-pypi\n# !pip install openbabel fsspec\n!pip install  fsspec\n\n# Clean up pip cache (optional)\n!pip cache purge","metadata":{"execution":{"iopub.status.busy":"2024-10-20T19:17:53.053035Z","iopub.execute_input":"2024-10-20T19:17:53.057593Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\nPyTorch version: 2.4.0\nCUDA available: True\nCUDA version: 12.3\nFound existing installation: torch 2.4.0\nUninstalling torch-2.4.0:\n  Successfully uninstalled torch-2.4.0\nFound existing installation: torchvision 0.19.0\nUninstalling torchvision-0.19.0:\n  Successfully uninstalled torchvision-0.19.0\nFound existing installation: torchaudio 2.4.0\nUninstalling torchaudio-2.4.0:\n  Successfully uninstalled torchaudio-2.4.0\nLooking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch==2.4.0+cu118\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.4.0%2Bcu118-cp310-cp310-linux_x86_64.whl (857.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.7/857.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.20.5 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.20.5-py3-none-manylinux2014_x86_64.whl (142.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.9/142.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0+cu118)\n  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.1%2Bcu118-cp310-cp310-linux_x86_64.whl (6.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.19.0%2Bcu118-cp310-cp310-linux_x86_64.whl (6.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\nINFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.1%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.4.0%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0+cu118) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0+cu118) (1.3.0)\nInstalling collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\nSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.20.5 nvidia-nvtx-cu11-11.8.86 torch-2.4.0+cu118 torchaudio-2.4.0+cu118 torchvision-0.19.0+cu118 triton-3.0.0\nRequirement already satisfied: torch==2.4.0+cu118 in /opt/conda/lib/python3.10/site-packages (2.4.0+cu118)\nCollecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.86)\nRequirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.9.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0+cu118) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0+cu118) (1.3.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\nLooking in links: https://data.pyg.org/whl/torch-2.4.0+cu118.html\nRequirement already satisfied: torch==2.4.0+cu118 in /opt/conda/lib/python3.10/site-packages (2.4.0+cu118)\nCollecting pyg_lib\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu118/pyg_lib-0.4.0%2Bpt24cu118-cp310-cp310-linux_x86_64.whl (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hCollecting torch_scatter\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu118/torch_scatter-2.1.2%2Bpt24cu118-cp310-cp310-linux_x86_64.whl (10.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting torch_sparse\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu118/torch_sparse-0.6.18%2Bpt24cu118-cp310-cp310-linux_x86_64.whl (5.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torch_cluster\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu118/torch_cluster-1.6.3%2Bpt24cu118-cp310-cp310-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting torch_spline_conv\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt24cu118-cp310-cp310-linux_x86_64.whl (942 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m942.0/942.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.86)\nRequirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.0.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_sparse) (1.14.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0+cu118) (2.1.5)\nRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy->torch_sparse) (1.26.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0+cu118) (1.3.0)\nInstalling collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\nSuccessfully installed pyg_lib-0.4.0+pt24cu118 torch_cluster-1.6.3+pt24cu118 torch_scatter-2.1.2+pt24cu118 torch_sparse-0.6.18+pt24cu118 torch_spline_conv-1.2.2+pt24cu118\nRequirement already satisfied: torch==2.4.0+cu118 in /opt/conda/lib/python3.10/site-packages (2.4.0+cu118)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (2.4.0)\nCollecting yacs\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.86)\nRequirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.0.0)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.66.4)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (6.0.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (21.3)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (0.11.7)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (70.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch-lightning) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0+cu118) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0+cu118) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nInstalling collected packages: yacs\nSuccessfully installed yacs-0.1.8\nRequirement already satisfied: torch==2.4.0+cu118 in /opt/conda/lib/python3.10/site-packages (2.4.0+cu118)\nCollecting performer-pytorch\n  Downloading performer_pytorch-1.1.4-py3-none-any.whl.metadata (763 bytes)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (11.8.86)\nRequirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0+cu118) (3.0.0)\nCollecting einops>=0.3 (from performer-pytorch)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting local-attention>=1.1.1 (from performer-pytorch)\n  Downloading local_attention-1.9.15-py3-none-any.whl.metadata (683 bytes)\nCollecting axial-positional-embedding>=0.1.0 (from performer-pytorch)\n  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0+cu118) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0+cu118) (1.3.0)\nDownloading performer_pytorch-1.1.4-py3-none-any.whl (13 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading local_attention-1.9.15-py3-none-any.whl (9.0 kB)\nBuilding wheels for collected packages: axial-positional-embedding\n  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2889 sha256=9c2884fd13bf1820efdbd7e2ab65c0e00f10a0d2728854ebdd06862e30939b82\n  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\nSuccessfully built axial-positional-embedding\nInstalling collected packages: einops, local-attention, axial-positional-embedding, performer-pytorch\nSuccessfully installed axial-positional-embedding-0.2.1 einops-0.8.0 local-attention-1.9.15 performer-pytorch-1.1.4\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (2.6.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (21.3)\nRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (3.20.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorboardX) (3.1.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\n# Check existing environment\nimport sys\nprint(\"Python version:\", sys.version)\nimport torch\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\n\n!pip uninstall -y torch torchvision torchaudio\n!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n\n!pip install torch_geometric\n\n# Optional dependencies:\n!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu124.html \n# Install other required packages\n!pip install pytorch-lightning yacs torchmetrics\n!pip install performer-pytorch\n!pip install tensorboardX\n!pip install ogb\n!pip install wandb\n\n# Attempt to install RDKit (may have limitations)\n!pip install rdkit-pypi\n# !pip install openbabel fsspec\n!pip install  fsspec\n\n# Clean up pip cache (optional)\n!pip cache purge\n0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n\n!pip install torch_geometric\n\n# Optional dependencies:\n#17.10 23:24 - !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n!pip install 2.0.1+cu118  0.1+cu118.html\n# Install other required packages\n!pip install 2.0.1+cu118  2.0.1+cu118  \n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nprint(\"Python version:\", sys.version)\nimport torch\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch_geometric\nprint(\"PyG version:\", torch_geometric.__version__)\n\nimport pytorch_lightning\nprint(\"PyTorch Lightning version:\", pytorch_lightning.__version__)\n\nimport rdkit\nprint(\"RDKit version:\", rdkit.__version__)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(\"Torch version:\", torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T16:50:12.964907Z","iopub.execute_input":"2024-10-20T16:50:12.965500Z","iopub.status.idle":"2024-10-20T16:50:12.969795Z","shell.execute_reply.started":"2024-10-20T16:50:12.965458Z","shell.execute_reply":"2024-10-20T16:50:12.968883Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Running their model- GPSConv","metadata":{}},{"cell_type":"code","source":"# %cd /kaggle/working/GraphGPS\n# !WANDB_MODE=disabled python main.py --cfg configs/GPS/ogbg-molhiv-GPS.yaml","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:09:56.675569Z","iopub.execute_input":"2024-10-20T20:09:56.676048Z","iopub.status.idle":"2024-10-20T20:09:56.680819Z","shell.execute_reply.started":"2024-10-20T20:09:56.676009Z","shell.execute_reply":"2024-10-20T20:09:56.679740Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Our Model - WeightedGraphGPS","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!rm -rf WeightedGraphGPS","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:35:37.653536Z","iopub.execute_input":"2024-10-20T20:35:37.654417Z","iopub.status.idle":"2024-10-20T20:35:38.730677Z","shell.execute_reply.started":"2024-10-20T20:35:37.654369Z","shell.execute_reply":"2024-10-20T20:35:38.729342Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"#clone for our project\n%cd /kaggle/working\n!git clone --branch print_inference https://github.com/omertalmi5/WeightedGraphGPS.git","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:35:39.988035Z","iopub.execute_input":"2024-10-20T20:35:39.988480Z","iopub.status.idle":"2024-10-20T20:35:41.848085Z","shell.execute_reply.started":"2024-10-20T20:35:39.988439Z","shell.execute_reply":"2024-10-20T20:35:41.846885Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'WeightedGraphGPS'...\nremote: Enumerating objects: 1123, done.\u001b[K\nremote: Counting objects: 100% (878/878), done.\u001b[K\nremote: Compressing objects: 100% (453/453), done.\u001b[K\nremote: Total 1123 (delta 706), reused 508 (delta 425), pack-reused 245 (from 1)\u001b[K\nReceiving objects: 100% (1123/1123), 13.11 MiB | 37.60 MiB/s, done.\nResolving deltas: 100% (795/795), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-10-20T19:50:22.696680Z","iopub.execute_input":"2024-10-20T19:50:22.697038Z","iopub.status.idle":"2024-10-20T19:50:34.268505Z","shell.execute_reply.started":"2024-10-20T19:50:22.697002Z","shell.execute_reply":"2024-10-20T19:50:34.267464Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.3)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.15.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"!wandb login 5f1c292fde40243d281618fe88d712346cfe90d7","metadata":{"execution":{"iopub.status.busy":"2024-10-20T19:50:34.271661Z","iopub.execute_input":"2024-10-20T19:50:34.272197Z","iopub.status.idle":"2024-10-20T19:50:37.182697Z","shell.execute_reply.started":"2024-10-20T19:50:34.272147Z","shell.execute_reply":"2024-10-20T19:50:37.181570Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n# Set the environment variable\nos.environ['USERNAME'] = \"omertalmi-tel-aviv-university\"","metadata":{"execution":{"iopub.status.busy":"2024-10-20T19:50:37.184024Z","iopub.execute_input":"2024-10-20T19:50:37.184362Z","iopub.status.idle":"2024-10-20T19:50:37.189302Z","shell.execute_reply.started":"2024-10-20T19:50:37.184329Z","shell.execute_reply":"2024-10-20T19:50:37.188465Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/WeightedGraphGPS\nos.environ['BRANCH_NAME']= os.popen('git rev-parse --abbrev-ref HEAD').read().strip()\n!WANDB_MODE=online python main.py --cfg configs/GPS/ogbg-molhiv-GPS.yaml train.mode inference wandb.use True wandb.entity $USERNAME wandb.name $BRANCH_NAME train.radius /kaggle/input/model-molhiv/99.ckpt","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:35:43.140128Z","iopub.execute_input":"2024-10-20T20:35:43.140985Z","iopub.status.idle":"2024-10-20T20:37:18.211873Z","shell.execute_reply.started":"2024-10-20T20:35:43.140940Z","shell.execute_reply":"2024-10-20T20:37:18.210690Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"/kaggle/working/WeightedGraphGPS\n[*] Run ID 0: seed=0, split_index=0\n    Starting now: 2024-10-20 20:35:59.941413\nDownloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\nDownloaded 0.00 GB: 100%|█████████████████████████| 3/3 [00:00<00:00,  3.36it/s]\nExtracting ./datasets/hiv.zip\nProcessing...\nLoading necessary files...\nThis might take a while.\nProcessing graphs...\n100%|██████████████████████████████████| 41127/41127 [00:00<00:00, 67493.84it/s]\nConverting graphs into PyG objects...\n100%|██████████████████████████████████| 41127/41127 [00:02<00:00, 18744.00it/s]\nSaving...\nDone!\n/opt/conda/lib/python3.10/site-packages/ogb/graphproppred/dataset_pyg.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.data, self.slices = torch.load(self.processed_paths[0])\n[*] Loaded dataset 'ogbg-molhiv' from 'OGB':\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n  Data(edge_index=[2, 2259376], edge_attr=[2259376, 3], x=[1049163, 9], y=[41127, 1], num_nodes=1049163)\n  undirected: True\n  num graphs: 41127\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n  avg num_nodes/graph: 25\n  num node features: 9\n  num edge features: 3\n  num tasks: 1\n  num classes: 2\nPrecomputing Positional Encoding statistics: ['LapPE'] for all graphs...\n  ...estimated to be undirected: True\n100%|████████████████████████████████████| 41127/41127 [00:50<00:00, 816.07it/s]\nDone! Took 00:00:51.91\nGraphGymModule(\n  (model): GPSModel(\n    (encoder): FeatureEncoder(\n      (node_encoder): Concat2NodeEncoder(\n        (encoder1): AtomEncoder(\n          (atom_embedding_list): ModuleList(\n            (0): Embedding(119, 56)\n            (1): Embedding(5, 56)\n            (2-3): 2 x Embedding(12, 56)\n            (4): Embedding(10, 56)\n            (5-6): 2 x Embedding(6, 56)\n            (7-8): 2 x Embedding(2, 56)\n          )\n        )\n        (encoder2): LapPENodeEncoder(\n          (linear_A): Linear(in_features=2, out_features=16, bias=True)\n          (pe_encoder): Sequential(\n            (0): ReLU()\n            (1): Linear(in_features=16, out_features=8, bias=True)\n            (2): ReLU()\n          )\n        )\n      )\n      (edge_encoder): BondEncoder(\n        (bond_embedding_list): ModuleList(\n          (0): Embedding(5, 64)\n          (1): Embedding(6, 64)\n          (2): Embedding(2, 64)\n        )\n      )\n    )\n    (layers): Sequential(\n      (0): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_conv1): GCNConv(64, 32)\n        (gating_relu): ReLU()\n        (gating_conv2): GCNConv(32, 2)\n        (gating_softmax): Softmax(dim=1)\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (1): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_conv1): GCNConv(64, 32)\n        (gating_relu): ReLU()\n        (gating_conv2): GCNConv(32, 2)\n        (gating_softmax): Softmax(dim=1)\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (2): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_conv1): GCNConv(64, 32)\n        (gating_relu): ReLU()\n        (gating_conv2): GCNConv(32, 2)\n        (gating_softmax): Softmax(dim=1)\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (3): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_conv1): GCNConv(64, 32)\n        (gating_relu): ReLU()\n        (gating_conv2): GCNConv(32, 2)\n        (gating_softmax): Softmax(dim=1)\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (4): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_conv1): GCNConv(64, 32)\n        (gating_relu): ReLU()\n        (gating_conv2): GCNConv(32, 2)\n        (gating_softmax): Softmax(dim=1)\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (5): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_conv1): GCNConv(64, 32)\n        (gating_relu): ReLU()\n        (gating_conv2): GCNConv(32, 2)\n        (gating_softmax): Softmax(dim=1)\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (6): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_conv1): GCNConv(64, 32)\n        (gating_relu): ReLU()\n        (gating_conv2): GCNConv(32, 2)\n        (gating_softmax): Softmax(dim=1)\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (7): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_conv1): GCNConv(64, 32)\n        (gating_relu): ReLU()\n        (gating_conv2): GCNConv(32, 2)\n        (gating_softmax): Softmax(dim=1)\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (8): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_conv1): GCNConv(64, 32)\n        (gating_relu): ReLU()\n        (gating_conv2): GCNConv(32, 2)\n        (gating_softmax): Softmax(dim=1)\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (9): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_conv1): GCNConv(64, 32)\n        (gating_relu): ReLU()\n        (gating_conv2): GCNConv(32, 2)\n        (gating_softmax): Softmax(dim=1)\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n    )\n    (post_mp): SANGraphHead(\n      (FC_layers): ModuleList(\n        (0): Linear(in_features=64, out_features=32, bias=True)\n        (1): Linear(in_features=32, out_features=16, bias=True)\n        (2): Linear(in_features=16, out_features=1, bias=True)\n      )\n      (activation): ReLU()\n    )\n  )\n)\naccelerator: cuda\nbenchmark: False\nbn:\n  eps: 1e-05\n  mom: 0.1\ncfg_dest: config.yaml\ncustom_metrics: []\ndataset:\n  cache_load: False\n  cache_save: False\n  dir: ./datasets\n  edge_dim: 128\n  edge_encoder: True\n  edge_encoder_bn: False\n  edge_encoder_name: Bond\n  edge_encoder_num_types: 0\n  edge_message_ratio: 0.8\n  edge_negative_sampling_ratio: 1.0\n  edge_train_mode: all\n  encoder: True\n  encoder_bn: True\n  encoder_dim: 128\n  encoder_name: db\n  format: OGB\n  infer_link_label: None\n  label_column: none\n  label_table: none\n  location: local\n  name: ogbg-molhiv\n  node_encoder: True\n  node_encoder_bn: False\n  node_encoder_name: Atom+LapPE\n  node_encoder_num_types: 0\n  remove_feature: False\n  resample_disjoint: False\n  resample_negative: False\n  shuffle_split: True\n  slic_compactness: 10\n  split: [0.8, 0.1, 0.1]\n  split_dir: ./splits\n  split_index: 0\n  split_mode: standard\n  task: graph\n  task_type: classification\n  to_undirected: False\n  transductive: False\n  transform: none\n  tu_simple: True\ndevices: 1\nexample_arg: example\nexample_group:\n  example_arg: example\ngnn:\n  act: relu\n  agg: mean\n  att_final_linear: False\n  att_final_linear_bn: False\n  att_heads: 1\n  batchnorm: True\n  clear_feature: True\n  dim_edge: 64\n  dim_inner: 64\n  dropout: 0.0\n  head: san_graph\n  keep_edge: 0.5\n  l2norm: True\n  layer_type: generalconv\n  layers_mp: 2\n  layers_post_mp: 3\n  layers_pre_mp: 0\n  msg_direction: single\n  normalize_adj: False\n  residual: False\n  self_msg: concat\n  skip_every: 1\n  stage_type: stack\ngpu_mem: False\ngraphormer:\n  attention_dropout: 0.0\n  dropout: 0.0\n  embed_dim: 80\n  input_dropout: 0.0\n  mlp_dropout: 0.0\n  num_heads: 4\n  num_layers: 6\n  use_graph_token: True\ngt:\n  attn_dropout: 0.5\n  batch_norm: True\n  bigbird:\n    add_cross_attention: False\n    attention_type: block_sparse\n    block_size: 3\n    chunk_size_feed_forward: 0\n    hidden_act: relu\n    is_decoder: False\n    layer_norm_eps: 1e-06\n    max_position_embeddings: 128\n    num_random_blocks: 3\n    use_bias: False\n  dim_hidden: 64\n  dropout: 0.05\n  full_graph: True\n  gamma: 1e-05\n  layer_norm: False\n  layer_type: CustomGatedGCN+Transformer\n  layers: 10\n  n_heads: 4\n  pna_degrees: []\n  residual: True\nmem:\n  inplace: False\nmetric_agg: argmax\nmetric_best: auc\nmodel:\n  edge_decoding: dot\n  graph_pooling: mean\n  loss_fun: cross_entropy\n  match_upper: True\n  size_average: mean\n  thresh: 0.5\n  type: GPSModel\nname_tag: \nnum_threads: 6\nnum_workers: 0\noptim:\n  base_lr: 0.0001\n  batch_accumulation: 1\n  clip_grad_norm: True\n  clip_grad_norm_value: 1.0\n  lr_decay: 0.1\n  max_epoch: 2\n  min_lr: 0.0\n  momentum: 0.9\n  num_warmup_epochs: 5\n  optimizer: adamW\n  reduce_factor: 0.1\n  schedule_patience: 10\n  scheduler: cosine_with_warmup\n  steps: [30, 60, 90]\n  weight_decay: 1e-05\nout_dir: results/ogbg-molhiv-GPS\nposenc_ElstaticSE:\n  dim_pe: 16\n  enable: False\n  kernel:\n    times: []\n    times_func: range(10)\n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_EquivStableLapPE:\n  eigen:\n    eigvec_norm: L2\n    laplacian_norm: sym\n    max_freqs: 10\n  enable: False\n  raw_norm_type: none\nposenc_GraphormerBias:\n  dim_pe: 0\n  enable: False\n  node_degrees_only: False\n  num_in_degrees: None\n  num_out_degrees: None\n  num_spatial_types: None\nposenc_HKdiagSE:\n  dim_pe: 16\n  enable: False\n  kernel:\n    times: []\n    times_func: \n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_LapPE:\n  dim_pe: 8\n  eigen:\n    eigvec_norm: L2\n    laplacian_norm: none\n    max_freqs: 8\n  enable: True\n  layers: 2\n  model: DeepSet\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_RWSE:\n  dim_pe: 16\n  enable: False\n  kernel:\n    times: []\n    times_func: \n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_SignNet:\n  dim_pe: 16\n  eigen:\n    eigvec_norm: L2\n    laplacian_norm: sym\n    max_freqs: 10\n  enable: False\n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  phi_hidden_dim: 64\n  phi_out_dim: 4\n  post_layers: 0\n  raw_norm_type: none\npretrained:\n  dir: \n  freeze_main: False\n  reset_prediction_head: True\nprint: both\nround: 5\nrun_dir: results/ogbg-molhiv-GPS/0\nrun_id: 0\nrun_multiple_splits: []\nseed: 0\nshare:\n  dim_in: 9\n  dim_out: 2\n  num_splits: 3\ntensorboard_agg: True\ntensorboard_each_run: False\ntrain:\n  auto_resume: False\n  batch_size: 32\n  ckpt_best: False\n  ckpt_clean: True\n  ckpt_period: 100\n  enable_ckpt: True\n  epoch_resume: -1\n  eval_period: 1\n  iter_per_epoch: 32\n  mode: inference\n  neighbor_sizes: [20, 15, 10, 5]\n  node_per_graph: 32\n  radius: /kaggle/input/model-molhiv/99.ckpt\n  sample_node: False\n  sampler: full_batch\n  skip_train_eval: False\n  walk_length: 4\nval:\n  node_per_graph: 32\n  radius: extend\n  sample_node: False\n  sampler: full_batch\nview_emb: False\nwandb:\n  entity: omertalmi-tel-aviv-university\n  name: print_inference\n  project: molhiv\n  use: True\nNum parameters: 581405\nStarting inference...\nLoading checkpoint from /kaggle/input/model-molhiv/99.ckpt\n/kaggle/working/WeightedGraphGPS/graphgps/train/custom_train.py:247: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(cfg.train.radius, map_location=torch.device(cfg.accelerator))\na_mag mean: 0.6938058137893677\na_mag mean: 0.8514159321784973\na_mag mean: 0.5567368865013123\na_mag mean: 0.6543751358985901\na_mag mean: 0.6928808093070984\na_mag mean: 0.7773343920707703\na_mag mean: 0.5454766154289246\na_mag mean: 0.6687201857566833\na_mag mean: 0.46708348393440247\na_mag mean: 0.15195271372795105\na_mag mean: 0.7026256918907166\na_mag mean: 0.7596679925918579\na_mag mean: 0.664006233215332\na_mag mean: 0.7134369611740112\na_mag mean: 0.6054686903953552\na_mag mean: 0.739410936832428\na_mag mean: 0.6350144147872925\na_mag mean: 0.6246461272239685\na_mag mean: 0.4905775189399719\na_mag mean: 0.1969655603170395\na_mag mean: 0.7051418423652649\na_mag mean: 0.7677174806594849\na_mag mean: 0.6102077960968018\na_mag mean: 0.7126182913780212\na_mag mean: 0.7121437191963196\na_mag mean: 0.7144101858139038\na_mag mean: 0.5943165421485901\na_mag mean: 0.7077046632766724\na_mag mean: 0.46550989151000977\na_mag mean: 0.14496687054634094\na_mag mean: 0.7123037576675415\na_mag mean: 0.7719624042510986\na_mag mean: 0.6579591631889343\na_mag mean: 0.6810385584831238\na_mag mean: 0.6163879632949829\na_mag mean: 0.702305018901825\na_mag mean: 0.7105425596237183\na_mag mean: 0.6844454407691956\na_mag mean: 0.46365201473236084\na_mag mean: 0.13893525302410126\na_mag mean: 0.715887188911438\na_mag mean: 0.710761547088623\na_mag mean: 0.528265118598938\na_mag mean: 0.6146772503852844\na_mag mean: 0.7372037172317505\na_mag mean: 0.7355303764343262\na_mag mean: 0.645423173904419\na_mag mean: 0.6235367655754089\na_mag mean: 0.3303092420101166\na_mag mean: 0.13301511108875275\na_mag mean: 0.7084113359451294\na_mag mean: 0.8275047540664673\na_mag mean: 0.6478303074836731\na_mag mean: 0.6885024309158325\na_mag mean: 0.7285465598106384\na_mag mean: 0.752297043800354\na_mag mean: 0.547757089138031\na_mag mean: 0.6262058615684509\na_mag mean: 0.498791366815567\na_mag mean: 0.09563563764095306\na_mag mean: 0.7146821618080139\na_mag mean: 0.7162493467330933\na_mag mean: 0.5614320039749146\na_mag mean: 0.6713739633560181\na_mag mean: 0.7356935739517212\na_mag mean: 0.6473307609558105\na_mag mean: 0.6200519800186157\na_mag mean: 0.6227229833602905\na_mag mean: 0.2701793909072876\na_mag mean: 0.10685978084802628\na_mag mean: 0.7194514274597168\na_mag mean: 0.805661141872406\na_mag mean: 0.5524991750717163\na_mag mean: 0.6452040672302246\na_mag mean: 0.7120301127433777\na_mag mean: 0.7994737029075623\na_mag mean: 0.5642600059509277\na_mag mean: 0.6902483105659485\na_mag mean: 0.47569528222084045\na_mag mean: 0.07035152614116669\na_mag mean: 0.6970511674880981\na_mag mean: 0.795704185962677\na_mag mean: 0.6379318237304688\na_mag mean: 0.7068886160850525\na_mag mean: 0.6657052040100098\na_mag mean: 0.7739669680595398\na_mag mean: 0.5953155755996704\na_mag mean: 0.7121442556381226\na_mag mean: 0.43834421038627625\na_mag mean: 0.08331266790628433\na_mag mean: 0.7131004333496094\na_mag mean: 0.7901664972305298\na_mag mean: 0.6306456327438354\na_mag mean: 0.6726149320602417\na_mag mean: 0.6677457094192505\na_mag mean: 0.7882699370384216\na_mag mean: 0.49036622047424316\na_mag mean: 0.6883819103240967\na_mag mean: 0.4690704047679901\na_mag mean: 0.13526034355163574\na_mag mean: 0.7228557467460632\na_mag mean: 0.8001798987388611\na_mag mean: 0.6867773532867432\na_mag mean: 0.7216576337814331\na_mag mean: 0.675849974155426\na_mag mean: 0.8224803805351257\na_mag mean: 0.502630352973938\na_mag mean: 0.7775828242301941\na_mag mean: 0.457733154296875\na_mag mean: 0.07341258227825165\na_mag mean: 0.7144650816917419\na_mag mean: 0.7779272794723511\na_mag mean: 0.5265723466873169\na_mag mean: 0.6101568937301636\na_mag mean: 0.6610029935836792\na_mag mean: 0.7338548302650452\na_mag mean: 0.5840520262718201\na_mag mean: 0.6353729367256165\na_mag mean: 0.3765290081501007\na_mag mean: 0.14109329879283905\na_mag mean: 0.7142265439033508\na_mag mean: 0.7247930765151978\na_mag mean: 0.5913777947425842\na_mag mean: 0.6798174977302551\na_mag mean: 0.7020295858383179\na_mag mean: 0.6574636697769165\na_mag mean: 0.7104174494743347\na_mag mean: 0.6223199963569641\na_mag mean: 0.30168429017066956\na_mag mean: 0.10247880220413208\na_mag mean: 0.7042148113250732\na_mag mean: 0.7698546648025513\na_mag mean: 0.5203630328178406\na_mag mean: 0.6418915390968323\na_mag mean: 0.7124377489089966\na_mag mean: 0.6436963081359863\na_mag mean: 0.7092226147651672\na_mag mean: 0.5369970202445984\na_mag mean: 0.4087127149105072\na_mag mean: 0.2547297775745392\na_mag mean: 0.6934060454368591\na_mag mean: 0.7960638403892517\na_mag mean: 0.49056679010391235\na_mag mean: 0.6965628266334534\na_mag mean: 0.7709125280380249\na_mag mean: 0.6304723620414734\na_mag mean: 0.735758364200592\na_mag mean: 0.4687615931034088\na_mag mean: 0.5259112119674683\na_mag mean: 0.3370553255081177\na_mag mean: 0.7136046886444092\na_mag mean: 0.7756510972976685\na_mag mean: 0.6597648859024048\na_mag mean: 0.6938179731369019\na_mag mean: 0.6437297463417053\na_mag mean: 0.7544208765029907\na_mag mean: 0.5452964901924133\na_mag mean: 0.6540341973304749\na_mag mean: 0.4880174398422241\na_mag mean: 0.054188042879104614\na_mag mean: 0.7197750806808472\na_mag mean: 0.8113335371017456\na_mag mean: 0.65114426612854\na_mag mean: 0.6786825656890869\na_mag mean: 0.6591686606407166\na_mag mean: 0.7665882706642151\na_mag mean: 0.5709511041641235\na_mag mean: 0.745050847530365\na_mag mean: 0.45781221985816956\na_mag mean: 0.023276066407561302\na_mag mean: 0.7115404009819031\na_mag mean: 0.854746401309967\na_mag mean: 0.604063093662262\na_mag mean: 0.6210644841194153\na_mag mean: 0.5992560386657715\na_mag mean: 0.7558325529098511\na_mag mean: 0.49311187863349915\na_mag mean: 0.7092813849449158\na_mag mean: 0.4452495872974396\na_mag mean: 0.09883898496627808\na_mag mean: 0.7129071950912476\na_mag mean: 0.8123170137405396\na_mag mean: 0.6179683804512024\na_mag mean: 0.6700349450111389\na_mag mean: 0.7172959446907043\na_mag mean: 0.7229636311531067\na_mag mean: 0.5970900654792786\na_mag mean: 0.6294100284576416\na_mag mean: 0.33123165369033813\na_mag mean: 0.14097322523593903\na_mag mean: 0.7085176706314087\na_mag mean: 0.7719578146934509\na_mag mean: 0.5735914707183838\na_mag mean: 0.7070682644844055\na_mag mean: 0.7551670670509338\na_mag mean: 0.7156705856323242\na_mag mean: 0.5867635607719421\na_mag mean: 0.6536663174629211\na_mag mean: 0.508254885673523\na_mag mean: 0.15715709328651428\na_mag mean: 0.70881187915802\na_mag mean: 0.7897896766662598\na_mag mean: 0.6630112528800964\na_mag mean: 0.7354819774627686\na_mag mean: 0.6503055691719055\na_mag mean: 0.7678737640380859\na_mag mean: 0.5011295676231384\na_mag mean: 0.7107675671577454\na_mag mean: 0.47000831365585327\na_mag mean: 0.11686649173498154\na_mag mean: 0.7221152186393738\na_mag mean: 0.7757783532142639\na_mag mean: 0.5665297508239746\na_mag mean: 0.7191742658615112\na_mag mean: 0.6913264989852905\na_mag mean: 0.7080119848251343\na_mag mean: 0.5169456601142883\na_mag mean: 0.7205833196640015\na_mag mean: 0.585539698600769\na_mag mean: 0.1375715285539627\na_mag mean: 0.7331955432891846\na_mag mean: 0.7507257461547852\na_mag mean: 0.6769543886184692\na_mag mean: 0.7123262286186218\na_mag mean: 0.6340782046318054\na_mag mean: 0.7635485529899597\na_mag mean: 0.6327465176582336\na_mag mean: 0.6555092334747314\na_mag mean: 0.4179152846336365\na_mag mean: 0.12283588200807571\na_mag mean: 0.7020390033721924\na_mag mean: 0.7521771192550659\na_mag mean: 0.46720972657203674\na_mag mean: 0.6535821557044983\na_mag mean: 0.8008108139038086\na_mag mean: 0.6061709523200989\na_mag mean: 0.6840985417366028\na_mag mean: 0.4651710093021393\na_mag mean: 0.41865426301956177\na_mag mean: 0.23913557827472687\na_mag mean: 0.7238363027572632\na_mag mean: 0.7631387114524841\na_mag mean: 0.5479491353034973\na_mag mean: 0.6334168910980225\na_mag mean: 0.6902416944503784\na_mag mean: 0.6896552443504333\na_mag mean: 0.5188801884651184\na_mag mean: 0.6151336431503296\na_mag mean: 0.43785348534584045\na_mag mean: 0.19353874027729034\na_mag mean: 0.7190170884132385\na_mag mean: 0.7704029679298401\na_mag mean: 0.6831014156341553\na_mag mean: 0.7101896405220032\na_mag mean: 0.5988534092903137\na_mag mean: 0.7602246999740601\na_mag mean: 0.5528628826141357\na_mag mean: 0.6741005182266235\na_mag mean: 0.5171321034431458\na_mag mean: 0.13137106597423553\na_mag mean: 0.7131010890007019\na_mag mean: 0.7888076901435852\na_mag mean: 0.5716557502746582\na_mag mean: 0.6478999853134155\na_mag mean: 0.6807154417037964\na_mag mean: 0.7434271574020386\na_mag mean: 0.5407453179359436\na_mag mean: 0.6644777655601501\na_mag mean: 0.517650842666626\na_mag mean: 0.15588785707950592\na_mag mean: 0.7208751440048218\na_mag mean: 0.7824937105178833\na_mag mean: 0.5815615653991699\na_mag mean: 0.6760863065719604\na_mag mean: 0.7161058187484741\na_mag mean: 0.7032483816146851\na_mag mean: 0.5932469367980957\na_mag mean: 0.6270732283592224\na_mag mean: 0.3470366597175598\na_mag mean: 0.09722468256950378\na_mag mean: 0.7329091429710388\na_mag mean: 0.7556636929512024\na_mag mean: 0.6042858362197876\na_mag mean: 0.7229925990104675\na_mag mean: 0.7177395820617676\na_mag mean: 0.7355346083641052\na_mag mean: 0.6357664465904236\na_mag mean: 0.6400984525680542\na_mag mean: 0.4581241309642792\na_mag mean: 0.1491987407207489\na_mag mean: 0.7156198620796204\na_mag mean: 0.7264884114265442\na_mag mean: 0.4518427550792694\na_mag mean: 0.7000203132629395\na_mag mean: 0.805927574634552\na_mag mean: 0.6256284713745117\na_mag mean: 0.694233238697052\na_mag mean: 0.5138764977455139\na_mag mean: 0.474424809217453\na_mag mean: 0.28939518332481384\na_mag mean: 0.7170054316520691\na_mag mean: 0.7151651382446289\na_mag mean: 0.5807608366012573\na_mag mean: 0.7065110206604004\na_mag mean: 0.7412989735603333\na_mag mean: 0.6906797289848328\na_mag mean: 0.65704745054245\na_mag mean: 0.5926679372787476\na_mag mean: 0.3836379051208496\na_mag mean: 0.1814899444580078\na_mag mean: 0.716562032699585\na_mag mean: 0.755528450012207\na_mag mean: 0.6281391382217407\na_mag mean: 0.6795852780342102\na_mag mean: 0.6617264151573181\na_mag mean: 0.7723501324653625\na_mag mean: 0.5597701072692871\na_mag mean: 0.7085651755332947\na_mag mean: 0.46704238653182983\na_mag mean: 0.12499716877937317\na_mag mean: 0.7228018045425415\na_mag mean: 0.8012567758560181\na_mag mean: 0.5709836483001709\na_mag mean: 0.6723807454109192\na_mag mean: 0.7474477291107178\na_mag mean: 0.6569942831993103\na_mag mean: 0.6739360094070435\na_mag mean: 0.6373389959335327\na_mag mean: 0.2920357882976532\na_mag mean: 0.06596440076828003\na_mag mean: 0.7203614711761475\na_mag mean: 0.7721413969993591\na_mag mean: 0.6444076895713806\na_mag mean: 0.6947388052940369\na_mag mean: 0.6954556107521057\na_mag mean: 0.7448309063911438\na_mag mean: 0.562720537185669\na_mag mean: 0.6755070686340332\na_mag mean: 0.34247809648513794\na_mag mean: 0.07686904072761536\na_mag mean: 0.7129701972007751\na_mag mean: 0.7322601675987244\na_mag mean: 0.5611079335212708\na_mag mean: 0.6607608795166016\na_mag mean: 0.7235057353973389\na_mag mean: 0.6921011805534363\na_mag mean: 0.6226293444633484\na_mag mean: 0.558584451675415\na_mag mean: 0.33056166768074036\na_mag mean: 0.13991126418113708\na_mag mean: 0.7251052856445312\na_mag mean: 0.7830946445465088\na_mag mean: 0.6947469711303711\na_mag mean: 0.7329739928245544\na_mag mean: 0.6997975707054138\na_mag mean: 0.7405436635017395\na_mag mean: 0.6252464652061462\na_mag mean: 0.7109909653663635\na_mag mean: 0.45251914858818054\na_mag mean: 0.0668424740433693\na_mag mean: 0.7143954634666443\na_mag mean: 0.8394960761070251\na_mag mean: 0.6525788903236389\na_mag mean: 0.6815761923789978\na_mag mean: 0.5443409085273743\na_mag mean: 0.7298978567123413\na_mag mean: 0.6115110516548157\na_mag mean: 0.5912155508995056\na_mag mean: 0.43832927942276\na_mag mean: 0.19570831954479218\na_mag mean: 0.714756190776825\na_mag mean: 0.7795333862304688\na_mag mean: 0.6255422234535217\na_mag mean: 0.7115523815155029\na_mag mean: 0.6777552366256714\na_mag mean: 0.7305193543434143\na_mag mean: 0.49519991874694824\na_mag mean: 0.7099273800849915\na_mag mean: 0.5951336026191711\na_mag mean: 0.0416199192404747\na_mag mean: 0.6826576590538025\na_mag mean: 0.8062755465507507\na_mag mean: 0.5670462250709534\na_mag mean: 0.6607764363288879\na_mag mean: 0.6828340291976929\na_mag mean: 0.7222765684127808\na_mag mean: 0.5901255011558533\na_mag mean: 0.6238001585006714\na_mag mean: 0.40980514883995056\na_mag mean: 0.12497470527887344\na_mag mean: 0.7256487011909485\na_mag mean: 0.725030779838562\na_mag mean: 0.6370792388916016\na_mag mean: 0.6654996871948242\na_mag mean: 0.6804603934288025\na_mag mean: 0.7389929890632629\na_mag mean: 0.5954041481018066\na_mag mean: 0.6762701272964478\na_mag mean: 0.30495786666870117\na_mag mean: 0.09368292987346649\na_mag mean: 0.72244793176651\na_mag mean: 0.7933642268180847\na_mag mean: 0.5985720157623291\na_mag mean: 0.6623806357383728\na_mag mean: 0.7008680701255798\na_mag mean: 0.7111098766326904\na_mag mean: 0.6278282403945923\na_mag mean: 0.6155057549476624\na_mag mean: 0.3023878037929535\na_mag mean: 0.10073511302471161\na_mag mean: 0.7179391384124756\na_mag mean: 0.7668707370758057\na_mag mean: 0.6658349633216858\na_mag mean: 0.6058491468429565\na_mag mean: 0.6066710352897644\na_mag mean: 0.7097993493080139\na_mag mean: 0.6231606602668762\na_mag mean: 0.684807300567627\na_mag mean: 0.310344934463501\na_mag mean: 0.08768945187330246\na_mag mean: 0.7063120603561401\na_mag mean: 0.7460840940475464\na_mag mean: 0.5780487656593323\na_mag mean: 0.6445450782775879\na_mag mean: 0.6721165180206299\na_mag mean: 0.6971884965896606\na_mag mean: 0.6521535515785217\na_mag mean: 0.6473309993743896\na_mag mean: 0.39132463932037354\na_mag mean: 0.09824784845113754\na_mag mean: 0.6897818446159363\na_mag mean: 0.7754022479057312\na_mag mean: 0.5002584457397461\na_mag mean: 0.5689008831977844\na_mag mean: 0.6938552260398865\na_mag mean: 0.6638813614845276\na_mag mean: 0.6229859590530396\na_mag mean: 0.5611807703971863\na_mag mean: 0.3068988025188446\na_mag mean: 0.14813002943992615\na_mag mean: 0.7215740084648132\na_mag mean: 0.7265874147415161\na_mag mean: 0.5487685203552246\na_mag mean: 0.597673773765564\na_mag mean: 0.6829215288162231\na_mag mean: 0.7034741044044495\na_mag mean: 0.6880412697792053\na_mag mean: 0.6501123905181885\na_mag mean: 0.2958814799785614\na_mag mean: 0.11295638978481293\na_mag mean: 0.7153481841087341\na_mag mean: 0.78357994556427\na_mag mean: 0.5429383516311646\na_mag mean: 0.6043530106544495\na_mag mean: 0.6603814363479614\na_mag mean: 0.6967834830284119\na_mag mean: 0.5980939865112305\na_mag mean: 0.6526154279708862\na_mag mean: 0.30394986271858215\na_mag mean: 0.127803772687912\na_mag mean: 0.691281259059906\na_mag mean: 0.7523875832557678\na_mag mean: 0.4714202582836151\na_mag mean: 0.5437216758728027\na_mag mean: 0.7452390789985657\na_mag mean: 0.6272068619728088\na_mag mean: 0.6671649217605591\na_mag mean: 0.4964383542537689\na_mag mean: 0.32798317074775696\na_mag mean: 0.3532005548477173\na_mag mean: 0.7169181108474731\na_mag mean: 0.7564845681190491\na_mag mean: 0.5558843612670898\na_mag mean: 0.6644179821014404\na_mag mean: 0.7366483807563782\na_mag mean: 0.6941496133804321\na_mag mean: 0.612371027469635\na_mag mean: 0.6462570428848267\na_mag mean: 0.31876906752586365\na_mag mean: 0.07710077613592148\na_mag mean: 0.7131485342979431\na_mag mean: 0.7909325957298279\na_mag mean: 0.5609731674194336\na_mag mean: 0.6377040147781372\na_mag mean: 0.6717370748519897\na_mag mean: 0.6968940496444702\na_mag mean: 0.6548974514007568\na_mag mean: 0.6738680005073547\na_mag mean: 0.26214107871055603\na_mag mean: 0.07771292328834534\na_mag mean: 0.6991668939590454\na_mag mean: 0.8144719004631042\na_mag mean: 0.6278197169303894\na_mag mean: 0.7053623199462891\na_mag mean: 0.6915642023086548\na_mag mean: 0.7978910207748413\na_mag mean: 0.571240246295929\na_mag mean: 0.674396276473999\na_mag mean: 0.4090563952922821\na_mag mean: 0.17325693368911743\na_mag mean: 0.7157971262931824\na_mag mean: 0.8437158465385437\na_mag mean: 0.5833482146263123\na_mag mean: 0.6762560606002808\na_mag mean: 0.7268766760826111\na_mag mean: 0.7662969827651978\na_mag mean: 0.6011888980865479\na_mag mean: 0.6476415991783142\na_mag mean: 0.2916262745857239\na_mag mean: 0.02315564639866352\na_mag mean: 0.7132089138031006\na_mag mean: 0.7939415574073792\na_mag mean: 0.6080830097198486\na_mag mean: 0.704892098903656\na_mag mean: 0.718304455280304\na_mag mean: 0.6680689454078674\na_mag mean: 0.6369568109512329\na_mag mean: 0.6500550508499146\na_mag mean: 0.3492487668991089\na_mag mean: 0.12921039760112762\na_mag mean: 0.7383846044540405\na_mag mean: 0.658785879611969\na_mag mean: 0.7033481001853943\na_mag mean: 0.6736539006233215\na_mag mean: 0.611666202545166\na_mag mean: 0.7459092140197754\na_mag mean: 0.6240503191947937\na_mag mean: 0.6106845140457153\na_mag mean: 0.3045579791069031\na_mag mean: 0.1044970229268074\na_mag mean: 0.7125805020332336\na_mag mean: 0.7572907209396362\na_mag mean: 0.5031671524047852\na_mag mean: 0.5842075347900391\na_mag mean: 0.7210297584533691\na_mag mean: 0.7269902229309082\na_mag mean: 0.5175354480743408\na_mag mean: 0.5915046334266663\na_mag mean: 0.36490878462791443\na_mag mean: 0.16037823259830475\na_mag mean: 0.704865038394928\na_mag mean: 0.8237151503562927\na_mag mean: 0.5206584930419922\na_mag mean: 0.6498401761054993\na_mag mean: 0.6946371793746948\na_mag mean: 0.8025306463241577\na_mag mean: 0.44695159792900085\na_mag mean: 0.647148072719574\na_mag mean: 0.4765526354312897\na_mag mean: 0.10204507410526276\na_mag mean: 0.7140722274780273\na_mag mean: 0.8395159244537354\na_mag mean: 0.6557906270027161\na_mag mean: 0.7229859828948975\na_mag mean: 0.6812391877174377\na_mag mean: 0.7715448141098022\na_mag mean: 0.4629281461238861\na_mag mean: 0.7185583114624023\na_mag mean: 0.44199323654174805\na_mag mean: 0.08148019760847092\na_mag mean: 0.7093456983566284\na_mag mean: 0.7823066711425781\na_mag mean: 0.6023020148277283\na_mag mean: 0.6711212992668152\na_mag mean: 0.6497581005096436\na_mag mean: 0.7222044467926025\na_mag mean: 0.5921284556388855\na_mag mean: 0.6814930438995361\na_mag mean: 0.36039748787879944\na_mag mean: 0.06968282163143158\na_mag mean: 0.7148638367652893\na_mag mean: 0.7233788967132568\na_mag mean: 0.5541735291481018\na_mag mean: 0.7044042348861694\na_mag mean: 0.5760833621025085\na_mag mean: 0.7350848317146301\na_mag mean: 0.6446455121040344\na_mag mean: 0.5704609751701355\na_mag mean: 0.5809007883071899\na_mag mean: 0.07643484324216843\na_mag mean: 0.6998263597488403\na_mag mean: 0.7534862160682678\na_mag mean: 0.510093092918396\na_mag mean: 0.6631473302841187\na_mag mean: 0.6648700833320618\na_mag mean: 0.7574173212051392\na_mag mean: 0.6172056198120117\na_mag mean: 0.5464925169944763\na_mag mean: 0.5106267333030701\na_mag mean: 0.09441116452217102\na_mag mean: 0.7221760749816895\na_mag mean: 0.7879670262336731\na_mag mean: 0.5730836987495422\na_mag mean: 0.6712435483932495\na_mag mean: 0.6260707378387451\na_mag mean: 0.7777653336524963\na_mag mean: 0.5662403106689453\na_mag mean: 0.6362158060073853\na_mag mean: 0.5062923431396484\na_mag mean: 0.025553014129400253\na_mag mean: 0.7150089144706726\na_mag mean: 0.7765930891036987\na_mag mean: 0.6659634709358215\na_mag mean: 0.6819851994514465\na_mag mean: 0.6286010146141052\na_mag mean: 0.7398284673690796\na_mag mean: 0.6099772453308105\na_mag mean: 0.6703735589981079\na_mag mean: 0.43782004714012146\na_mag mean: 0.10506949573755264\na_mag mean: 0.7155247926712036\na_mag mean: 0.8205644488334656\na_mag mean: 0.6258516311645508\na_mag mean: 0.6929769515991211\na_mag mean: 0.6742273569107056\na_mag mean: 0.7846481204032898\na_mag mean: 0.4813718795776367\na_mag mean: 0.7015572190284729\na_mag mean: 0.48210814595222473\na_mag mean: 0.08500364422798157\na_mag mean: 0.7179051041603088\na_mag mean: 0.8016127943992615\na_mag mean: 0.6352407336235046\na_mag mean: 0.6406728625297546\na_mag mean: 0.589562714099884\na_mag mean: 0.7378940582275391\na_mag mean: 0.4882587492465973\na_mag mean: 0.6736515760421753\na_mag mean: 0.44158855080604553\na_mag mean: 0.11854513734579086\na_mag mean: 0.7068561315536499\na_mag mean: 0.8090564608573914\na_mag mean: 0.6075233817100525\na_mag mean: 0.6809330582618713\na_mag mean: 0.6702733039855957\na_mag mean: 0.81456059217453\na_mag mean: 0.4205552339553833\na_mag mean: 0.7614665627479553\na_mag mean: 0.5361763834953308\na_mag mean: 0.07746358960866928\na_mag mean: 0.7039214372634888\na_mag mean: 0.6878765821456909\na_mag mean: 0.516570508480072\na_mag mean: 0.6683915257453918\na_mag mean: 0.6402507424354553\na_mag mean: 0.7435153126716614\na_mag mean: 0.5537602305412292\na_mag mean: 0.6069968938827515\na_mag mean: 0.5245295166969299\na_mag mean: 0.12841768562793732\na_mag mean: 0.7216717600822449\na_mag mean: 0.8124315142631531\na_mag mean: 0.5751304030418396\na_mag mean: 0.6367180347442627\na_mag mean: 0.6860031485557556\na_mag mean: 0.7183014154434204\na_mag mean: 0.5565948486328125\na_mag mean: 0.6524697542190552\na_mag mean: 0.3495861887931824\na_mag mean: 0.06670884788036346\na_mag mean: 0.7247074842453003\na_mag mean: 0.7708377838134766\na_mag mean: 0.6312713027000427\na_mag mean: 0.6437721848487854\na_mag mean: 0.6530324220657349\na_mag mean: 0.7667783498764038\na_mag mean: 0.5120206475257874\na_mag mean: 0.7026243209838867\na_mag mean: 0.42436158657073975\na_mag mean: 0.09213116019964218\na_mag mean: 0.709113359451294\na_mag mean: 0.8317514061927795\na_mag mean: 0.5702042579650879\na_mag mean: 0.6371207237243652\na_mag mean: 0.6755498051643372\na_mag mean: 0.7874173521995544\na_mag mean: 0.49369189143180847\na_mag mean: 0.6880924701690674\na_mag mean: 0.5261774063110352\na_mag mean: 0.05575181171298027\na_mag mean: 0.7010067701339722\na_mag mean: 0.859226644039154\na_mag mean: 0.6683749556541443\na_mag mean: 0.7101917266845703\na_mag mean: 0.6340656876564026\na_mag mean: 0.7509639263153076\na_mag mean: 0.4763142168521881\na_mag mean: 0.638641893863678\na_mag mean: 0.5050814151763916\na_mag mean: 0.15019695460796356\na_mag mean: 0.718227207660675\na_mag mean: 0.7629303932189941\na_mag mean: 0.593473494052887\na_mag mean: 0.6889448761940002\na_mag mean: 0.6468451619148254\na_mag mean: 0.7732865810394287\na_mag mean: 0.6207972168922424\na_mag mean: 0.6313415765762329\na_mag mean: 0.48232418298721313\na_mag mean: 0.08123477548360825\na_mag mean: 0.6984922885894775\na_mag mean: 0.7878192663192749\na_mag mean: 0.5944542288780212\na_mag mean: 0.6843098402023315\na_mag mean: 0.6780510544776917\na_mag mean: 0.8035181760787964\na_mag mean: 0.4830128848552704\na_mag mean: 0.6891594529151917\na_mag mean: 0.5449845790863037\na_mag mean: 0.11374415457248688\na_mag mean: 0.7164163589477539\na_mag mean: 0.7729516625404358\na_mag mean: 0.6271690726280212\na_mag mean: 0.6732231974601746\na_mag mean: 0.6489450931549072\na_mag mean: 0.7803260087966919\na_mag mean: 0.5399300456047058\na_mag mean: 0.6811965107917786\na_mag mean: 0.5008763074874878\na_mag mean: 0.08285567164421082\na_mag mean: 0.7136097550392151\na_mag mean: 0.7770251035690308\na_mag mean: 0.5730566382408142\na_mag mean: 0.6332637667655945\na_mag mean: 0.6825257539749146\na_mag mean: 0.7325064539909363\na_mag mean: 0.5157371759414673\na_mag mean: 0.5768727660179138\na_mag mean: 0.3554871082305908\na_mag mean: 0.1511598378419876\na_mag mean: 0.7159916162490845\na_mag mean: 0.8199530243873596\na_mag mean: 0.5809349417686462\na_mag mean: 0.6489399671554565\na_mag mean: 0.686113715171814\na_mag mean: 0.6992688179016113\na_mag mean: 0.5645615458488464\na_mag mean: 0.6233346462249756\na_mag mean: 0.33415675163269043\na_mag mean: 0.1763238161802292\na_mag mean: 0.7001335024833679\na_mag mean: 0.7777502536773682\na_mag mean: 0.5577069520950317\na_mag mean: 0.679621696472168\na_mag mean: 0.6984460949897766\na_mag mean: 0.7069218754768372\na_mag mean: 0.6389113068580627\na_mag mean: 0.5567690134048462\na_mag mean: 0.38821449875831604\na_mag mean: 0.1109660193324089\na_mag mean: 0.7125975489616394\na_mag mean: 0.7271121740341187\na_mag mean: 0.48815956711769104\na_mag mean: 0.5558681488037109\na_mag mean: 0.6348506808280945\na_mag mean: 0.7410140633583069\na_mag mean: 0.555862545967102\na_mag mean: 0.5192438960075378\na_mag mean: 0.3886263966560364\na_mag mean: 0.20485036075115204\na_mag mean: 0.7036706209182739\na_mag mean: 0.8390999436378479\na_mag mean: 0.7076128721237183\na_mag mean: 0.7591024041175842\na_mag mean: 0.6275116205215454\na_mag mean: 0.7662063837051392\na_mag mean: 0.5243440866470337\na_mag mean: 0.6581259369850159\na_mag mean: 0.5114518404006958\na_mag mean: 0.0854608565568924\na_mag mean: 0.7254258990287781\na_mag mean: 0.7284618616104126\na_mag mean: 0.504155158996582\na_mag mean: 0.5572696924209595\na_mag mean: 0.6475809216499329\na_mag mean: 0.7453159093856812\na_mag mean: 0.5587415099143982\na_mag mean: 0.6950660347938538\na_mag mean: 0.3184749186038971\na_mag mean: 0.05301434174180031\na_mag mean: 0.7038477063179016\na_mag mean: 0.7412614822387695\na_mag mean: 0.5902007222175598\na_mag mean: 0.6552600264549255\na_mag mean: 0.6442363262176514\na_mag mean: 0.7731836438179016\na_mag mean: 0.5046707391738892\na_mag mean: 0.6723930239677429\na_mag mean: 0.4734448790550232\na_mag mean: 0.049173250794410706\na_mag mean: 0.7166163921356201\na_mag mean: 0.7492036819458008\na_mag mean: 0.5427777767181396\na_mag mean: 0.5999979972839355\na_mag mean: 0.6362999677658081\na_mag mean: 0.7205508351325989\na_mag mean: 0.6085846424102783\na_mag mean: 0.664089024066925\na_mag mean: 0.4151177406311035\na_mag mean: 0.10623061656951904\na_mag mean: 0.7227105498313904\na_mag mean: 0.7786097526550293\na_mag mean: 0.6170513033866882\na_mag mean: 0.6793879866600037\na_mag mean: 0.6487432718276978\na_mag mean: 0.7721912860870361\na_mag mean: 0.5092995762825012\na_mag mean: 0.6380302906036377\na_mag mean: 0.41066622734069824\na_mag mean: 0.0944681316614151\na_mag mean: 0.7169342637062073\na_mag mean: 0.769830584526062\na_mag mean: 0.5351069569587708\na_mag mean: 0.5600975751876831\na_mag mean: 0.6424466371536255\na_mag mean: 0.7294142246246338\na_mag mean: 0.6408541202545166\na_mag mean: 0.608511209487915\na_mag mean: 0.3789924681186676\na_mag mean: 0.1476134955883026\na_mag mean: 0.7061082124710083\na_mag mean: 0.7591477632522583\na_mag mean: 0.5853358507156372\na_mag mean: 0.592160165309906\na_mag mean: 0.6665912866592407\na_mag mean: 0.7382425665855408\na_mag mean: 0.4929921329021454\na_mag mean: 0.6411731839179993\na_mag mean: 0.3647553622722626\na_mag mean: 0.09283999353647232\na_mag mean: 0.7247320413589478\na_mag mean: 0.7633270025253296\na_mag mean: 0.5600142478942871\na_mag mean: 0.6028853058815002\na_mag mean: 0.6672695875167847\na_mag mean: 0.7819398641586304\na_mag mean: 0.5525704622268677\na_mag mean: 0.7358049750328064\na_mag mean: 0.39373210072517395\na_mag mean: 0.08080937713384628\na_mag mean: 0.7175378203392029\na_mag mean: 0.7606407403945923\na_mag mean: 0.5991041660308838\na_mag mean: 0.6331210732460022\na_mag mean: 0.6621366739273071\na_mag mean: 0.7480179071426392\na_mag mean: 0.5697813630104065\na_mag mean: 0.644699215888977\na_mag mean: 0.3922826945781708\na_mag mean: 0.10770063102245331\na_mag mean: 0.7112975716590881\na_mag mean: 0.7937948703765869\na_mag mean: 0.6537312865257263\na_mag mean: 0.6944875121116638\na_mag mean: 0.6769786477088928\na_mag mean: 0.7863719463348389\na_mag mean: 0.521675705909729\na_mag mean: 0.653779923915863\na_mag mean: 0.41598227620124817\na_mag mean: 0.08919206261634827\na_mag mean: 0.7228618860244751\na_mag mean: 0.7515649199485779\na_mag mean: 0.5864809155464172\na_mag mean: 0.6952134966850281\na_mag mean: 0.6424320936203003\na_mag mean: 0.7957810759544373\na_mag mean: 0.5306925177574158\na_mag mean: 0.658762514591217\na_mag mean: 0.4735063910484314\na_mag mean: 0.1021694615483284\na_mag mean: 0.7176573276519775\na_mag mean: 0.7586914896965027\na_mag mean: 0.5711826086044312\na_mag mean: 0.6316735148429871\na_mag mean: 0.6676764488220215\na_mag mean: 0.7238507866859436\na_mag mean: 0.6361513733863831\na_mag mean: 0.700169026851654\na_mag mean: 0.27006226778030396\na_mag mean: 0.08653955906629562\na_mag mean: 0.724190354347229\na_mag mean: 0.7590802311897278\na_mag mean: 0.6362075209617615\na_mag mean: 0.6752964854240417\na_mag mean: 0.6630442142486572\na_mag mean: 0.7640566825866699\na_mag mean: 0.5718587040901184\na_mag mean: 0.6693436503410339\na_mag mean: 0.41911137104034424\na_mag mean: 0.10373373329639435\na_mag mean: 0.7142443060874939\na_mag mean: 0.7873973250389099\na_mag mean: 0.6483259201049805\na_mag mean: 0.6394158005714417\na_mag mean: 0.6864795088768005\na_mag mean: 0.7859591245651245\na_mag mean: 0.5287359356880188\na_mag mean: 0.7354813814163208\na_mag mean: 0.4112718105316162\na_mag mean: 0.09699227660894394\na_mag mean: 0.6988530158996582\na_mag mean: 0.8574675917625427\na_mag mean: 0.6030941009521484\na_mag mean: 0.6522385478019714\na_mag mean: 0.658184289932251\na_mag mean: 0.7767524719238281\na_mag mean: 0.5642622113227844\na_mag mean: 0.7204698920249939\na_mag mean: 0.457783043384552\na_mag mean: 0.07399129867553711\na_mag mean: 0.7119491696357727\na_mag mean: 0.7721635103225708\na_mag mean: 0.5541905760765076\na_mag mean: 0.6174259185791016\na_mag mean: 0.628563404083252\na_mag mean: 0.7429847717285156\na_mag mean: 0.5263881683349609\na_mag mean: 0.674098789691925\na_mag mean: 0.4814116656780243\na_mag mean: 0.10149088501930237\na_mag mean: 0.7269799709320068\na_mag mean: 0.7581086158752441\na_mag mean: 0.5325304269790649\na_mag mean: 0.6654728055000305\na_mag mean: 0.721277117729187\na_mag mean: 0.7083679437637329\na_mag mean: 0.5950977802276611\na_mag mean: 0.6877447366714478\na_mag mean: 0.32960793375968933\na_mag mean: 0.1341279000043869\na_mag mean: 0.712239146232605\na_mag mean: 0.8237515091896057\na_mag mean: 0.5850472450256348\na_mag mean: 0.6609071493148804\na_mag mean: 0.7058265805244446\na_mag mean: 0.7230755686759949\na_mag mean: 0.6160697340965271\na_mag mean: 0.6730573773384094\na_mag mean: 0.33015942573547363\na_mag mean: 0.11344616860151291\na_mag mean: 0.707318127155304\na_mag mean: 0.7529352307319641\na_mag mean: 0.5652135014533997\na_mag mean: 0.6163126826286316\na_mag mean: 0.6507142186164856\na_mag mean: 0.7316867113113403\na_mag mean: 0.5726889371871948\na_mag mean: 0.6587691307067871\na_mag mean: 0.4294493794441223\na_mag mean: 0.15589523315429688\na_mag mean: 0.7010469436645508\na_mag mean: 0.7509775161743164\na_mag mean: 0.4995339810848236\na_mag mean: 0.6360397338867188\na_mag mean: 0.6882703304290771\na_mag mean: 0.7512945532798767\na_mag mean: 0.6327621340751648\na_mag mean: 0.4720703959465027\na_mag mean: 0.3916912376880646\na_mag mean: 0.2694112956523895\na_mag mean: 0.7002529501914978\na_mag mean: 0.7923442721366882\na_mag mean: 0.5746724009513855\na_mag mean: 0.6438676714897156\na_mag mean: 0.6501182317733765\na_mag mean: 0.7349309921264648\na_mag mean: 0.5301639437675476\na_mag mean: 0.6826542615890503\na_mag mean: 0.444009929895401\na_mag mean: 0.13466186821460724\na_mag mean: 0.7162312269210815\na_mag mean: 0.7968361973762512\na_mag mean: 0.5855342745780945\na_mag mean: 0.6602025628089905\na_mag mean: 0.6548573970794678\na_mag mean: 0.7891252636909485\na_mag mean: 0.563535213470459\na_mag mean: 0.6319749355316162\na_mag mean: 0.4607416093349457\na_mag mean: 0.18413692712783813\na_mag mean: 0.7038556933403015\na_mag mean: 0.738877534866333\na_mag mean: 0.6089664101600647\na_mag mean: 0.6220827102661133\na_mag mean: 0.6724210977554321\na_mag mean: 0.688629150390625\na_mag mean: 0.5879977345466614\na_mag mean: 0.6276724934577942\na_mag mean: 0.2918677031993866\na_mag mean: 0.12648159265518188\na_mag mean: 0.6995841264724731\na_mag mean: 0.727528989315033\na_mag mean: 0.5637699365615845\na_mag mean: 0.6385934352874756\na_mag mean: 0.6459477543830872\na_mag mean: 0.7202157974243164\na_mag mean: 0.6576071381568909\na_mag mean: 0.6087232232093811\na_mag mean: 0.3829948902130127\na_mag mean: 0.19970625638961792\na_mag mean: 0.7042847275733948\na_mag mean: 0.8133833408355713\na_mag mean: 0.6181710958480835\na_mag mean: 0.6934374570846558\na_mag mean: 0.7124090194702148\na_mag mean: 0.7753729224205017\na_mag mean: 0.5401818752288818\na_mag mean: 0.6898005604743958\na_mag mean: 0.4185112714767456\na_mag mean: 0.11901308596134186\na_mag mean: 0.7179133892059326\na_mag mean: 0.7158670425415039\na_mag mean: 0.6041722893714905\na_mag mean: 0.6400415897369385\na_mag mean: 0.6607996225357056\na_mag mean: 0.737553596496582\na_mag mean: 0.6129106879234314\na_mag mean: 0.6301803588867188\na_mag mean: 0.4023118019104004\na_mag mean: 0.09844037890434265\na_mag mean: 0.7236123085021973\na_mag mean: 0.68819659948349\na_mag mean: 0.6026180982589722\na_mag mean: 0.6147788763046265\na_mag mean: 0.6865491271018982\na_mag mean: 0.7576095461845398\na_mag mean: 0.6296685338020325\na_mag mean: 0.6616295576095581\na_mag mean: 0.2877028286457062\na_mag mean: 0.07562049478292465\na_mag mean: 0.7144611477851868\na_mag mean: 0.7745541930198669\na_mag mean: 0.650454044342041\na_mag mean: 0.6428298950195312\na_mag mean: 0.6461977362632751\na_mag mean: 0.7888327240943909\na_mag mean: 0.5463348031044006\na_mag mean: 0.7291575074195862\na_mag mean: 0.4388462007045746\na_mag mean: 0.06406015157699585\na_mag mean: 0.7293263077735901\na_mag mean: 0.7497713565826416\na_mag mean: 0.6009207963943481\na_mag mean: 0.5984982252120972\na_mag mean: 0.6525997519493103\na_mag mean: 0.7686390280723572\na_mag mean: 0.6127675175666809\na_mag mean: 0.6863052845001221\na_mag mean: 0.3687411844730377\na_mag mean: 0.09402525424957275\na_mag mean: 0.7091268301010132\na_mag mean: 0.7186949253082275\na_mag mean: 0.608534038066864\na_mag mean: 0.6428176760673523\na_mag mean: 0.657179057598114\na_mag mean: 0.7225959897041321\na_mag mean: 0.6437127590179443\na_mag mean: 0.6429089307785034\na_mag mean: 0.30767565965652466\na_mag mean: 0.09342765063047409\na_mag mean: 0.7162898182868958\na_mag mean: 0.7256038784980774\na_mag mean: 0.6023824214935303\na_mag mean: 0.6575598120689392\na_mag mean: 0.6505798697471619\na_mag mean: 0.7190538048744202\na_mag mean: 0.5853805541992188\na_mag mean: 0.6452956795692444\na_mag mean: 0.35793349146842957\na_mag mean: 0.2130795270204544\na_mag mean: 0.7190686464309692\na_mag mean: 0.7157186269760132\na_mag mean: 0.6084040999412537\na_mag mean: 0.6185599565505981\na_mag mean: 0.6833630204200745\na_mag mean: 0.7312356233596802\na_mag mean: 0.5574004650115967\na_mag mean: 0.6765241622924805\na_mag mean: 0.34725192189216614\na_mag mean: 0.10177353769540787\na_mag mean: 0.7125760912895203\na_mag mean: 0.7166680693626404\na_mag mean: 0.6207666993141174\na_mag mean: 0.6442952156066895\na_mag mean: 0.6295753121376038\na_mag mean: 0.7341741919517517\na_mag mean: 0.5992122292518616\na_mag mean: 0.6469866037368774\na_mag mean: 0.28751906752586365\na_mag mean: 0.13528208434581757\na_mag mean: 0.7062115669250488\na_mag mean: 0.7646498680114746\na_mag mean: 0.536453127861023\na_mag mean: 0.6439548134803772\na_mag mean: 0.7045760154724121\na_mag mean: 0.7291927337646484\na_mag mean: 0.5902678966522217\na_mag mean: 0.6800853610038757\na_mag mean: 0.38556408882141113\na_mag mean: 0.10806869715452194\na_mag mean: 0.7239479422569275\na_mag mean: 0.7449149489402771\na_mag mean: 0.6638932824134827\na_mag mean: 0.6747310161590576\na_mag mean: 0.6185581088066101\na_mag mean: 0.7297492623329163\na_mag mean: 0.5507608652114868\na_mag mean: 0.6710367798805237\na_mag mean: 0.40079253911972046\na_mag mean: 0.16285890340805054\na_mag mean: 0.7151483297348022\na_mag mean: 0.7224842309951782\na_mag mean: 0.5091118812561035\na_mag mean: 0.6055931448936462\na_mag mean: 0.744012176990509\na_mag mean: 0.6756550073623657\na_mag mean: 0.6234254837036133\na_mag mean: 0.5988988876342773\na_mag mean: 0.36048585176467896\na_mag mean: 0.2565910220146179\na_mag mean: 0.7255048155784607\na_mag mean: 0.7242385149002075\na_mag mean: 0.6126740574836731\na_mag mean: 0.6479300856590271\na_mag mean: 0.6493645906448364\na_mag mean: 0.7190255522727966\na_mag mean: 0.6027552485466003\na_mag mean: 0.6911189556121826\na_mag mean: 0.2953099310398102\na_mag mean: 0.11067616194486618\na_mag mean: 0.7257448434829712\na_mag mean: 0.7332599759101868\na_mag mean: 0.6145135760307312\na_mag mean: 0.6241429448127747\na_mag mean: 0.6260684728622437\na_mag mean: 0.7655702829360962\na_mag mean: 0.6251813769340515\na_mag mean: 0.6872190833091736\na_mag mean: 0.420841246843338\na_mag mean: 0.1523023247718811\na_mag mean: 0.7099882364273071\na_mag mean: 0.7028295397758484\na_mag mean: 0.5427880883216858\na_mag mean: 0.5862037539482117\na_mag mean: 0.6457532644271851\na_mag mean: 0.7386746406555176\na_mag mean: 0.5835756659507751\na_mag mean: 0.6346616744995117\na_mag mean: 0.35425761342048645\na_mag mean: 0.16298873722553253\na_mag mean: 0.7114792466163635\na_mag mean: 0.7909859418869019\na_mag mean: 0.6189400553703308\na_mag mean: 0.6351956725120544\na_mag mean: 0.6622534990310669\na_mag mean: 0.7583189606666565\na_mag mean: 0.6030675768852234\na_mag mean: 0.7405951619148254\na_mag mean: 0.41980740427970886\na_mag mean: 0.11579730361700058\na_mag mean: 0.695469081401825\na_mag mean: 0.7391416430473328\na_mag mean: 0.4227835536003113\na_mag mean: 0.4997413754463196\na_mag mean: 0.7054179906845093\na_mag mean: 0.6493782997131348\na_mag mean: 0.6106539368629456\na_mag mean: 0.4843347668647766\na_mag mean: 0.3000381290912628\na_mag mean: 0.32730263471603394\na_mag mean: 0.7193934917449951\na_mag mean: 0.7016463279724121\na_mag mean: 0.5289433598518372\na_mag mean: 0.6304076313972473\na_mag mean: 0.7038942575454712\na_mag mean: 0.6558417677879333\na_mag mean: 0.6176621317863464\na_mag mean: 0.6181598901748657\na_mag mean: 0.3548195958137512\na_mag mean: 0.18963050842285156\na_mag mean: 0.707546591758728\na_mag mean: 0.7799797058105469\na_mag mean: 0.5590930581092834\na_mag mean: 0.6115692257881165\na_mag mean: 0.6064386367797852\na_mag mean: 0.6991786360740662\na_mag mean: 0.6259862184524536\na_mag mean: 0.7039046883583069\na_mag mean: 0.37708744406700134\na_mag mean: 0.08400271087884903\na_mag mean: 0.7187447547912598\na_mag mean: 0.7283409237861633\na_mag mean: 0.6109659671783447\na_mag mean: 0.6283910274505615\na_mag mean: 0.6733607649803162\na_mag mean: 0.7451095581054688\na_mag mean: 0.5274354219436646\na_mag mean: 0.6961127519607544\na_mag mean: 0.3485833406448364\na_mag mean: 0.1155809536576271\na_mag mean: 0.716209352016449\na_mag mean: 0.7009522318840027\na_mag mean: 0.4661251902580261\na_mag mean: 0.6674929261207581\na_mag mean: 0.7483897805213928\na_mag mean: 0.628355860710144\na_mag mean: 0.6466270089149475\na_mag mean: 0.558860182762146\na_mag mean: 0.4218529164791107\na_mag mean: 0.31083783507347107\na_mag mean: 0.7023532390594482\na_mag mean: 0.7745583057403564\na_mag mean: 0.5998789668083191\na_mag mean: 0.6153174042701721\na_mag mean: 0.6567816734313965\na_mag mean: 0.7558197379112244\na_mag mean: 0.5515202879905701\na_mag mean: 0.7367607951164246\na_mag mean: 0.3976304829120636\na_mag mean: 0.11030349880456924\na_mag mean: 0.7185570001602173\na_mag mean: 0.7605733275413513\na_mag mean: 0.5779586434364319\na_mag mean: 0.6486867070198059\na_mag mean: 0.6574755311012268\na_mag mean: 0.7410823702812195\na_mag mean: 0.638171374797821\na_mag mean: 0.7201594114303589\na_mag mean: 0.33558428287506104\na_mag mean: 0.05999073013663292\na_mag mean: 0.7163546085357666\na_mag mean: 0.8165205121040344\na_mag mean: 0.6707495450973511\na_mag mean: 0.7113868594169617\na_mag mean: 0.6138395667076111\na_mag mean: 0.7849985957145691\na_mag mean: 0.5062426328659058\na_mag mean: 0.7272231578826904\na_mag mean: 0.4828857183456421\na_mag mean: 0.04985018074512482\na_mag mean: 0.7222424745559692\na_mag mean: 0.7567402124404907\na_mag mean: 0.5355640053749084\na_mag mean: 0.6822609305381775\na_mag mean: 0.735996425151825\na_mag mean: 0.6719838380813599\na_mag mean: 0.6751837730407715\na_mag mean: 0.6555796265602112\na_mag mean: 0.316407173871994\na_mag mean: 0.05603816360235214\na_mag mean: 0.7035771012306213\na_mag mean: 0.7913752198219299\na_mag mean: 0.6196858882904053\na_mag mean: 0.6187266111373901\na_mag mean: 0.6199254989624023\na_mag mean: 0.746812641620636\na_mag mean: 0.5867109894752502\na_mag mean: 0.694709300994873\na_mag mean: 0.403048574924469\na_mag mean: 0.1330430805683136\na_mag mean: 0.719386637210846\na_mag mean: 0.767198383808136\na_mag mean: 0.5629144310951233\na_mag mean: 0.6051375865936279\na_mag mean: 0.6698694825172424\na_mag mean: 0.6877501010894775\na_mag mean: 0.6737176179885864\na_mag mean: 0.5974441170692444\na_mag mean: 0.301859587430954\na_mag mean: 0.1686307042837143\na_mag mean: 0.7064382433891296\na_mag mean: 0.7540009617805481\na_mag mean: 0.5641997456550598\na_mag mean: 0.6605753302574158\na_mag mean: 0.7067246437072754\na_mag mean: 0.6707615852355957\na_mag mean: 0.6586041450500488\na_mag mean: 0.6220510005950928\na_mag mean: 0.41021308302879333\na_mag mean: 0.20468437671661377\na_mag mean: 0.7083156108856201\na_mag mean: 0.7397977709770203\na_mag mean: 0.5990068316459656\na_mag mean: 0.5822997093200684\na_mag mean: 0.5707159042358398\na_mag mean: 0.7171347737312317\na_mag mean: 0.535467803478241\na_mag mean: 0.6630865335464478\na_mag mean: 0.3609737753868103\na_mag mean: 0.11312714219093323\nExported a_mag values to ./a_mag_values.parquet\nInference completed! Took 10.35s\n[*] All done: 2024-10-20 20:37:16.801744\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Analysis","metadata":{}},{"cell_type":"code","source":"def plot_a_mag_from_parquet(parquet_file=\"./a_mag_values.parquet\", save_path=\"./a_mag_histogram.png\"):\n    df = pd.read_parquet(parquet_file)\n    \n    plt.figure(figsize=(10, 6))\n    plt.hist(df['a_mag_mean'], bins=30, edgecolor='black', alpha=0.7)\n    plt.title('Histogram of a_mag Mean Values in molhiv, project number 1')\n    plt.xlabel('Mean Value')\n    plt.ylabel('Frequency')\n    plt.grid(axis='y', alpha=0.75)\n    \n    # Set x-axis ticks from 0 to 1 with 0.1 intervals\n    plt.xticks(np.arange(0, 1.1, 0.1))\n    \n    # Save the plot as an image file\n    plt.savefig(save_path, format='png', dpi=300, bbox_inches='tight')\n    \n    # Display the plot\n    plt.show()\n\nplot_a_mag_from_parquet()","metadata":{"execution":{"iopub.status.busy":"2024-10-20T20:45:09.515862Z","iopub.execute_input":"2024-10-20T20:45:09.516833Z","iopub.status.idle":"2024-10-20T20:45:10.294827Z","shell.execute_reply.started":"2024-10-20T20:45:09.516786Z","shell.execute_reply":"2024-10-20T20:45:10.294042Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}