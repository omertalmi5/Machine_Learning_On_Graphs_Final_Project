{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8942367,"sourceType":"datasetVersion","datasetId":5380713}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-09-24T13:01:32.532028Z","iopub.execute_input":"2024-09-24T13:01:32.533026Z","iopub.status.idle":"2024-09-24T13:01:32.905211Z","shell.execute_reply.started":"2024-09-24T13:01:32.532974Z","shell.execute_reply":"2024-09-24T13:01:32.904298Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl\n/kaggle/input/torch_geometric-2.5.3-py3-none-any.whl\n/kaggle/input/torch_cluster-1.6.3-cp310-cp310-linux_x86_64.whl\n/kaggle/input/torch_spline_conv-1.2.2-cp310-cp310-linux_x86_64.whl\n/kaggle/input/torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/rampasek/GraphGPS.git","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:01:32.906655Z","iopub.execute_input":"2024-09-24T13:01:32.907045Z","iopub.status.idle":"2024-09-24T13:01:35.061159Z","shell.execute_reply.started":"2024-09-24T13:01:32.907011Z","shell.execute_reply":"2024-09-24T13:01:35.060159Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'GraphGPS'...\nremote: Enumerating objects: 526, done.\u001b[K\nremote: Counting objects: 100% (316/316), done.\u001b[K\nremote: Compressing objects: 100% (91/91), done.\u001b[K\nremote: Total 526 (delta 254), reused 225 (delta 225), pack-reused 210 (from 1)\u001b[K\nReceiving objects: 100% (526/526), 12.93 MiB | 35.49 MiB/s, done.\nResolving deltas: 100% (336/336), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/GraphGPS\n!ls ","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:01:35.063197Z","iopub.execute_input":"2024-09-24T13:01:35.063515Z","iopub.status.idle":"2024-09-24T13:01:36.068854Z","shell.execute_reply.started":"2024-09-24T13:01:35.063482Z","shell.execute_reply":"2024-09-24T13:01:36.067922Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/GraphGPS\nGraphGPS.png  README.md  final-results.zip  main.py  setup.py  unittests\nLICENSE       configs\t graphgps\t    run      tests\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##  installation for slurm if needed using conda ","metadata":{}},{"cell_type":"code","source":"# # for slurm if needed using conda \n# conda create -n graphgps python=3.10\n# conda activate graphgps\n\n# conda install pytorch=1.13 torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n# conda install pyg=2.2 -c pyg -c conda-forge\n# pip install pyg-lib -f https://data.pyg.org/whl/torch-1.13.0+cu117.html\n\n# # RDKit is required for OGB-LSC PCQM4Mv2 and datasets derived from it.  \n# conda install openbabel fsspec rdkit -c conda-forge\n\n# pip install pytorch-lightning yacs torchmetrics\n# pip install performer-pytorch\n# pip install tensorboardX\n# pip install ogb\n# pip install wandb\n\n# conda clean --all","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:01:36.603986Z","iopub.execute_input":"2024-09-24T13:01:36.604347Z","iopub.status.idle":"2024-09-24T13:01:36.609318Z","shell.execute_reply.started":"2024-09-24T13:01:36.604315Z","shell.execute_reply":"2024-09-24T13:01:36.608294Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# installation for kaggle","metadata":{}},{"cell_type":"code","source":"# Check existing environment\nimport sys\nprint(\"Python version:\", sys.version)\nimport torch\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\n\n!pip uninstall -y torch torchvision torchaudio\n!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n\n!pip install torch_geometric\n\n# Optional dependencies:\n!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n# Install other required packages\n!pip install pytorch-lightning yacs torchmetrics\n!pip install performer-pytorch\n!pip install tensorboardX\n!pip install ogb\n!pip install wandb\n\n# Attempt to install RDKit (may have limitations)\n!pip install rdkit-pypi\n# !pip install openbabel fsspec\n!pip install  fsspec\n\n# Clean up pip cache (optional)\n!pip cache purge\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:01:38.676615Z","iopub.execute_input":"2024-09-24T13:01:38.676987Z","iopub.status.idle":"2024-09-24T13:09:11.811324Z","shell.execute_reply.started":"2024-09-24T13:01:38.676952Z","shell.execute_reply":"2024-09-24T13:09:11.809992Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\nPyTorch version: 2.4.0\nCUDA available: True\nCUDA version: 12.3\nFound existing installation: torch 2.4.0\nUninstalling torch-2.4.0:\n  Successfully uninstalled torch-2.4.0\nFound existing installation: torchvision 0.19.0\nUninstalling torchvision-0.19.0:\n  Successfully uninstalled torchvision-0.19.0\nFound existing installation: torchaudio 2.4.0\nUninstalling torchaudio-2.4.0:\n  Successfully uninstalled torchaudio-2.4.0\nLooking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch==2.0.1+cu118\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m399.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.15.2+cu118\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==2.0.2+cu118\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp310-cp310-linux_x86_64.whl (4.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.15.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.1.4)\nCollecting triton==2.0.0 (from torch==2.0.1+cu118)\n  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.15.2+cu118) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.15.2+cu118) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.15.2+cu118) (10.3.0)\nCollecting cmake (from triton==2.0.0->torch==2.0.1+cu118)\n  Downloading https://download.pytorch.org/whl/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.1+cu118)\n  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1+cu118) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\nBuilding wheels for collected packages: lit\n  Building wheel for lit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89987 sha256=62901f946d41005e66ba0da83def3c5ce3d8632a69b94366478cfbe4a606abe9\n  Stored in directory: /root/.cache/pip/wheels/27/2c/b6/3ed2983b1b44fe0dea1bb35234b09f2c22fb8ebb308679c922\nSuccessfully built lit\nInstalling collected packages: lit, cmake, triton, torch, torchvision, torchaudio\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.4.0 requires torch>=2.1.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cmake-3.25.0 lit-15.0.7 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 triton-2.0.0\nCollecting torch_geometric\n  Downloading torch_geometric-2.6.0-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m630.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.9.5)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2024.6.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2024.8.30)\nDownloading torch_geometric-2.6.0-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.0\nLooking in links: https://data.pyg.org/whl/torch-2.4.0+cu124.html\nCollecting pyg_lib\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/pyg_lib-0.4.0%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting torch_scatter\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_scatter-2.1.2%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (10.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting torch_sparse\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_sparse-0.6.18%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (5.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hCollecting torch_cluster\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_cluster-1.6.3%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hCollecting torch_spline_conv\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (992 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m992.4/992.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_sparse) (1.14.1)\nRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy->torch_sparse) (1.26.4)\nInstalling collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\nSuccessfully installed pyg_lib-0.4.0+pt24cu124 torch_cluster-1.6.3+pt24cu124 torch_scatter-2.1.2+pt24cu124 torch_sparse-0.6.18+pt24cu124 torch_spline_conv-1.2.2+pt24cu124\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (2.4.0)\nCollecting yacs\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.2)\nCollecting torch>=2.1.0 (from pytorch-lightning)\n  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.66.4)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (6.0.2)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (21.3)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.12.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (0.11.7)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (70.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch-lightning) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch>=2.1.0->pytorch-lightning)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m853.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: yacs, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: triton\n    Found existing installation: triton 2.0.0\n    Uninstalling triton-2.0.0:\n      Successfully uninstalled triton-2.0.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.1+cu118\n    Uninstalling torch-2.0.1+cu118:\n      Successfully uninstalled torch-2.0.1+cu118\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.4.1 which is incompatible.\ntorchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0 yacs-0.1.8\nCollecting performer-pytorch\n  Downloading performer_pytorch-1.1.4-py3-none-any.whl.metadata (763 bytes)\nCollecting einops>=0.3 (from performer-pytorch)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting local-attention>=1.1.1 (from performer-pytorch)\n  Downloading local_attention-1.9.15-py3-none-any.whl.metadata (683 bytes)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from performer-pytorch) (2.4.1)\nCollecting axial-positional-embedding>=0.1.0 (from performer-pytorch)\n  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->performer-pytorch) (12.6.68)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->performer-pytorch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->performer-pytorch) (1.3.0)\nDownloading performer_pytorch-1.1.4-py3-none-any.whl (13 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m707.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading local_attention-1.9.15-py3-none-any.whl (9.0 kB)\nBuilding wheels for collected packages: axial-positional-embedding\n  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2889 sha256=f52a11641df5aaf4084bd76a859486f11febb6645816e8ac399bae04c11bbccc\n  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\nSuccessfully built axial-positional-embedding\nInstalling collected packages: einops, local-attention, axial-positional-embedding, performer-pytorch\nSuccessfully installed axial-positional-embedding-0.2.1 einops-0.8.0 local-attention-1.9.15 performer-pytorch-1.1.4\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (2.6.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (21.3)\nRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (3.20.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorboardX) (3.1.2)\nCollecting ogb\n  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (2.4.1)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.26.4)\nRequirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (4.66.4)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.2.2)\nRequirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (2.2.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.16.0)\nRequirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.26.18)\nCollecting outdated>=0.2.0 (from ogb)\n  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: setuptools>=44 in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (70.0.0)\nCollecting littleutils (from outdated>=0.2.0->ogb)\n  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (2.32.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2024.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.6.68)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\nDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\nInstalling collected packages: littleutils, outdated, ogb\nSuccessfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.14.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nCollecting rdkit-pypi\n  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rdkit-pypi) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit-pypi) (10.3.0)\nDownloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rdkit-pypi\nSuccessfully installed rdkit-pypi-2022.9.5\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (2024.6.1)\nFiles removed: 184\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nprint(\"Python version:\", sys.version)\nimport torch\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:09:11.813749Z","iopub.execute_input":"2024-09-24T13:09:11.814514Z","iopub.status.idle":"2024-09-24T13:09:11.821425Z","shell.execute_reply.started":"2024-09-24T13:09:11.814471Z","shell.execute_reply":"2024-09-24T13:09:11.820579Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\nPyTorch version: 2.4.0\nCUDA available: True\nCUDA version: 12.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch_geometric\nprint(\"PyG version:\", torch_geometric.__version__)\n\nimport pytorch_lightning\nprint(\"PyTorch Lightning version:\", pytorch_lightning.__version__)\n\nimport rdkit\nprint(\"RDKit version:\", rdkit.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:09:11.822884Z","iopub.execute_input":"2024-09-24T13:09:11.823176Z","iopub.status.idle":"2024-09-24T13:09:17.543853Z","shell.execute_reply.started":"2024-09-24T13:09:11.823144Z","shell.execute_reply":"2024-09-24T13:09:17.542861Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/libpyg.so: undefined symbol: _ZNK5torch8autograd4Node4nameEv\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n","output_type":"stream"},{"name":"stdout","text":"PyG version: 2.6.0\nPyTorch Lightning version: 2.4.0\nRDKit version: 2022.09.5\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Torch version:\", torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:09:17.546447Z","iopub.execute_input":"2024-09-24T13:09:17.547314Z","iopub.status.idle":"2024-09-24T13:09:17.552273Z","shell.execute_reply.started":"2024-09-24T13:09:17.547273Z","shell.execute_reply":"2024-09-24T13:09:17.551419Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Torch version: 2.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/GraphGPS\n!WANDB_MODE=disabled python main.py --cfg configs/GPS/ogbg-molhiv-GPS.yaml","metadata":{"execution":{"iopub.execute_input":"2024-09-23T13:16:01.896414Z","iopub.status.busy":"2024-09-23T13:16:01.895798Z","iopub.status.idle":"2024-09-23T13:40:46.966746Z","shell.execute_reply":"2024-09-23T13:40:46.965498Z","shell.execute_reply.started":"2024-09-23T13:16:01.896375Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"/kaggle/working/GraphGPS\n\n[*] Run ID 0: seed=0, split_index=0\n\n    Starting now: 2024-09-23 13:16:20.178181\n\nDownloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n\nDownloaded 0.00 GB: 100%|█████████████████████████| 3/3 [00:00<00:00,  3.88it/s]\n\nExtracting ./datasets/hiv.zip\n\nProcessing...\n\nLoading necessary files...\n\nThis might take a while.\n\nProcessing graphs...\n\n100%|██████████████████████████████████| 41127/41127 [00:00<00:00, 60420.42it/s]\n\nConverting graphs into PyG objects...\n\n100%|██████████████████████████████████| 41127/41127 [00:02<00:00, 18891.90it/s]\n\nSaving...\n\nDone!\n\n/opt/conda/lib/python3.10/site-packages/ogb/graphproppred/dataset_pyg.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\n  self.data, self.slices = torch.load(self.processed_paths[0])\n\n[*] Loaded dataset 'ogbg-molhiv' from 'OGB':\n\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n\n  warnings.warn(msg)\n\n  Data(edge_index=[2, 2259376], edge_attr=[2259376, 3], x=[1049163, 9], y=[41127, 1], num_nodes=1049163)\n\n  undirected: True\n\n  num graphs: 41127\n\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n\n  warnings.warn(msg)\n\n  avg num_nodes/graph: 25\n\n  num node features: 9\n\n  num edge features: 3\n\n  num tasks: 1\n\n  num classes: 2\n\nPrecomputing Positional Encoding statistics: ['LapPE'] for all graphs...\n\n  ...estimated to be undirected: True\n\n100%|████████████████████████████████████| 41127/41127 [00:53<00:00, 764.44it/s]\n\nDone! Took 00:00:55.39\n\nGraphGymModule(\n\n  (model): GPSModel(\n\n    (encoder): FeatureEncoder(\n\n      (node_encoder): Concat2NodeEncoder(\n\n        (encoder1): AtomEncoder(\n\n          (atom_embedding_list): ModuleList(\n\n            (0): Embedding(119, 56)\n\n            (1): Embedding(5, 56)\n\n            (2-3): 2 x Embedding(12, 56)\n\n            (4): Embedding(10, 56)\n\n            (5-6): 2 x Embedding(6, 56)\n\n            (7-8): 2 x Embedding(2, 56)\n\n          )\n\n        )\n\n        (encoder2): LapPENodeEncoder(\n\n          (linear_A): Linear(in_features=2, out_features=16, bias=True)\n\n          (pe_encoder): Sequential(\n\n            (0): ReLU()\n\n            (1): Linear(in_features=16, out_features=8, bias=True)\n\n            (2): ReLU()\n\n          )\n\n        )\n\n      )\n\n      (edge_encoder): BondEncoder(\n\n        (bond_embedding_list): ModuleList(\n\n          (0): Embedding(5, 64)\n\n          (1): Embedding(6, 64)\n\n          (2): Embedding(2, 64)\n\n        )\n\n      )\n\n    )\n\n    (layers): Sequential(\n\n      (0): GPSLayer(\n\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n\n        (local_model): GatedGCNLayer()\n\n        (self_attn): MultiheadAttention(\n\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n\n        )\n\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (dropout_local): Dropout(p=0.05, inplace=False)\n\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n\n        (act_fn_ff): ReLU()\n\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n\n      )\n\n      (1): GPSLayer(\n\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n\n        (local_model): GatedGCNLayer()\n\n        (self_attn): MultiheadAttention(\n\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n\n        )\n\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (dropout_local): Dropout(p=0.05, inplace=False)\n\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n\n        (act_fn_ff): ReLU()\n\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n\n      )\n\n      (2): GPSLayer(\n\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n\n        (local_model): GatedGCNLayer()\n\n        (self_attn): MultiheadAttention(\n\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n\n        )\n\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (dropout_local): Dropout(p=0.05, inplace=False)\n\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n\n        (act_fn_ff): ReLU()\n\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n\n      )\n\n      (3): GPSLayer(\n\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n\n        (local_model): GatedGCNLayer()\n\n        (self_attn): MultiheadAttention(\n\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n\n        )\n\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (dropout_local): Dropout(p=0.05, inplace=False)\n\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n\n        (act_fn_ff): ReLU()\n\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n\n      )\n\n      (4): GPSLayer(\n\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n\n        (local_model): GatedGCNLayer()\n\n        (self_attn): MultiheadAttention(\n\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n\n        )\n\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (dropout_local): Dropout(p=0.05, inplace=False)\n\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n\n        (act_fn_ff): ReLU()\n\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n\n      )\n\n      (5): GPSLayer(\n\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n\n        (local_model): GatedGCNLayer()\n\n        (self_attn): MultiheadAttention(\n\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n\n        )\n\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (dropout_local): Dropout(p=0.05, inplace=False)\n\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n\n        (act_fn_ff): ReLU()\n\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n\n      )\n\n      (6): GPSLayer(\n\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n\n        (local_model): GatedGCNLayer()\n\n        (self_attn): MultiheadAttention(\n\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n\n        )\n\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (dropout_local): Dropout(p=0.05, inplace=False)\n\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n\n        (act_fn_ff): ReLU()\n\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n\n      )\n\n      (7): GPSLayer(\n\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n\n        (local_model): GatedGCNLayer()\n\n        (self_attn): MultiheadAttention(\n\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n\n        )\n\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (dropout_local): Dropout(p=0.05, inplace=False)\n\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n\n        (act_fn_ff): ReLU()\n\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n\n      )\n\n      (8): GPSLayer(\n\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n\n        (local_model): GatedGCNLayer()\n\n        (self_attn): MultiheadAttention(\n\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n\n        )\n\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (dropout_local): Dropout(p=0.05, inplace=False)\n\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n\n        (act_fn_ff): ReLU()\n\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n\n      )\n\n      (9): GPSLayer(\n\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n\n        (local_model): GatedGCNLayer()\n\n        (self_attn): MultiheadAttention(\n\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n\n        )\n\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (dropout_local): Dropout(p=0.05, inplace=False)\n\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n\n        (act_fn_ff): ReLU()\n\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n\n      )\n\n    )\n\n    (post_mp): SANGraphHead(\n\n      (FC_layers): ModuleList(\n\n        (0): Linear(in_features=64, out_features=32, bias=True)\n\n        (1): Linear(in_features=32, out_features=16, bias=True)\n\n        (2): Linear(in_features=16, out_features=1, bias=True)\n\n      )\n\n      (activation): ReLU()\n\n    )\n\n  )\n\n)\n\naccelerator: cuda\n\nbenchmark: False\n\nbn:\n\n  eps: 1e-05\n\n  mom: 0.1\n\ncfg_dest: config.yaml\n\ncustom_metrics: []\n\ndataset:\n\n  cache_load: False\n\n  cache_save: False\n\n  dir: ./datasets\n\n  edge_dim: 128\n\n  edge_encoder: True\n\n  edge_encoder_bn: False\n\n  edge_encoder_name: Bond\n\n  edge_encoder_num_types: 0\n\n  edge_message_ratio: 0.8\n\n  edge_negative_sampling_ratio: 1.0\n\n  edge_train_mode: all\n\n  encoder: True\n\n  encoder_bn: True\n\n  encoder_dim: 128\n\n  encoder_name: db\n\n  format: OGB\n\n  infer_link_label: None\n\n  label_column: none\n\n  label_table: none\n\n  location: local\n\n  name: ogbg-molhiv\n\n  node_encoder: True\n\n  node_encoder_bn: False\n\n  node_encoder_name: Atom+LapPE\n\n  node_encoder_num_types: 0\n\n  remove_feature: False\n\n  resample_disjoint: False\n\n  resample_negative: False\n\n  shuffle_split: True\n\n  slic_compactness: 10\n\n  split: [0.8, 0.1, 0.1]\n\n  split_dir: ./splits\n\n  split_index: 0\n\n  split_mode: standard\n\n  task: graph\n\n  task_type: classification\n\n  to_undirected: False\n\n  transductive: False\n\n  transform: none\n\n  tu_simple: True\n\ndevices: 1\n\nexample_arg: example\n\nexample_group:\n\n  example_arg: example\n\ngnn:\n\n  act: relu\n\n  agg: mean\n\n  att_final_linear: False\n\n  att_final_linear_bn: False\n\n  att_heads: 1\n\n  batchnorm: True\n\n  clear_feature: True\n\n  dim_edge: 64\n\n  dim_inner: 64\n\n  dropout: 0.0\n\n  head: san_graph\n\n  keep_edge: 0.5\n\n  l2norm: True\n\n  layer_type: generalconv\n\n  layers_mp: 2\n\n  layers_post_mp: 3\n\n  layers_pre_mp: 0\n\n  msg_direction: single\n\n  normalize_adj: False\n\n  residual: False\n\n  self_msg: concat\n\n  skip_every: 1\n\n  stage_type: stack\n\ngpu_mem: False\n\ngraphormer:\n\n  attention_dropout: 0.0\n\n  dropout: 0.0\n\n  embed_dim: 80\n\n  input_dropout: 0.0\n\n  mlp_dropout: 0.0\n\n  num_heads: 4\n\n  num_layers: 6\n\n  use_graph_token: True\n\ngt:\n\n  attn_dropout: 0.5\n\n  batch_norm: True\n\n  bigbird:\n\n    add_cross_attention: False\n\n    attention_type: block_sparse\n\n    block_size: 3\n\n    chunk_size_feed_forward: 0\n\n    hidden_act: relu\n\n    is_decoder: False\n\n    layer_norm_eps: 1e-06\n\n    max_position_embeddings: 128\n\n    num_random_blocks: 3\n\n    use_bias: False\n\n  dim_hidden: 64\n\n  dropout: 0.05\n\n  full_graph: True\n\n  gamma: 1e-05\n\n  layer_norm: False\n\n  layer_type: CustomGatedGCN+Transformer\n\n  layers: 10\n\n  n_heads: 4\n\n  pna_degrees: []\n\n  residual: True\n\nmem:\n\n  inplace: False\n\nmetric_agg: argmax\n\nmetric_best: auc\n\nmodel:\n\n  edge_decoding: dot\n\n  graph_pooling: mean\n\n  loss_fun: cross_entropy\n\n  match_upper: True\n\n  size_average: mean\n\n  thresh: 0.5\n\n  type: GPSModel\n\nname_tag: \n\nnum_threads: 6\n\nnum_workers: 0\n\noptim:\n\n  base_lr: 0.0001\n\n  batch_accumulation: 1\n\n  clip_grad_norm: True\n\n  clip_grad_norm_value: 1.0\n\n  lr_decay: 0.1\n\n  max_epoch: 100\n\n  min_lr: 0.0\n\n  momentum: 0.9\n\n  num_warmup_epochs: 5\n\n  optimizer: adamW\n\n  reduce_factor: 0.1\n\n  schedule_patience: 10\n\n  scheduler: cosine_with_warmup\n\n  steps: [30, 60, 90]\n\n  weight_decay: 1e-05\n\nout_dir: results/ogbg-molhiv-GPS\n\nposenc_ElstaticSE:\n\n  dim_pe: 16\n\n  enable: False\n\n  kernel:\n\n    times: []\n\n    times_func: range(10)\n\n  layers: 3\n\n  model: none\n\n  n_heads: 4\n\n  pass_as_var: False\n\n  post_layers: 0\n\n  raw_norm_type: none\n\nposenc_EquivStableLapPE:\n\n  eigen:\n\n    eigvec_norm: L2\n\n    laplacian_norm: sym\n\n    max_freqs: 10\n\n  enable: False\n\n  raw_norm_type: none\n\nposenc_GraphormerBias:\n\n  dim_pe: 0\n\n  enable: False\n\n  node_degrees_only: False\n\n  num_in_degrees: None\n\n  num_out_degrees: None\n\n  num_spatial_types: None\n\nposenc_HKdiagSE:\n\n  dim_pe: 16\n\n  enable: False\n\n  kernel:\n\n    times: []\n\n    times_func: \n\n  layers: 3\n\n  model: none\n\n  n_heads: 4\n\n  pass_as_var: False\n\n  post_layers: 0\n\n  raw_norm_type: none\n\nposenc_LapPE:\n\n  dim_pe: 8\n\n  eigen:\n\n    eigvec_norm: L2\n\n    laplacian_norm: none\n\n    max_freqs: 8\n\n  enable: True\n\n  layers: 2\n\n  model: DeepSet\n\n  n_heads: 4\n\n  pass_as_var: False\n\n  post_layers: 0\n\n  raw_norm_type: none\n\nposenc_RWSE:\n\n  dim_pe: 16\n\n  enable: False\n\n  kernel:\n\n    times: []\n\n    times_func: \n\n  layers: 3\n\n  model: none\n\n  n_heads: 4\n\n  pass_as_var: False\n\n  post_layers: 0\n\n  raw_norm_type: none\n\nposenc_SignNet:\n\n  dim_pe: 16\n\n  eigen:\n\n    eigvec_norm: L2\n\n    laplacian_norm: sym\n\n    max_freqs: 10\n\n  enable: False\n\n  layers: 3\n\n  model: none\n\n  n_heads: 4\n\n  pass_as_var: False\n\n  phi_hidden_dim: 64\n\n  phi_out_dim: 4\n\n  post_layers: 0\n\n  raw_norm_type: none\n\npretrained:\n\n  dir: \n\n  freeze_main: False\n\n  reset_prediction_head: True\n\nprint: both\n\nround: 5\n\nrun_dir: results/ogbg-molhiv-GPS/0\n\nrun_id: 0\n\nrun_multiple_splits: []\n\nseed: 0\n\nshare:\n\n  dim_in: 9\n\n  dim_out: 2\n\n  num_splits: 3\n\ntensorboard_agg: True\n\ntensorboard_each_run: False\n\ntrain:\n\n  auto_resume: False\n\n  batch_size: 32\n\n  ckpt_best: False\n\n  ckpt_clean: True\n\n  ckpt_period: 100\n\n  enable_ckpt: True\n\n  epoch_resume: -1\n\n  eval_period: 1\n\n  iter_per_epoch: 32\n\n  mode: custom\n\n  neighbor_sizes: [20, 15, 10, 5]\n\n  node_per_graph: 32\n\n  radius: extend\n\n  sample_node: False\n\n  sampler: full_batch\n\n  skip_train_eval: False\n\n  walk_length: 4\n\nval:\n\n  node_per_graph: 32\n\n  radius: extend\n\n  sample_node: False\n\n  sampler: full_batch\n\nview_emb: False\n\nwandb:\n\n  entity: gtransformers\n\n  name: \n\n  project: molhiv\n\n  use: True\n\nNum parameters: 559945\n\nStart from epoch 0\n\nKey posenc_GraphormerBias.num_spatial_types with value <class 'NoneType'> is not a valid type; valid types: {<class 'list'>, <class 'str'>, <class 'bool'>, <class 'int'>, <class 'tuple'>, <class 'float'>}\n\nKey posenc_GraphormerBias.num_in_degrees with value <class 'NoneType'> is not a valid type; valid types: {<class 'list'>, <class 'str'>, <class 'bool'>, <class 'int'>, <class 'tuple'>, <class 'float'>}\n\nKey posenc_GraphormerBias.num_out_degrees with value <class 'NoneType'> is not a valid type; valid types: {<class 'list'>, <class 'str'>, <class 'bool'>, <class 'int'>, <class 'tuple'>, <class 'float'>}\n\ntrain: {'epoch': 0, 'time_epoch': 84.40366, 'eta': 8355.96265, 'eta_hours': 2.3211, 'loss': 0.79160294, 'lr': 0.0, 'params': 559945, 'time_iter': 0.08202, 'accuracy': 0.03748, 'precision': 0.03745, 'recall': 1.0, 'f1': 0.07219, 'auc': 0.55785}\n\n...computing epoch stats took: 0.09s\n\nval: {'epoch': 0, 'time_epoch': 4.67517, 'loss': 0.79690719, 'lr': 0, 'params': 559945, 'time_iter': 0.03624, 'accuracy': 0.01969, 'precision': 0.01969, 'recall': 1.0, 'f1': 0.03863, 'auc': 0.53566}\n\n...computing epoch stats took: 0.01s\n\ntest: {'epoch': 0, 'time_epoch': 4.69167, 'loss': 0.79430784, 'lr': 0, 'params': 559945, 'time_iter': 0.03637, 'accuracy': 0.03161, 'precision': 0.03161, 'recall': 1.0, 'f1': 0.06128, 'auc': 0.57793}\n\n...computing epoch stats took: 0.01s\n\n> Epoch 0: took 93.9s (avg 93.9s) | Best so far: epoch 0\ttrain_loss: 0.7916 train_auc: 0.5578\tval_loss: 0.7969 val_auc: 0.5357\ttest_loss: 0.7943 test_auc: 0.5779\n\ntrain: {'epoch': 1, 'time_epoch': 85.05092, 'eta': 8303.27479, 'eta_hours': 2.30647, 'loss': 0.49213173, 'lr': 2e-05, 'params': 559945, 'time_iter': 0.08265, 'accuracy': 0.76715, 'precision': 0.04719, 'recall': 0.27192, 'f1': 0.08042, 'auc': 0.56012}\n\n...computing epoch stats took: 0.08s\n\nval: {'epoch': 1, 'time_epoch': 4.29309, 'loss': 0.2666023, 'lr': 0, 'params': 559945, 'time_iter': 0.03328, 'accuracy': 0.97788, 'precision': 0.1875, 'recall': 0.03704, 'f1': 0.06186, 'auc': 0.65604}\n\n...computing epoch stats took: 0.02s\n\ntest: {'epoch': 1, 'time_epoch': 4.12836, 'loss': 0.26772595, 'lr': 0, 'params': 559945, 'time_iter': 0.032, 'accuracy': 0.96791, 'precision': 0.4375, 'recall': 0.05385, 'f1': 0.09589, 'auc': 0.65993}\n\n...computing epoch stats took: 0.01s\n\n> Epoch 1: took 93.6s (avg 93.8s) | Best so far: epoch 1\ttrain_loss: 0.4921 train_auc: 0.5601\tval_loss: 0.2666 val_auc: 0.6560\ttest_loss: 0.2677 test_auc: 0.6599\n\ntrain: {'epoch': 2, 'time_epoch': 82.26323, 'eta': 8138.87595, 'eta_hours': 2.2608, 'loss': 0.19033828, 'lr': 4e-05, 'params': 559945, 'time_iter': 0.07994, 'accuracy': 0.96213, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.61406}\n\n...computing epoch stats took: 0.07s\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\nval: {'epoch': 2, 'time_epoch': 4.00851, 'loss': 0.10388157, 'lr': 0, 'params': 559945, 'time_iter': 0.03107, 'accuracy': 0.98031, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.67792}\n\n...computing epoch stats took: 0.02s\n\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n\n  _warn_prf(average, modifier, msg_start, len(result))\n\ntest: {'epoch': 2, 'time_epoch': 3.92167, 'loss': 0.13049798, 'lr': 0, 'params': 559945, 'time_iter': 0.0304, 'accuracy': 0.96839, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'auc': 0.73954}\n\n...computing epoch stats took: 0.01s\n\n> Epoch 2: took 90.3s (avg 92.6s) | Best so far: epoch 2\ttrain_loss: 0.1903 train_auc: 0.6141\tval_loss: 0.1039 val_auc: 0.6779\ttest_loss: 0.1305 test_auc: 0.7395\n\ntrain: {'epoch': 3, 'time_epoch': 81.60161, 'eta': 7999.66604, 'eta_hours': 2.22213, 'loss': 0.14729549, 'lr': 6e-05, 'params': 559945, 'time_iter': 0.0793, 'accuracy': 0.96246, 'precision': 0.44444, 'recall': 0.00974, 'f1': 0.01906, 'auc': 0.70096}\n\nval: {'epoch': 3, 'time_epoch': 4.08507, 'loss': 0.08416821, 'lr': 0, 'params': 559945, 'time_iter': 0.03167, 'accuracy': 0.98177, 'precision': 0.8, 'recall': 0.09877, 'f1': 0.17582, 'auc': 0.71341}\n\ntest: {'epoch': 3, 'time_epoch': 4.06441, 'loss': 0.11695114, 'lr': 0, 'params': 559945, 'time_iter': 0.03151, 'accuracy': 0.97009, 'precision': 0.73333, 'recall': 0.08462, 'f1': 0.15172, 'auc': 0.74161}\n\n> Epoch 3: took 89.9s (avg 91.9s) | Best so far: epoch 3\ttrain_loss: 0.1473 train_auc: 0.7010\tval_loss: 0.0842 val_auc: 0.7134\ttest_loss: 0.1170 test_auc: 0.7416\n\ntrain: {'epoch': 4, 'time_epoch': 90.12413, 'eta': 8045.42749, 'eta_hours': 2.23484, 'loss': 0.139009, 'lr': 8e-05, 'params': 559945, 'time_iter': 0.08758, 'accuracy': 0.96365, 'precision': 0.65254, 'recall': 0.0625, 'f1': 0.11407, 'auc': 0.73357}\n\nval: {'epoch': 4, 'time_epoch': 4.46761, 'loss': 0.07925676, 'lr': 0, 'params': 559945, 'time_iter': 0.03463, 'accuracy': 0.98128, 'precision': 0.59091, 'recall': 0.16049, 'f1': 0.25243, 'auc': 0.77718}\n\ntest: {'epoch': 4, 'time_epoch': 4.39571, 'loss': 0.11466429, 'lr': 0, 'params': 559945, 'time_iter': 0.03408, 'accuracy': 0.97204, 'precision': 0.71429, 'recall': 0.19231, 'f1': 0.30303, 'auc': 0.75446}\n\n> Epoch 4: took 99.1s (avg 93.4s) | Best so far: epoch 4\ttrain_loss: 0.1390 train_auc: 0.7336\tval_loss: 0.0793 val_auc: 0.7772\ttest_loss: 0.1147 test_auc: 0.7545\n\ntrain: {'epoch': 5, 'time_epoch': 91.05291, 'eta': 8060.44458, 'eta_hours': 2.23901, 'loss': 0.13337819, 'lr': 0.0001, 'params': 559945, 'time_iter': 0.08849, 'accuracy': 0.96459, 'precision': 0.66667, 'recall': 0.10877, 'f1': 0.18702, 'auc': 0.76058}\n\nval: {'epoch': 5, 'time_epoch': 4.40095, 'loss': 0.07868337, 'lr': 0, 'params': 559945, 'time_iter': 0.03412, 'accuracy': 0.98201, 'precision': 0.65217, 'recall': 0.18519, 'f1': 0.28846, 'auc': 0.80137}\n\ntest: {'epoch': 5, 'time_epoch': 4.40035, 'loss': 0.11791497, 'lr': 0, 'params': 559945, 'time_iter': 0.03411, 'accuracy': 0.97277, 'precision': 0.70455, 'recall': 0.23846, 'f1': 0.35632, 'auc': 0.73834}\n\n> Epoch 5: took 100.0s (avg 94.5s) | Best so far: epoch 5\ttrain_loss: 0.1334 train_auc: 0.7606\tval_loss: 0.0787 val_auc: 0.8014\ttest_loss: 0.1179 test_auc: 0.7383\n\ntrain: {'epoch': 6, 'time_epoch': 91.18149, 'eta': 8046.86421, 'eta_hours': 2.23524, 'loss': 0.12866286, 'lr': 9.997e-05, 'params': 559945, 'time_iter': 0.08861, 'accuracy': 0.96541, 'precision': 0.66786, 'recall': 0.15179, 'f1': 0.24735, 'auc': 0.78484}\n\nval: {'epoch': 6, 'time_epoch': 4.38265, 'loss': 0.08027428, 'lr': 0, 'params': 559945, 'time_iter': 0.03397, 'accuracy': 0.98152, 'precision': 0.6, 'recall': 0.18519, 'f1': 0.28302, 'auc': 0.7706}\n\ntest: {'epoch': 6, 'time_epoch': 4.36296, 'loss': 0.11583503, 'lr': 0, 'params': 559945, 'time_iter': 0.03382, 'accuracy': 0.97228, 'precision': 0.7, 'recall': 0.21538, 'f1': 0.32941, 'auc': 0.74859}\n\n> Epoch 6: took 100.1s (avg 95.3s) | Best so far: epoch 5\ttrain_loss: 0.1334 train_auc: 0.7606\tval_loss: 0.0787 val_auc: 0.8014\ttest_loss: 0.1179 test_auc: 0.7383\n\ntrain: {'epoch': 7, 'time_epoch': 91.64785, 'eta': 8019.24666, 'eta_hours': 2.22757, 'loss': 0.12493481, 'lr': 9.989e-05, 'params': 559945, 'time_iter': 0.08906, 'accuracy': 0.9652, 'precision': 0.62254, 'recall': 0.17938, 'f1': 0.27851, 'auc': 0.79837}\n\nval: {'epoch': 7, 'time_epoch': 4.38963, 'loss': 0.08174635, 'lr': 0, 'params': 559945, 'time_iter': 0.03403, 'accuracy': 0.97812, 'precision': 0.40426, 'recall': 0.23457, 'f1': 0.29688, 'auc': 0.80875}\n\ntest: {'epoch': 7, 'time_epoch': 4.43311, 'loss': 0.12535343, 'lr': 0, 'params': 559945, 'time_iter': 0.03437, 'accuracy': 0.9662, 'precision': 0.42857, 'recall': 0.20769, 'f1': 0.27979, 'auc': 0.74739}\n\n> Epoch 7: took 100.6s (avg 96.0s) | Best so far: epoch 7\ttrain_loss: 0.1249 train_auc: 0.7984\tval_loss: 0.0817 val_auc: 0.8087\ttest_loss: 0.1254 test_auc: 0.7474\n\ntrain: {'epoch': 8, 'time_epoch': 91.73049, 'eta': 7978.23583, 'eta_hours': 2.21618, 'loss': 0.12176636, 'lr': 9.975e-05, 'params': 559945, 'time_iter': 0.08915, 'accuracy': 0.96562, 'precision': 0.6211, 'recall': 0.21023, 'f1': 0.31413, 'auc': 0.81242}\n\nval: {'epoch': 8, 'time_epoch': 4.34453, 'loss': 0.073936, 'lr': 0, 'params': 559945, 'time_iter': 0.03368, 'accuracy': 0.98177, 'precision': 0.6, 'recall': 0.22222, 'f1': 0.32432, 'auc': 0.80337}\n\ntest: {'epoch': 8, 'time_epoch': 4.2437, 'loss': 0.12585412, 'lr': 0, 'params': 559945, 'time_iter': 0.0329, 'accuracy': 0.96864, 'precision': 0.53846, 'recall': 0.05385, 'f1': 0.0979, 'auc': 0.72682}\n\n> Epoch 8: took 100.5s (avg 96.5s) | Best so far: epoch 7\ttrain_loss: 0.1249 train_auc: 0.7984\tval_loss: 0.0817 val_auc: 0.8087\ttest_loss: 0.1254 test_auc: 0.7474\n\ntrain: {'epoch': 9, 'time_epoch': 87.84393, 'eta': 7892.10202, 'eta_hours': 2.19225, 'loss': 0.1173127, 'lr': 9.956e-05, 'params': 559945, 'time_iter': 0.08537, 'accuracy': 0.96626, 'precision': 0.63319, 'recall': 0.23539, 'f1': 0.3432, 'auc': 0.83017}\n\nval: {'epoch': 9, 'time_epoch': 4.41907, 'loss': 0.07359199, 'lr': 0, 'params': 559945, 'time_iter': 0.03426, 'accuracy': 0.98152, 'precision': 0.57576, 'recall': 0.23457, 'f1': 0.33333, 'auc': 0.79728}\n\ntest: {'epoch': 9, 'time_epoch': 4.43283, 'loss': 0.11204209, 'lr': 0, 'params': 559945, 'time_iter': 0.03436, 'accuracy': 0.97204, 'precision': 0.7027, 'recall': 0.2, 'f1': 0.31138, 'auc': 0.76026}\n\n> Epoch 9: took 96.9s (avg 96.5s) | Best so far: epoch 7\ttrain_loss: 0.1249 train_auc: 0.7984\tval_loss: 0.0817 val_auc: 0.8087\ttest_loss: 0.1254 test_auc: 0.7474\n\ntrain: {'epoch': 10, 'time_epoch': 90.93576, 'eta': 7830.67297, 'eta_hours': 2.17519, 'loss': 0.11654024, 'lr': 9.932e-05, 'params': 559945, 'time_iter': 0.08837, 'accuracy': 0.96714, 'precision': 0.65895, 'recall': 0.25406, 'f1': 0.36673, 'auc': 0.82992}\n\nval: {'epoch': 10, 'time_epoch': 4.38048, 'loss': 0.08608254, 'lr': 0, 'params': 559945, 'time_iter': 0.03396, 'accuracy': 0.9735, 'precision': 0.31579, 'recall': 0.2963, 'f1': 0.30573, 'auc': 0.80887}\n\ntest: {'epoch': 10, 'time_epoch': 4.40179, 'loss': 0.13245251, 'lr': 0, 'params': 559945, 'time_iter': 0.03412, 'accuracy': 0.96159, 'precision': 0.32051, 'recall': 0.19231, 'f1': 0.24038, 'auc': 0.75078}\n\n> Epoch 10: took 99.9s (avg 96.8s) | Best so far: epoch 10\ttrain_loss: 0.1165 train_auc: 0.8299\tval_loss: 0.0861 val_auc: 0.8089\ttest_loss: 0.1325 test_auc: 0.7508\n\ntrain: {'epoch': 11, 'time_epoch': 90.52255, 'eta': 7761.29594, 'eta_hours': 2.15592, 'loss': 0.1147073, 'lr': 9.902e-05, 'params': 559945, 'time_iter': 0.08797, 'accuracy': 0.96821, 'precision': 0.68526, 'recall': 0.27922, 'f1': 0.39677, 'auc': 0.83273}\n\nval: {'epoch': 11, 'time_epoch': 4.36263, 'loss': 0.06772218, 'lr': 0, 'params': 559945, 'time_iter': 0.03382, 'accuracy': 0.98177, 'precision': 0.60714, 'recall': 0.20988, 'f1': 0.31193, 'auc': 0.84713}\n\ntest: {'epoch': 11, 'time_epoch': 4.39689, 'loss': 0.11511386, 'lr': 0, 'params': 559945, 'time_iter': 0.03408, 'accuracy': 0.97228, 'precision': 0.76667, 'recall': 0.17692, 'f1': 0.2875, 'auc': 0.76493}\n\n> Epoch 11: took 99.4s (avg 97.0s) | Best so far: epoch 11\ttrain_loss: 0.1147 train_auc: 0.8327\tval_loss: 0.0677 val_auc: 0.8471\ttest_loss: 0.1151 test_auc: 0.7649\n\ntrain: {'epoch': 12, 'time_epoch': 91.41145, 'eta': 7694.6145, 'eta_hours': 2.13739, 'loss': 0.1118576, 'lr': 9.867e-05, 'params': 559945, 'time_iter': 0.08884, 'accuracy': 0.96833, 'precision': 0.67086, 'recall': 0.30276, 'f1': 0.41723, 'auc': 0.84482}\n\nval: {'epoch': 12, 'time_epoch': 4.35081, 'loss': 0.07778883, 'lr': 0, 'params': 559945, 'time_iter': 0.03373, 'accuracy': 0.98079, 'precision': 0.52, 'recall': 0.32099, 'f1': 0.39695, 'auc': 0.8176}\n\ntest: {'epoch': 12, 'time_epoch': 4.35684, 'loss': 0.11871899, 'lr': 0, 'params': 559945, 'time_iter': 0.03377, 'accuracy': 0.96937, 'precision': 0.52564, 'recall': 0.31538, 'f1': 0.39423, 'auc': 0.76064}\n\n> Epoch 12: took 100.3s (avg 97.3s) | Best so far: epoch 11\ttrain_loss: 0.1147 train_auc: 0.8327\tval_loss: 0.0677 val_auc: 0.8471\ttest_loss: 0.1151 test_auc: 0.7649\n\ntrain: {'epoch': 13, 'time_epoch': 90.82026, 'eta': 7620.76865, 'eta_hours': 2.11688, 'loss': 0.11112533, 'lr': 9.826e-05, 'params': 559945, 'time_iter': 0.08826, 'accuracy': 0.96824, 'precision': 0.6781, 'recall': 0.28896, 'f1': 0.40524, 'auc': 0.85051}\n\nval: {'epoch': 13, 'time_epoch': 4.33339, 'loss': 0.06962319, 'lr': 0, 'params': 559945, 'time_iter': 0.03359, 'accuracy': 0.98347, 'precision': 0.78261, 'recall': 0.22222, 'f1': 0.34615, 'auc': 0.83401}\n\ntest: {'epoch': 13, 'time_epoch': 4.52311, 'loss': 0.12011549, 'lr': 0, 'params': 559945, 'time_iter': 0.03506, 'accuracy': 0.97131, 'precision': 0.83333, 'recall': 0.11538, 'f1': 0.2027, 'auc': 0.759}\n\n> Epoch 13: took 99.8s (avg 97.5s) | Best so far: epoch 11\ttrain_loss: 0.1147 train_auc: 0.8327\tval_loss: 0.0677 val_auc: 0.8471\ttest_loss: 0.1151 test_auc: 0.7649\n\n^C\n\nTraceback (most recent call last):\n\n  File \"/kaggle/working/GraphGPS/main.py\", line 166, in <module>\n\n    train_dict[cfg.train.mode](loggers, loaders, model, optimizer,\n\n  File \"/kaggle/working/GraphGPS/graphgps/train/custom_train.py\", line 121, in custom_train\n\n    train_epoch(loggers[0], loaders[0], model, optimizer, scheduler,\n\n  File \"/kaggle/working/GraphGPS/graphgps/train/custom_train.py\", line 23, in train_epoch\n\n    pred, true = model(batch)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n\n    return self._call_impl(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n\n    return forward_call(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch_geometric/graphgym/model_builder.py\", line 24, in forward\n\n    return self.model(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n\n    return self._call_impl(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n\n    return forward_call(*args, **kwargs)\n\n  File \"/kaggle/working/GraphGPS/graphgps/network/gps_model.py\", line 107, in forward\n\n    batch = module(batch)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n\n    return self._call_impl(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n\n    return forward_call(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 219, in forward\n\n    input = module(input)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n\n    return self._call_impl(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n\n    return forward_call(*args, **kwargs)\n\n  File \"/kaggle/working/GraphGPS/graphgps/layer/gps_layer.py\", line 167, in forward\n\n    local_out = self.local_model(Batch(batch=batch,\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n\n    return self._call_impl(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n\n    return forward_call(*args, **kwargs)\n\n  File \"/kaggle/working/GraphGPS/graphgps/layer/gatedgcn_layer.py\", line 61, in forward\n\n    Ex = self.E(x)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n\n    return self._call_impl(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n\n    return forward_call(*args, **kwargs)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py\", line 147, in forward\n\n    return F.linear(x, self.weight, self.bias)\n\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1716, in __getattr__\n\n    def __getattr__(self, name: str) -> Any:\n\nKeyboardInterrupt\n"}]},{"cell_type":"markdown","source":"# Our Model - WeightedGraphGPS","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working\n!rm -rf WeightedGraphGPS","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:57:40.653408Z","iopub.execute_input":"2024-09-24T13:57:40.653870Z","iopub.status.idle":"2024-09-24T13:57:41.678623Z","shell.execute_reply.started":"2024-09-24T13:57:40.653830Z","shell.execute_reply":"2024-09-24T13:57:41.677551Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"#clone for our project\n%cd /kaggle/working\n!git clone --branch log_scalar https://github.com/omertalmi5/WeightedGraphGPS.git","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:57:41.680999Z","iopub.execute_input":"2024-09-24T13:57:41.681467Z","iopub.status.idle":"2024-09-24T13:57:43.653774Z","shell.execute_reply.started":"2024-09-24T13:57:41.681416Z","shell.execute_reply":"2024-09-24T13:57:43.652628Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'WeightedGraphGPS'...\nremote: Enumerating objects: 548, done.\u001b[K\nremote: Counting objects: 100% (340/340), done.\u001b[K\nremote: Compressing objects: 100% (118/118), done.\u001b[K\nremote: Total 548 (delta 273), reused 222 (delta 222), pack-reused 208 (from 1)\u001b[K\nReceiving objects: 100% (548/548), 12.93 MiB | 29.30 MiB/s, done.\nResolving deltas: 100% (355/355), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:09:17.553323Z","iopub.execute_input":"2024-09-24T13:09:17.553634Z","iopub.status.idle":"2024-09-24T13:09:29.735005Z","shell.execute_reply.started":"2024-09-24T13:09:17.553596Z","shell.execute_reply":"2024-09-24T13:09:29.733626Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.14.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"!wandb login 5f1c292fde40243d281618fe88d712346cfe90d7","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:09:29.736795Z","iopub.execute_input":"2024-09-24T13:09:29.737182Z","iopub.status.idle":"2024-09-24T13:09:33.478961Z","shell.execute_reply.started":"2024-09-24T13:09:29.737141Z","shell.execute_reply":"2024-09-24T13:09:33.477796Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n# Set the environment variable\nos.environ['USERNAME'] = \"omertalmi-tel-aviv-university\"","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:09:33.480709Z","iopub.execute_input":"2024-09-24T13:09:33.481070Z","iopub.status.idle":"2024-09-24T13:09:33.485941Z","shell.execute_reply.started":"2024-09-24T13:09:33.481032Z","shell.execute_reply":"2024-09-24T13:09:33.485003Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/WeightedGraphGPS\n!WANDB_MODE=online python main.py --cfg configs/GPS/ogbg-molhiv-GPS.yaml wandb.use True wandb.entity $USERNAME","metadata":{"execution":{"iopub.status.busy":"2024-09-24T13:57:52.351092Z","iopub.execute_input":"2024-09-24T13:57:52.351513Z","iopub.status.idle":"2024-09-24T14:02:34.059042Z","shell.execute_reply.started":"2024-09-24T13:57:52.351471Z","shell.execute_reply":"2024-09-24T14:02:34.057989Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"/kaggle/working/WeightedGraphGPS\n[*] Run ID 0: seed=0, split_index=0\n    Starting now: 2024-09-24 13:58:09.568139\nDownloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\nDownloaded 0.00 GB: 100%|█████████████████████████| 3/3 [00:00<00:00,  3.88it/s]\nExtracting ./datasets/hiv.zip\nProcessing...\nLoading necessary files...\nThis might take a while.\nProcessing graphs...\n100%|██████████████████████████████████| 41127/41127 [00:00<00:00, 66797.30it/s]\nConverting graphs into PyG objects...\n100%|██████████████████████████████████| 41127/41127 [00:02<00:00, 18570.04it/s]\nSaving...\nDone!\n/opt/conda/lib/python3.10/site-packages/ogb/graphproppred/dataset_pyg.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.data, self.slices = torch.load(self.processed_paths[0])\n[*] Loaded dataset 'ogbg-molhiv' from 'OGB':\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n  Data(edge_index=[2, 2259376], edge_attr=[2259376, 3], x=[1049163, 9], y=[41127, 1], num_nodes=1049163)\n  undirected: True\n  num graphs: 41127\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n  avg num_nodes/graph: 25\n  num node features: 9\n  num edge features: 3\n  num tasks: 1\n  num classes: 2\nPrecomputing Positional Encoding statistics: ['LapPE'] for all graphs...\n  ...estimated to be undirected: True\n100%|████████████████████████████████████| 41127/41127 [00:53<00:00, 775.61it/s]\nDone! Took 00:00:54.56\nGraphGymModule(\n  (model): GPSModel(\n    (encoder): FeatureEncoder(\n      (node_encoder): Concat2NodeEncoder(\n        (encoder1): AtomEncoder(\n          (atom_embedding_list): ModuleList(\n            (0): Embedding(119, 56)\n            (1): Embedding(5, 56)\n            (2-3): 2 x Embedding(12, 56)\n            (4): Embedding(10, 56)\n            (5-6): 2 x Embedding(6, 56)\n            (7-8): 2 x Embedding(2, 56)\n          )\n        )\n        (encoder2): LapPENodeEncoder(\n          (linear_A): Linear(in_features=2, out_features=16, bias=True)\n          (pe_encoder): Sequential(\n            (0): ReLU()\n            (1): Linear(in_features=16, out_features=8, bias=True)\n            (2): ReLU()\n          )\n        )\n      )\n      (edge_encoder): BondEncoder(\n        (bond_embedding_list): ModuleList(\n          (0): Embedding(5, 64)\n          (1): Embedding(6, 64)\n          (2): Embedding(2, 64)\n        )\n      )\n    )\n    (layers): Sequential(\n      (0): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (1): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (2): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (3): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (4): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (5): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (6): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (7): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (8): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n      (9): GPSLayer(\n        summary: dim_h=64, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (norm1_local): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.05, inplace=False)\n        (dropout_attn): Dropout(p=0.05, inplace=False)\n        (ff_linear1): Linear(in_features=64, out_features=128, bias=True)\n        (ff_linear2): Linear(in_features=128, out_features=64, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.05, inplace=False)\n        (ff_dropout2): Dropout(p=0.05, inplace=False)\n      )\n    )\n    (post_mp): SANGraphHead(\n      (FC_layers): ModuleList(\n        (0): Linear(in_features=64, out_features=32, bias=True)\n        (1): Linear(in_features=32, out_features=16, bias=True)\n        (2): Linear(in_features=16, out_features=1, bias=True)\n      )\n      (activation): ReLU()\n    )\n  )\n)\naccelerator: cuda\nbenchmark: False\nbn:\n  eps: 1e-05\n  mom: 0.1\ncfg_dest: config.yaml\ncustom_metrics: []\ndataset:\n  cache_load: False\n  cache_save: False\n  dir: ./datasets\n  edge_dim: 128\n  edge_encoder: True\n  edge_encoder_bn: False\n  edge_encoder_name: Bond\n  edge_encoder_num_types: 0\n  edge_message_ratio: 0.8\n  edge_negative_sampling_ratio: 1.0\n  edge_train_mode: all\n  encoder: True\n  encoder_bn: True\n  encoder_dim: 128\n  encoder_name: db\n  format: OGB\n  infer_link_label: None\n  label_column: none\n  label_table: none\n  location: local\n  name: ogbg-molhiv\n  node_encoder: True\n  node_encoder_bn: False\n  node_encoder_name: Atom+LapPE\n  node_encoder_num_types: 0\n  remove_feature: False\n  resample_disjoint: False\n  resample_negative: False\n  shuffle_split: True\n  slic_compactness: 10\n  split: [0.8, 0.1, 0.1]\n  split_dir: ./splits\n  split_index: 0\n  split_mode: standard\n  task: graph\n  task_type: classification\n  to_undirected: False\n  transductive: False\n  transform: none\n  tu_simple: True\ndevices: 1\nexample_arg: example\nexample_group:\n  example_arg: example\ngnn:\n  act: relu\n  agg: mean\n  att_final_linear: False\n  att_final_linear_bn: False\n  att_heads: 1\n  batchnorm: True\n  clear_feature: True\n  dim_edge: 64\n  dim_inner: 64\n  dropout: 0.0\n  head: san_graph\n  keep_edge: 0.5\n  l2norm: True\n  layer_type: generalconv\n  layers_mp: 2\n  layers_post_mp: 3\n  layers_pre_mp: 0\n  msg_direction: single\n  normalize_adj: False\n  residual: False\n  self_msg: concat\n  skip_every: 1\n  stage_type: stack\ngpu_mem: False\ngraphormer:\n  attention_dropout: 0.0\n  dropout: 0.0\n  embed_dim: 80\n  input_dropout: 0.0\n  mlp_dropout: 0.0\n  num_heads: 4\n  num_layers: 6\n  use_graph_token: True\ngt:\n  attn_dropout: 0.5\n  batch_norm: True\n  bigbird:\n    add_cross_attention: False\n    attention_type: block_sparse\n    block_size: 3\n    chunk_size_feed_forward: 0\n    hidden_act: relu\n    is_decoder: False\n    layer_norm_eps: 1e-06\n    max_position_embeddings: 128\n    num_random_blocks: 3\n    use_bias: False\n  dim_hidden: 64\n  dropout: 0.05\n  full_graph: True\n  gamma: 1e-05\n  layer_norm: False\n  layer_type: CustomGatedGCN+Transformer\n  layers: 10\n  n_heads: 4\n  pna_degrees: []\n  residual: True\nmem:\n  inplace: False\nmetric_agg: argmax\nmetric_best: auc\nmodel:\n  edge_decoding: dot\n  graph_pooling: mean\n  loss_fun: cross_entropy\n  match_upper: True\n  size_average: mean\n  thresh: 0.5\n  type: GPSModel\nname_tag: \nnum_threads: 6\nnum_workers: 0\noptim:\n  base_lr: 0.0001\n  batch_accumulation: 1\n  clip_grad_norm: True\n  clip_grad_norm_value: 1.0\n  lr_decay: 0.1\n  max_epoch: 2\n  min_lr: 0.0\n  momentum: 0.9\n  num_warmup_epochs: 5\n  optimizer: adamW\n  reduce_factor: 0.1\n  schedule_patience: 10\n  scheduler: cosine_with_warmup\n  steps: [30, 60, 90]\n  weight_decay: 1e-05\nout_dir: results/ogbg-molhiv-GPS\nposenc_ElstaticSE:\n  dim_pe: 16\n  enable: False\n  kernel:\n    times: []\n    times_func: range(10)\n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_EquivStableLapPE:\n  eigen:\n    eigvec_norm: L2\n    laplacian_norm: sym\n    max_freqs: 10\n  enable: False\n  raw_norm_type: none\nposenc_GraphormerBias:\n  dim_pe: 0\n  enable: False\n  node_degrees_only: False\n  num_in_degrees: None\n  num_out_degrees: None\n  num_spatial_types: None\nposenc_HKdiagSE:\n  dim_pe: 16\n  enable: False\n  kernel:\n    times: []\n    times_func: \n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_LapPE:\n  dim_pe: 8\n  eigen:\n    eigvec_norm: L2\n    laplacian_norm: none\n    max_freqs: 8\n  enable: True\n  layers: 2\n  model: DeepSet\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_RWSE:\n  dim_pe: 16\n  enable: False\n  kernel:\n    times: []\n    times_func: \n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_SignNet:\n  dim_pe: 16\n  eigen:\n    eigvec_norm: L2\n    laplacian_norm: sym\n    max_freqs: 10\n  enable: False\n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  phi_hidden_dim: 64\n  phi_out_dim: 4\n  post_layers: 0\n  raw_norm_type: none\npretrained:\n  dir: \n  freeze_main: False\n  reset_prediction_head: True\nprint: both\nround: 5\nrun_dir: results/ogbg-molhiv-GPS/0\nrun_id: 0\nrun_multiple_splits: []\nseed: 0\nshare:\n  dim_in: 9\n  dim_out: 2\n  num_splits: 3\ntensorboard_agg: True\ntensorboard_each_run: False\ntrain:\n  auto_resume: False\n  batch_size: 32\n  ckpt_best: False\n  ckpt_clean: True\n  ckpt_period: 100\n  enable_ckpt: True\n  epoch_resume: -1\n  eval_period: 1\n  iter_per_epoch: 32\n  mode: custom\n  neighbor_sizes: [20, 15, 10, 5]\n  node_per_graph: 32\n  radius: extend\n  sample_node: False\n  sampler: full_batch\n  skip_train_eval: False\n  walk_length: 4\nval:\n  node_per_graph: 32\n  radius: extend\n  sample_node: False\n  sampler: full_batch\nview_emb: False\nwandb:\n  entity: omertalmi-tel-aviv-university\n  name: \n  project: molhiv\n  use: True\nNum parameters: 559955\nStart from epoch 0\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33momertalmi-\u001b[0m (\u001b[33momertalmi-tel-aviv-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.1\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/WeightedGraphGPS/wandb/run-20240924_135919-9h26ckr0\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mogbg-molhiv.GPS.CustomGatedGCN+Transformer+LapPE.r0\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/omertalmi-tel-aviv-university/molhiv\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/omertalmi-tel-aviv-university/molhiv/runs/9h26ckr0\u001b[0m\nKey posenc_GraphormerBias.num_spatial_types with value <class 'NoneType'> is not a valid type; valid types: {<class 'list'>, <class 'str'>, <class 'bool'>, <class 'int'>, <class 'tuple'>, <class 'float'>}\nKey posenc_GraphormerBias.num_in_degrees with value <class 'NoneType'> is not a valid type; valid types: {<class 'list'>, <class 'str'>, <class 'bool'>, <class 'int'>, <class 'tuple'>, <class 'float'>}\nKey posenc_GraphormerBias.num_out_degrees with value <class 'NoneType'> is not a valid type; valid types: {<class 'list'>, <class 'str'>, <class 'bool'>, <class 'int'>, <class 'tuple'>, <class 'float'>}\ntrain: {'epoch': 0, 'time_epoch': 88.04174, 'eta': 88.04174, 'eta_hours': 0.02446, 'loss': 0.79160831, 'lr': 0.0, 'params': 559955, 'time_iter': 0.08556, 'accuracy': 0.03748, 'precision': 0.03745, 'recall': 1.0, 'f1': 0.07219, 'auc': 0.55613}\n...computing epoch stats took: 0.08s\nval: {'epoch': 0, 'time_epoch': 4.79261, 'loss': 0.79657471, 'lr': 0, 'params': 559955, 'time_iter': 0.03715, 'accuracy': 0.01969, 'precision': 0.01969, 'recall': 1.0, 'f1': 0.03863, 'auc': 0.53256}\n...computing epoch stats took: 0.01s\ntest: {'epoch': 0, 'time_epoch': 5.1955, 'loss': 0.79389984, 'lr': 0, 'params': 559955, 'time_iter': 0.04028, 'accuracy': 0.03161, 'precision': 0.03161, 'recall': 1.0, 'f1': 0.06128, 'auc': 0.57868}\n...computing epoch stats took: 0.01s\n> Epoch 0: took 98.2s (avg 98.2s) | Best so far: epoch 0\ttrain_loss: 0.7916 train_auc: 0.5561\tval_loss: 0.7966 val_auc: 0.5326\ttest_loss: 0.7939 test_auc: 0.5787\ntrain: {'epoch': 1, 'time_epoch': 82.37029, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.49183594, 'lr': 2e-05, 'params': 559955, 'time_iter': 0.08005, 'accuracy': 0.76736, 'precision': 0.04762, 'recall': 0.27435, 'f1': 0.08115, 'auc': 0.55996}\n...computing epoch stats took: 0.08s\nval: {'epoch': 1, 'time_epoch': 4.06826, 'loss': 0.26775433, 'lr': 0, 'params': 559955, 'time_iter': 0.03154, 'accuracy': 0.97715, 'precision': 0.19048, 'recall': 0.04938, 'f1': 0.07843, 'auc': 0.66316}\n...computing epoch stats took: 0.01s\ntest: {'epoch': 1, 'time_epoch': 4.07665, 'loss': 0.26809919, 'lr': 0, 'params': 559955, 'time_iter': 0.0316, 'accuracy': 0.96839, 'precision': 0.5, 'recall': 0.06923, 'f1': 0.12162, 'auc': 0.67273}\n...computing epoch stats took: 0.01s\n> Epoch 1: took 90.7s (avg 94.4s) | Best so far: epoch 1\ttrain_loss: 0.4918 train_auc: 0.5600\tval_loss: 0.2678 val_auc: 0.6632\ttest_loss: 0.2681 test_auc: 0.6727\nAvg time per epoch: 94.41s\nTotal train loop time: 0.05h\n\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:       best/epoch ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:    best/test_auc ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:   best/test_loss █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:   best/train_auc ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:  best/train_loss █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:     best/val_auc ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:    best/val_loss █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:    test/accuracy ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:         test/auc ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:       test/epoch ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:          test/f1 ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:        test/loss █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:          test/lr ▁▁\n\u001b[34m\u001b[1mwandb\u001b[0m:      test/params ▁▁\n\u001b[34m\u001b[1mwandb\u001b[0m:   test/precision ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:      test/recall █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:  test/time_epoch █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:   test/time_iter █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:   train/accuracy ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:        train/auc ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:      train/epoch ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:        train/eta █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:  train/eta_hours █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:         train/f1 ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:       train/loss █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:         train/lr ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:     train/params ▁▁\n\u001b[34m\u001b[1mwandb\u001b[0m:  train/precision ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:     train/recall █▁\n\u001b[34m\u001b[1mwandb\u001b[0m: train/time_epoch █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:  train/time_iter █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:     val/accuracy ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:          val/auc ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:        val/epoch ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:           val/f1 ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:         val/loss █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:           val/lr ▁▁\n\u001b[34m\u001b[1mwandb\u001b[0m:       val/params ▁▁\n\u001b[34m\u001b[1mwandb\u001b[0m:    val/precision ▁█\n\u001b[34m\u001b[1mwandb\u001b[0m:       val/recall █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:   val/time_epoch █▁\n\u001b[34m\u001b[1mwandb\u001b[0m:    val/time_iter █▁\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:          best/epoch 1\n\u001b[34m\u001b[1mwandb\u001b[0m:       best/test_auc 0.67273\n\u001b[34m\u001b[1mwandb\u001b[0m:      best/test_loss 0.2681\n\u001b[34m\u001b[1mwandb\u001b[0m:      best/train_auc 0.55996\n\u001b[34m\u001b[1mwandb\u001b[0m:     best/train_loss 0.49184\n\u001b[34m\u001b[1mwandb\u001b[0m:        best/val_auc 0.66316\n\u001b[34m\u001b[1mwandb\u001b[0m:       best/val_loss 0.26775\n\u001b[34m\u001b[1mwandb\u001b[0m:      best_test_perf 0.67273\n\u001b[34m\u001b[1mwandb\u001b[0m:     best_train_perf 0.55996\n\u001b[34m\u001b[1mwandb\u001b[0m:       best_val_perf 0.66316\n\u001b[34m\u001b[1mwandb\u001b[0m: full_epoch_time_avg 94.40952\n\u001b[34m\u001b[1mwandb\u001b[0m: full_epoch_time_sum 188.81904\n\u001b[34m\u001b[1mwandb\u001b[0m:       test/accuracy 0.96839\n\u001b[34m\u001b[1mwandb\u001b[0m:            test/auc 0.67273\n\u001b[34m\u001b[1mwandb\u001b[0m:          test/epoch 1\n\u001b[34m\u001b[1mwandb\u001b[0m:             test/f1 0.12162\n\u001b[34m\u001b[1mwandb\u001b[0m:           test/loss 0.2681\n\u001b[34m\u001b[1mwandb\u001b[0m:             test/lr 0\n\u001b[34m\u001b[1mwandb\u001b[0m:         test/params 559955\n\u001b[34m\u001b[1mwandb\u001b[0m:      test/precision 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m:         test/recall 0.06923\n\u001b[34m\u001b[1mwandb\u001b[0m:     test/time_epoch 4.07665\n\u001b[34m\u001b[1mwandb\u001b[0m:      test/time_iter 0.0316\n\u001b[34m\u001b[1mwandb\u001b[0m:      train/accuracy 0.76736\n\u001b[34m\u001b[1mwandb\u001b[0m:           train/auc 0.55996\n\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch 1\n\u001b[34m\u001b[1mwandb\u001b[0m:           train/eta 0\n\u001b[34m\u001b[1mwandb\u001b[0m:     train/eta_hours 0\n\u001b[34m\u001b[1mwandb\u001b[0m:            train/f1 0.08115\n\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 0.49184\n\u001b[34m\u001b[1mwandb\u001b[0m:            train/lr 2e-05\n\u001b[34m\u001b[1mwandb\u001b[0m:        train/params 559955\n\u001b[34m\u001b[1mwandb\u001b[0m:     train/precision 0.04762\n\u001b[34m\u001b[1mwandb\u001b[0m:        train/recall 0.27435\n\u001b[34m\u001b[1mwandb\u001b[0m:    train/time_epoch 82.37029\n\u001b[34m\u001b[1mwandb\u001b[0m:     train/time_iter 0.08005\n\u001b[34m\u001b[1mwandb\u001b[0m:        val/accuracy 0.97715\n\u001b[34m\u001b[1mwandb\u001b[0m:             val/auc 0.66316\n\u001b[34m\u001b[1mwandb\u001b[0m:           val/epoch 1\n\u001b[34m\u001b[1mwandb\u001b[0m:              val/f1 0.07843\n\u001b[34m\u001b[1mwandb\u001b[0m:            val/loss 0.26775\n\u001b[34m\u001b[1mwandb\u001b[0m:              val/lr 0\n\u001b[34m\u001b[1mwandb\u001b[0m:          val/params 559955\n\u001b[34m\u001b[1mwandb\u001b[0m:       val/precision 0.19048\n\u001b[34m\u001b[1mwandb\u001b[0m:          val/recall 0.04938\n\u001b[34m\u001b[1mwandb\u001b[0m:      val/time_epoch 4.06826\n\u001b[34m\u001b[1mwandb\u001b[0m:       val/time_iter 0.03154\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mogbg-molhiv.GPS.CustomGatedGCN+Transformer+LapPE.r0\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/omertalmi-tel-aviv-university/molhiv/runs/9h26ckr0\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/omertalmi-tel-aviv-university/molhiv\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240924_135919-9h26ckr0/logs\u001b[0m\nTask done, results saved in results/ogbg-molhiv-GPS/0\nModel saved to results/ogbg-molhiv-GPS/0/models/model_run_0.pt\nLearned value of 'a_param': -0.013928880915045738\n1\n{'epoch': 1, 'time_epoch': 4.06826, 'loss': 0.26775433, 'lr': 0, 'params': 559955, 'time_iter': 0.03154, 'accuracy': 0.97715, 'precision': 0.19048, 'recall': 0.04938, 'f1': 0.07843, 'auc': 0.66316}\n{'epoch': 1, 'time_epoch': 4.07665, 'loss': 0.26809919, 'lr': 0, 'params': 559955, 'time_iter': 0.0316, 'accuracy': 0.96839, 'precision': 0.5, 'recall': 0.06923, 'f1': 0.12162, 'auc': 0.67273}\n{'epoch': 1, 'time_epoch': 82.37029, 'eta': 0.0, 'eta_hours': 0.0, 'loss': 0.49183594, 'lr': 2e-05, 'params': 559955, 'time_iter': 0.08005, 'accuracy': 0.76736, 'precision': 0.04762, 'recall': 0.27435, 'f1': 0.08115, 'auc': 0.55996}\nResults aggregated across runs saved in results/ogbg-molhiv-GPS/agg\n[*] All done: 2024-09-24 14:02:31.753274\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}