{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8942367,"sourceType":"datasetVersion","datasetId":5380713}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-09-28T12:20:04.418416Z","iopub.execute_input":"2024-09-28T12:20:04.418711Z","iopub.status.idle":"2024-09-28T12:20:05.430590Z","shell.execute_reply.started":"2024-09-28T12:20:04.418679Z","shell.execute_reply":"2024-09-28T12:20:05.429529Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl\n/kaggle/input/torch_geometric-2.5.3-py3-none-any.whl\n/kaggle/input/torch_cluster-1.6.3-cp310-cp310-linux_x86_64.whl\n/kaggle/input/torch_spline_conv-1.2.2-cp310-cp310-linux_x86_64.whl\n/kaggle/input/torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/rampasek/GraphGPS.git","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:20:09.193654Z","iopub.execute_input":"2024-09-28T12:20:09.194710Z","iopub.status.idle":"2024-09-28T12:20:11.276076Z","shell.execute_reply.started":"2024-09-28T12:20:09.194667Z","shell.execute_reply":"2024-09-28T12:20:11.275093Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'GraphGPS'...\nremote: Enumerating objects: 526, done.\u001b[K\nremote: Counting objects: 100% (350/350), done.\u001b[K\nremote: Compressing objects: 100% (109/109), done.\u001b[K\nremote: Total 526 (delta 271), reused 241 (delta 241), pack-reused 176 (from 1)\u001b[K\nReceiving objects: 100% (526/526), 12.93 MiB | 33.09 MiB/s, done.\nResolving deltas: 100% (337/337), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/GraphGPS\n!ls ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:20:15.073392Z","iopub.execute_input":"2024-09-28T12:20:15.074371Z","iopub.status.idle":"2024-09-28T12:20:16.061797Z","shell.execute_reply.started":"2024-09-28T12:20:15.074326Z","shell.execute_reply":"2024-09-28T12:20:16.060633Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/GraphGPS\nGraphGPS.png  README.md  final-results.zip  main.py  setup.py  unittests\nLICENSE       configs\t graphgps\t    run      tests\n","output_type":"stream"}]},{"cell_type":"markdown","source":"##  installation for slurm if needed using conda ","metadata":{}},{"cell_type":"code","source":"# # for slurm if needed using conda \n# conda create -n graphgps python=3.10\n# conda activate graphgps\n\n# conda install pytorch=1.13 torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\n# conda install pyg=2.2 -c pyg -c conda-forge\n# pip install pyg-lib -f https://data.pyg.org/whl/torch-1.13.0+cu117.html\n\n# # RDKit is required for OGB-LSC PCQM4Mv2 and datasets derived from it.  \n# conda install openbabel fsspec rdkit -c conda-forge\n\n# pip install pytorch-lightning yacs torchmetrics\n# pip install performer-pytorch\n# pip install tensorboardX\n# pip install ogb\n# pip install wandb\n\n# conda clean --all","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:41:44.580188Z","iopub.execute_input":"2024-09-26T06:41:44.581028Z","iopub.status.idle":"2024-09-26T06:41:44.585678Z","shell.execute_reply.started":"2024-09-26T06:41:44.580983Z","shell.execute_reply":"2024-09-26T06:41:44.584637Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# installation for kaggle","metadata":{}},{"cell_type":"code","source":"# Check existing environment\nimport sys\nprint(\"Python version:\", sys.version)\nimport torch\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\n\n!pip uninstall -y torch torchvision torchaudio\n!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n\n!pip install torch_geometric\n\n# Optional dependencies:\n!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu124.html\n# Install other required packages\n!pip install pytorch-lightning yacs torchmetrics\n!pip install performer-pytorch\n!pip install tensorboardX\n!pip install ogb\n!pip install wandb\n\n# Attempt to install RDKit (may have limitations)\n!pip install rdkit-pypi\n# !pip install openbabel fsspec\n!pip install  fsspec\n\n# Clean up pip cache (optional)\n!pip cache purge\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:20:21.912738Z","iopub.execute_input":"2024-09-28T12:20:21.913723Z","iopub.status.idle":"2024-09-28T12:26:29.246097Z","shell.execute_reply.started":"2024-09-28T12:20:21.913678Z","shell.execute_reply":"2024-09-28T12:26:29.244845Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\nPyTorch version: 2.4.0\nCUDA available: True\nCUDA version: 12.3\nFound existing installation: torch 2.4.0\nUninstalling torch-2.4.0:\n  Successfully uninstalled torch-2.4.0\nFound existing installation: torchvision 0.19.0\nUninstalling torchvision-0.19.0:\n  Successfully uninstalled torchvision-0.19.0\nFound existing installation: torchaudio 2.4.0\nUninstalling torchaudio-2.4.0:\n  Successfully uninstalled torchaudio-2.4.0\nLooking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch==2.0.1+cu118\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m425.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.15.2+cu118\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp310-cp310-linux_x86_64.whl (6.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==2.0.2+cu118\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp310-cp310-linux_x86_64.whl (4.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.15.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1+cu118) (3.1.4)\nCollecting triton==2.0.0 (from torch==2.0.1+cu118)\n  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.15.2+cu118) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.15.2+cu118) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.15.2+cu118) (10.3.0)\nCollecting cmake (from triton==2.0.0->torch==2.0.1+cu118)\n  Downloading https://download.pytorch.org/whl/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.1+cu118)\n  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1+cu118) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.15.2+cu118) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\nBuilding wheels for collected packages: lit\n  Building wheel for lit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89987 sha256=40d08e2d9a73db31e65b5f01d53ba5aa80c28bd3bf2ec7e1004f12aeab195d91\n  Stored in directory: /root/.cache/pip/wheels/27/2c/b6/3ed2983b1b44fe0dea1bb35234b09f2c22fb8ebb308679c922\nSuccessfully built lit\nInstalling collected packages: lit, cmake, triton, torch, torchvision, torchaudio\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 2.4.0 requires torch>=2.1.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cmake-3.25.0 lit-15.0.7 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 triton-2.0.0\nCollecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.9.5)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2024.6.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2024.8.30)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\nLooking in links: https://data.pyg.org/whl/torch-2.4.0+cu124.html\nCollecting pyg_lib\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/pyg_lib-0.4.0%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torch_scatter\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_scatter-2.1.2%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (10.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting torch_sparse\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_sparse-0.6.18%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (5.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting torch_cluster\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_cluster-1.6.3%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting torch_spline_conv\n  Downloading https://data.pyg.org/whl/torch-2.4.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt24cu124-cp310-cp310-linux_x86_64.whl (992 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m992.4/992.4 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_sparse) (1.14.1)\nRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy->torch_sparse) (1.26.4)\nInstalling collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\nSuccessfully installed pyg_lib-0.4.0+pt24cu124 torch_cluster-1.6.3+pt24cu124 torch_scatter-2.1.2+pt24cu124 torch_sparse-0.6.18+pt24cu124 torch_spline_conv-1.2.2+pt24cu124\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (2.4.0)\nCollecting yacs\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.2)\nCollecting torch>=2.1.0 (from pytorch-lightning)\n  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.66.4)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (6.0.2)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (21.3)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (4.12.2)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from pytorch-lightning) (0.11.7)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (70.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch-lightning) (3.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch>=2.1.0->pytorch-lightning)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->pytorch-lightning)\n  Downloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\nDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\nDownloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m530.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.68-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: yacs, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: triton\n    Found existing installation: triton 2.0.0\n    Uninstalling triton-2.0.0:\n      Successfully uninstalled triton-2.0.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.1+cu118\n    Uninstalling torch-2.0.1+cu118:\n      Successfully uninstalled torch-2.0.1+cu118\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.4.1 which is incompatible.\ntorchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.68 nvidia-nvtx-cu12-12.1.105 torch-2.4.1 triton-3.0.0 yacs-0.1.8\nCollecting performer-pytorch\n  Downloading performer_pytorch-1.1.4-py3-none-any.whl.metadata (763 bytes)\nCollecting einops>=0.3 (from performer-pytorch)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting local-attention>=1.1.1 (from performer-pytorch)\n  Downloading local_attention-1.9.15-py3-none-any.whl.metadata (683 bytes)\nRequirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.10/site-packages (from performer-pytorch) (2.4.1)\nCollecting axial-positional-embedding>=0.1.0 (from performer-pytorch)\n  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6->performer-pytorch) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6->performer-pytorch) (12.6.68)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6->performer-pytorch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6->performer-pytorch) (1.3.0)\nDownloading performer_pytorch-1.1.4-py3-none-any.whl (13 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading local_attention-1.9.15-py3-none-any.whl (9.0 kB)\nBuilding wheels for collected packages: axial-positional-embedding\n  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2889 sha256=31a530292899ff5aef3259923c428a6991e254f5a8b69d8b8085a1ecc809ff24\n  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\nSuccessfully built axial-positional-embedding\nInstalling collected packages: einops, local-attention, axial-positional-embedding, performer-pytorch\nSuccessfully installed axial-positional-embedding-0.2.1 einops-0.8.0 local-attention-1.9.15 performer-pytorch-1.1.4\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (2.6.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (21.3)\nRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (3.20.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorboardX) (3.1.2)\nCollecting ogb\n  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (2.4.1)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.26.4)\nRequirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (4.66.4)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.2.2)\nRequirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (2.2.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.16.0)\nRequirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.26.18)\nCollecting outdated>=0.2.0 (from ogb)\n  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: setuptools>=44 in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (70.0.0)\nCollecting littleutils (from outdated>=0.2.0->ogb)\n  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (2.32.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2024.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.6.68)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\nDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m898.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\nInstalling collected packages: littleutils, outdated, ogb\nSuccessfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.14.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nCollecting rdkit-pypi\n  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rdkit-pypi) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit-pypi) (10.3.0)\nDownloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rdkit-pypi\nSuccessfully installed rdkit-pypi-2022.9.5\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (2024.6.1)\nFiles removed: 184\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nprint(\"Python version:\", sys.version)\nimport torch\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:26:57.266026Z","iopub.execute_input":"2024-09-28T12:26:57.266865Z","iopub.status.idle":"2024-09-28T12:26:57.273626Z","shell.execute_reply.started":"2024-09-28T12:26:57.266823Z","shell.execute_reply":"2024-09-28T12:26:57.272457Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Python version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]\nPyTorch version: 2.4.0\nCUDA available: True\nCUDA version: 12.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch_geometric\nprint(\"PyG version:\", torch_geometric.__version__)\n\nimport pytorch_lightning\nprint(\"PyTorch Lightning version:\", pytorch_lightning.__version__)\n\nimport rdkit\nprint(\"RDKit version:\", rdkit.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:27:09.985885Z","iopub.execute_input":"2024-09-28T12:27:09.986784Z","iopub.status.idle":"2024-09-28T12:27:14.902449Z","shell.execute_reply.started":"2024-09-28T12:27:09.986742Z","shell.execute_reply":"2024-09-28T12:27:14.901398Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/libpyg.so: undefined symbol: _ZNK5torch8autograd4Node4nameEv\n  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n","output_type":"stream"},{"name":"stdout","text":"PyG version: 2.6.1\nPyTorch Lightning version: 2.4.0\nRDKit version: 2022.09.5\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Torch version:\", torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:48:24.752643Z","iopub.execute_input":"2024-09-26T06:48:24.753299Z","iopub.status.idle":"2024-09-26T06:48:24.759710Z","shell.execute_reply.started":"2024-09-26T06:48:24.753251Z","shell.execute_reply":"2024-09-26T06:48:24.758558Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Torch version: 2.4.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Running their model- GPSConv","metadata":{}},{"cell_type":"code","source":"# %cd /kaggle/working/GraphGPS\n# !WANDB_MODE=disabled python main.py --cfg configs/GPS/ogbg-molhiv-GPS.yaml","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:48:24.761166Z","iopub.execute_input":"2024-09-26T06:48:24.761920Z","iopub.status.idle":"2024-09-26T06:48:24.859840Z","shell.execute_reply.started":"2024-09-26T06:48:24.761866Z","shell.execute_reply":"2024-09-26T06:48:24.859080Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Our Model - WeightedGraphGPS","metadata":{}},{"cell_type":"code","source":"#%cd /kaggle/working\n#!rm -rf WeightedGraphGPS","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:54:44.725945Z","iopub.execute_input":"2024-09-26T06:54:44.726891Z","iopub.status.idle":"2024-09-26T06:54:45.727443Z","shell.execute_reply.started":"2024-09-26T06:54:44.726849Z","shell.execute_reply":"2024-09-26T06:54:45.726419Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"#clone for our project\n%cd /kaggle/working\n!git clone --branch concat_two_layers https://github.com/omertalmi5/WeightedGraphGPS.git","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:27:51.022981Z","iopub.execute_input":"2024-09-28T12:27:51.023608Z","iopub.status.idle":"2024-09-28T12:27:53.085700Z","shell.execute_reply.started":"2024-09-28T12:27:51.023570Z","shell.execute_reply":"2024-09-28T12:27:53.084278Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'WeightedGraphGPS'...\nremote: Enumerating objects: 733, done.\u001b[K\nremote: Counting objects: 100% (488/488), done.\u001b[K\nremote: Compressing objects: 100% (140/140), done.\u001b[K\nremote: Total 733 (delta 407), reused 348 (delta 348), pack-reused 245 (from 1)\u001b[K\nReceiving objects: 100% (733/733), 13.03 MiB | 33.37 MiB/s, done.\nResolving deltas: 100% (496/496), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2024-09-26T06:54:47.653708Z","iopub.execute_input":"2024-09-26T06:54:47.654146Z","iopub.status.idle":"2024-09-26T06:54:59.304463Z","shell.execute_reply.started":"2024-09-26T06:54:47.654099Z","shell.execute_reply":"2024-09-26T06:54:59.303228Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.18.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.14.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (70.0.0)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"!wandb login 5f1c292fde40243d281618fe88d712346cfe90d7","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:29:10.525512Z","iopub.execute_input":"2024-09-28T12:29:10.526435Z","iopub.status.idle":"2024-09-28T12:29:14.051425Z","shell.execute_reply.started":"2024-09-28T12:29:10.526388Z","shell.execute_reply":"2024-09-28T12:29:14.050295Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n# Set the environment variable\nos.environ['USERNAME'] = \"omertalmi-tel-aviv-university\"","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:29:42.534356Z","iopub.execute_input":"2024-09-28T12:29:42.534783Z","iopub.status.idle":"2024-09-28T12:29:42.539729Z","shell.execute_reply.started":"2024-09-28T12:29:42.534741Z","shell.execute_reply":"2024-09-28T12:29:42.538530Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/WeightedGraphGPS\n!WANDB_MODE=online python main.py --cfg configs/GPS/ogbg-molpcba-GPS.yaml wandb.use True wandb.entity $USERNAME","metadata":{"execution":{"iopub.status.busy":"2024-09-28T12:30:18.543327Z","iopub.execute_input":"2024-09-28T12:30:18.543746Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/kaggle/working/WeightedGraphGPS\n[*] Run ID 0: seed=0, split_index=0\n    Starting now: 2024-09-28 12:30:36.724027\nDownloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/pcba.zip\nDownloaded 0.04 GB: 100%|███████████████████████| 39/39 [00:01<00:00, 19.75it/s]\nExtracting ./datasets/pcba.zip\nProcessing...\nLoading necessary files...\nThis might take a while.\nProcessing graphs...\n100%|████████████████████████████████| 437929/437929 [00:07<00:00, 62289.30it/s]\nConverting graphs into PyG objects...\n100%|████████████████████████████████| 437929/437929 [00:19<00:00, 21972.86it/s]\nSaving...\nDone!\n/opt/conda/lib/python3.10/site-packages/ogb/graphproppred/dataset_pyg.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.data, self.slices = torch.load(self.processed_paths[0])\n[*] Loaded dataset 'ogbg-molpcba' from 'OGB':\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n  Data(edge_index=[2, 24618372], edge_attr=[24618372, 3], x=[11373137, 9], y=[437929, 128], num_nodes=11373137)\n  undirected: True\n  num graphs: 437929\n/opt/conda/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n  warnings.warn(msg)\n  avg num_nodes/graph: 25\n  num node features: 9\n  num edge features: 3\n  num tasks: 128\n  num classes: 2\nPrecomputing Positional Encoding statistics: ['LapPE'] for all graphs...\n  ...estimated to be undirected: True\n100%|██████████████████████████████████| 437929/437929 [07:46<00:00, 938.35it/s]\nDone! Took 00:08:04.29\nGraphGymModule(\n  (model): GPSModel(\n    (encoder): FeatureEncoder(\n      (node_encoder): Concat2NodeEncoder(\n        (encoder1): AtomEncoder(\n          (atom_embedding_list): ModuleList(\n            (0): Embedding(119, 368)\n            (1): Embedding(5, 368)\n            (2-3): 2 x Embedding(12, 368)\n            (4): Embedding(10, 368)\n            (5-6): 2 x Embedding(6, 368)\n            (7-8): 2 x Embedding(2, 368)\n          )\n        )\n        (encoder2): LapPENodeEncoder(\n          (linear_A): Linear(in_features=2, out_features=32, bias=True)\n          (pe_encoder): Sequential(\n            (0): ReLU()\n            (1): Linear(in_features=32, out_features=16, bias=True)\n            (2): ReLU()\n          )\n        )\n      )\n      (edge_encoder): BondEncoder(\n        (bond_embedding_list): ModuleList(\n          (0): Embedding(5, 384)\n          (1): Embedding(6, 384)\n          (2): Embedding(2, 384)\n        )\n      )\n    )\n    (layers): Sequential(\n      (0): GPSLayer(\n        summary: dim_h=384, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_network): Sequential(\n          (0): Linear(in_features=768, out_features=384, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=384, out_features=1, bias=True)\n          (3): Sigmoid()\n        )\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n        )\n        (norm1_local): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.2, inplace=False)\n        (dropout_attn): Dropout(p=0.2, inplace=False)\n        (ff_linear1): Linear(in_features=384, out_features=768, bias=True)\n        (ff_linear2): Linear(in_features=768, out_features=384, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.2, inplace=False)\n        (ff_dropout2): Dropout(p=0.2, inplace=False)\n      )\n      (1): GPSLayer(\n        summary: dim_h=384, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_network): Sequential(\n          (0): Linear(in_features=768, out_features=384, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=384, out_features=1, bias=True)\n          (3): Sigmoid()\n        )\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n        )\n        (norm1_local): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.2, inplace=False)\n        (dropout_attn): Dropout(p=0.2, inplace=False)\n        (ff_linear1): Linear(in_features=384, out_features=768, bias=True)\n        (ff_linear2): Linear(in_features=768, out_features=384, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.2, inplace=False)\n        (ff_dropout2): Dropout(p=0.2, inplace=False)\n      )\n      (2): GPSLayer(\n        summary: dim_h=384, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_network): Sequential(\n          (0): Linear(in_features=768, out_features=384, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=384, out_features=1, bias=True)\n          (3): Sigmoid()\n        )\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n        )\n        (norm1_local): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.2, inplace=False)\n        (dropout_attn): Dropout(p=0.2, inplace=False)\n        (ff_linear1): Linear(in_features=384, out_features=768, bias=True)\n        (ff_linear2): Linear(in_features=768, out_features=384, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.2, inplace=False)\n        (ff_dropout2): Dropout(p=0.2, inplace=False)\n      )\n      (3): GPSLayer(\n        summary: dim_h=384, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_network): Sequential(\n          (0): Linear(in_features=768, out_features=384, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=384, out_features=1, bias=True)\n          (3): Sigmoid()\n        )\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n        )\n        (norm1_local): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.2, inplace=False)\n        (dropout_attn): Dropout(p=0.2, inplace=False)\n        (ff_linear1): Linear(in_features=384, out_features=768, bias=True)\n        (ff_linear2): Linear(in_features=768, out_features=384, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.2, inplace=False)\n        (ff_dropout2): Dropout(p=0.2, inplace=False)\n      )\n      (4): GPSLayer(\n        summary: dim_h=384, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4\n        (gating_network): Sequential(\n          (0): Linear(in_features=768, out_features=384, bias=True)\n          (1): ReLU()\n          (2): Linear(in_features=384, out_features=1, bias=True)\n          (3): Sigmoid()\n        )\n        (local_model): GatedGCNLayer()\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n        )\n        (norm1_local): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (norm1_attn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (dropout_local): Dropout(p=0.2, inplace=False)\n        (dropout_attn): Dropout(p=0.2, inplace=False)\n        (ff_linear1): Linear(in_features=384, out_features=768, bias=True)\n        (ff_linear2): Linear(in_features=768, out_features=384, bias=True)\n        (act_fn_ff): ReLU()\n        (norm2): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (ff_dropout1): Dropout(p=0.2, inplace=False)\n        (ff_dropout2): Dropout(p=0.2, inplace=False)\n      )\n    )\n    (post_mp): GNNGraphHead(\n      (layer_post_mp): MLP(\n        (model): Sequential(\n          (0): Linear(\n            (model): Linear(384, 128, bias=True)\n          )\n        )\n      )\n    )\n  )\n)\naccelerator: cuda\nbenchmark: False\nbn:\n  eps: 1e-05\n  mom: 0.1\ncfg_dest: config.yaml\ncustom_metrics: []\ndataset:\n  cache_load: False\n  cache_save: False\n  dir: ./datasets\n  edge_dim: 128\n  edge_encoder: True\n  edge_encoder_bn: False\n  edge_encoder_name: Bond\n  edge_encoder_num_types: 0\n  edge_message_ratio: 0.8\n  edge_negative_sampling_ratio: 1.0\n  edge_train_mode: all\n  encoder: True\n  encoder_bn: True\n  encoder_dim: 128\n  encoder_name: db\n  format: OGB\n  infer_link_label: None\n  label_column: none\n  label_table: none\n  location: local\n  name: ogbg-molpcba\n  node_encoder: True\n  node_encoder_bn: False\n  node_encoder_name: Atom+LapPE\n  node_encoder_num_types: 0\n  remove_feature: False\n  resample_disjoint: False\n  resample_negative: False\n  shuffle_split: True\n  slic_compactness: 10\n  split: [0.8, 0.1, 0.1]\n  split_dir: ./splits\n  split_index: 0\n  split_mode: standard\n  task: graph\n  task_type: classification_multilabel\n  to_undirected: False\n  transductive: False\n  transform: none\n  tu_simple: True\ndevices: 1\nexample_arg: example\nexample_group:\n  example_arg: example\ngnn:\n  act: relu\n  agg: mean\n  att_final_linear: False\n  att_final_linear_bn: False\n  att_heads: 1\n  batchnorm: True\n  clear_feature: True\n  dim_edge: 384\n  dim_inner: 384\n  dropout: 0.0\n  head: graph\n  keep_edge: 0.5\n  l2norm: True\n  layer_type: generalconv\n  layers_mp: 2\n  layers_post_mp: 1\n  layers_pre_mp: 0\n  msg_direction: single\n  normalize_adj: False\n  residual: False\n  self_msg: concat\n  skip_every: 1\n  stage_type: stack\ngpu_mem: False\ngraphormer:\n  attention_dropout: 0.0\n  dropout: 0.0\n  embed_dim: 80\n  input_dropout: 0.0\n  mlp_dropout: 0.0\n  num_heads: 4\n  num_layers: 6\n  use_graph_token: True\ngt:\n  attn_dropout: 0.5\n  batch_norm: True\n  bigbird:\n    add_cross_attention: False\n    attention_type: block_sparse\n    block_size: 3\n    chunk_size_feed_forward: 0\n    hidden_act: relu\n    is_decoder: False\n    layer_norm_eps: 1e-06\n    max_position_embeddings: 128\n    num_random_blocks: 3\n    use_bias: False\n  dim_hidden: 384\n  dropout: 0.2\n  full_graph: True\n  gamma: 1e-05\n  layer_norm: False\n  layer_type: CustomGatedGCN+Transformer\n  layers: 5\n  n_heads: 4\n  pna_degrees: []\n  residual: True\nmem:\n  inplace: False\nmetric_agg: argmax\nmetric_best: ap\nmodel:\n  edge_decoding: dot\n  graph_pooling: mean\n  loss_fun: cross_entropy\n  match_upper: True\n  size_average: mean\n  thresh: 0.5\n  type: GPSModel\nname_tag: \nnum_threads: 6\nnum_workers: 0\noptim:\n  base_lr: 0.0005\n  batch_accumulation: 1\n  clip_grad_norm: True\n  clip_grad_norm_value: 1.0\n  lr_decay: 0.1\n  max_epoch: 100\n  min_lr: 0.0\n  momentum: 0.9\n  num_warmup_epochs: 5\n  optimizer: adamW\n  reduce_factor: 0.1\n  schedule_patience: 10\n  scheduler: cosine_with_warmup\n  steps: [30, 60, 90]\n  weight_decay: 1e-05\nout_dir: results/ogbg-molpcba-GPS\nposenc_ElstaticSE:\n  dim_pe: 16\n  enable: False\n  kernel:\n    times: []\n    times_func: range(10)\n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_EquivStableLapPE:\n  eigen:\n    eigvec_norm: L2\n    laplacian_norm: sym\n    max_freqs: 10\n  enable: False\n  raw_norm_type: none\nposenc_GraphormerBias:\n  dim_pe: 0\n  enable: False\n  node_degrees_only: False\n  num_in_degrees: None\n  num_out_degrees: None\n  num_spatial_types: None\nposenc_HKdiagSE:\n  dim_pe: 16\n  enable: False\n  kernel:\n    times: []\n    times_func: \n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_LapPE:\n  dim_pe: 16\n  eigen:\n    eigvec_norm: L2\n    laplacian_norm: none\n    max_freqs: 10\n  enable: True\n  layers: 2\n  model: DeepSet\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_RWSE:\n  dim_pe: 16\n  enable: False\n  kernel:\n    times: []\n    times_func: \n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  post_layers: 0\n  raw_norm_type: none\nposenc_SignNet:\n  dim_pe: 16\n  eigen:\n    eigvec_norm: L2\n    laplacian_norm: sym\n    max_freqs: 10\n  enable: False\n  layers: 3\n  model: none\n  n_heads: 4\n  pass_as_var: False\n  phi_hidden_dim: 64\n  phi_out_dim: 4\n  post_layers: 0\n  raw_norm_type: none\npretrained:\n  dir: \n  freeze_main: False\n  reset_prediction_head: True\nprint: both\nround: 5\nrun_dir: results/ogbg-molpcba-GPS/0\nrun_id: 0\nrun_multiple_splits: []\nseed: 0\nshare:\n  dim_in: 9\n  dim_out: 128\n  num_splits: 3\ntensorboard_agg: True\ntensorboard_each_run: False\ntrain:\n  auto_resume: False\n  batch_size: 512\n  ckpt_best: False\n  ckpt_clean: True\n  ckpt_period: 100\n  enable_ckpt: True\n  epoch_resume: -1\n  eval_period: 1\n  iter_per_epoch: 32\n  mode: custom\n  neighbor_sizes: [20, 15, 10, 5]\n  node_per_graph: 32\n  radius: extend\n  sample_node: False\n  sampler: full_batch\n  skip_train_eval: False\n  walk_length: 4\nval:\n  node_per_graph: 32\n  radius: extend\n  sample_node: False\n  sampler: full_batch\nview_emb: False\nwandb:\n  entity: omertalmi-tel-aviv-university\n  name: \n  project: molpcba\n  use: True\nNum parameters: 11224213\nStart from epoch 0\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33momertalmi-\u001b[0m (\u001b[33momertalmi-tel-aviv-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.1\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/WeightedGraphGPS/wandb/run-20240928_124108-7c7tfrip\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mogbg-molpcba.GPS.CustomGatedGCN+Transformer+LapPE.r0\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/omertalmi-tel-aviv-university/molpcba\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/omertalmi-tel-aviv-university/molpcba/runs/7c7tfrip\u001b[0m\nKey posenc_GraphormerBias.num_spatial_types with value <class 'NoneType'> is not a valid type; valid types: {<class 'list'>, <class 'str'>, <class 'float'>, <class 'bool'>, <class 'int'>, <class 'tuple'>}\nKey posenc_GraphormerBias.num_in_degrees with value <class 'NoneType'> is not a valid type; valid types: {<class 'list'>, <class 'str'>, <class 'float'>, <class 'bool'>, <class 'int'>, <class 'tuple'>}\nKey posenc_GraphormerBias.num_out_degrees with value <class 'NoneType'> is not a valid type; valid types: {<class 'list'>, <class 'str'>, <class 'float'>, <class 'bool'>, <class 'int'>, <class 'tuple'>}\ntrain: {'epoch': 0, 'time_epoch': 525.26571, 'eta': 52001.30495, 'eta_hours': 14.44481, 'loss': 0.69920974, 'lr': 0.0, 'params': 11224213, 'time_iter': 0.76681, 'accuracy': 0.5022, 'auc': 0.49596, 'ap': 0.02144}\n...computing epoch stats took: 10.88s\nval: {'epoch': 0, 'time_epoch': 34.28756, 'loss': 0.69758885, 'lr': 0, 'params': 11224213, 'time_iter': 0.39869, 'accuracy': 0.50219, 'auc': 0.49673, 'ap': 0.02836}\n...computing epoch stats took: 1.43s\ntest: {'epoch': 0, 'time_epoch': 33.74528, 'loss': 0.69833603, 'lr': 0, 'params': 11224213, 'time_iter': 0.39239, 'accuracy': 0.50154, 'auc': 0.49979, 'ap': 0.02742}\n...computing epoch stats took: 1.40s\n> Epoch 0: took 607.0s (avg 607.0s) | Best so far: epoch 0\ttrain_loss: 0.6992 train_ap: 0.0214\tval_loss: 0.6976 val_ap: 0.0284\ttest_loss: 0.6983 test_ap: 0.0274\ntrain: {'epoch': 1, 'time_epoch': 478.10845, 'eta': 49165.33359, 'eta_hours': 13.65704, 'loss': 0.42306727, 'lr': 0.0001, 'params': 11224213, 'time_iter': 0.69797, 'accuracy': 0.91271, 'auc': 0.5388, 'ap': 0.03106}\n...computing epoch stats took: 10.64s\nval: {'epoch': 1, 'time_epoch': 25.50721, 'loss': 0.23888695, 'lr': 0, 'params': 11224213, 'time_iter': 0.2966, 'accuracy': 0.96358, 'auc': 0.59523, 'ap': 0.03993}\n...computing epoch stats took: 1.43s\ntest: {'epoch': 1, 'time_epoch': 27.11197, 'loss': 0.27090752, 'lr': 0, 'params': 11224213, 'time_iter': 0.31526, 'accuracy': 0.96179, 'auc': 0.58523, 'ap': 0.0402}\n...computing epoch stats took: 1.38s\n> Epoch 1: took 544.2s (avg 575.6s) | Best so far: epoch 1\ttrain_loss: 0.4231 train_ap: 0.0311\tval_loss: 0.2389 val_ap: 0.0399\ttest_loss: 0.2709 test_ap: 0.0402\ntrain: {'epoch': 2, 'time_epoch': 473.89903, 'eta': 47765.16637, 'eta_hours': 13.2681, 'loss': 0.08417363, 'lr': 0.0002, 'params': 11224213, 'time_iter': 0.69182, 'accuracy': 0.97864, 'auc': 0.60324, 'ap': 0.04096}\n...computing epoch stats took: 10.59s\nval: {'epoch': 2, 'time_epoch': 25.57963, 'loss': 0.06163495, 'lr': 0, 'params': 11224213, 'time_iter': 0.29744, 'accuracy': 0.97648, 'auc': 0.7229, 'ap': 0.07416}\n...computing epoch stats took: 1.43s\ntest: {'epoch': 2, 'time_epoch': 27.19363, 'loss': 0.06403801, 'lr': 0, 'params': 11224213, 'time_iter': 0.3162, 'accuracy': 0.97497, 'auc': 0.71593, 'ap': 0.07484}\n...computing epoch stats took: 1.38s\n> Epoch 2: took 540.1s (avg 563.8s) | Best so far: epoch 2\ttrain_loss: 0.0842 train_ap: 0.0410\tval_loss: 0.0616 val_ap: 0.0742\ttest_loss: 0.0640 test_ap: 0.0748\ntrain: {'epoch': 3, 'time_epoch': 474.33749, 'eta': 46838.65614, 'eta_hours': 13.01074, 'loss': 0.04832439, 'lr': 0.0003, 'params': 11224213, 'time_iter': 0.69246, 'accuracy': 0.98126, 'auc': 0.74417, 'ap': 0.08508}\nval: {'epoch': 3, 'time_epoch': 25.63024, 'loss': 0.05568541, 'lr': 0, 'params': 11224213, 'time_iter': 0.29803, 'accuracy': 0.97694, 'auc': 0.77392, 'ap': 0.11558}\ntest: {'epoch': 3, 'time_epoch': 27.12175, 'loss': 0.05819971, 'lr': 0, 'params': 11224213, 'time_iter': 0.31537, 'accuracy': 0.97536, 'auc': 0.77017, 'ap': 0.11465}\n> Epoch 3: took 540.7s (avg 558.0s) | Best so far: epoch 3\ttrain_loss: 0.0483 train_ap: 0.0851\tval_loss: 0.0557 val_ap: 0.1156\ttest_loss: 0.0582 test_ap: 0.1147\ntrain: {'epoch': 4, 'time_epoch': 473.43475, 'eta': 46075.86301, 'eta_hours': 12.79885, 'loss': 0.04451906, 'lr': 0.0004, 'params': 11224213, 'time_iter': 0.69115, 'accuracy': 0.982, 'auc': 0.80101, 'ap': 0.1151}\nval: {'epoch': 4, 'time_epoch': 25.65701, 'loss': 0.05321038, 'lr': 0, 'params': 11224213, 'time_iter': 0.29834, 'accuracy': 0.97736, 'auc': 0.8162, 'ap': 0.13828}\ntest: {'epoch': 4, 'time_epoch': 28.99204, 'loss': 0.05580151, 'lr': 0, 'params': 11224213, 'time_iter': 0.33712, 'accuracy': 0.97585, 'auc': 0.81169, 'ap': 0.13673}\n> Epoch 4: took 541.7s (avg 554.8s) | Best so far: epoch 4\ttrain_loss: 0.0445 train_ap: 0.1151\tval_loss: 0.0532 val_ap: 0.1383\ttest_loss: 0.0558 test_ap: 0.1367\ntrain: {'epoch': 5, 'time_epoch': 474.53775, 'eta': 45426.80301, 'eta_hours': 12.61856, 'loss': 0.04258533, 'lr': 0.0005, 'params': 11224213, 'time_iter': 0.69276, 'accuracy': 0.98256, 'auc': 0.82632, 'ap': 0.13658}\nval: {'epoch': 5, 'time_epoch': 25.59911, 'loss': 0.05097401, 'lr': 0, 'params': 11224213, 'time_iter': 0.29766, 'accuracy': 0.97791, 'auc': 0.83465, 'ap': 0.15986}\ntest: {'epoch': 5, 'time_epoch': 27.21213, 'loss': 0.05348837, 'lr': 0, 'params': 11224213, 'time_iter': 0.31642, 'accuracy': 0.97646, 'auc': 0.82638, 'ap': 0.16051}\n> Epoch 5: took 541.0s (avg 552.5s) | Best so far: epoch 5\ttrain_loss: 0.0426 train_ap: 0.1366\tval_loss: 0.0510 val_ap: 0.1599\ttest_loss: 0.0535 test_ap: 0.1605\ntrain: {'epoch': 6, 'time_epoch': 475.10197, 'eta': 44835.10263, 'eta_hours': 12.4542, 'loss': 0.04093637, 'lr': 0.00049986, 'params': 11224213, 'time_iter': 0.69358, 'accuracy': 0.98323, 'auc': 0.84454, 'ap': 0.15875}\nval: {'epoch': 6, 'time_epoch': 27.57344, 'loss': 0.05043094, 'lr': 0, 'params': 11224213, 'time_iter': 0.32062, 'accuracy': 0.97917, 'auc': 0.84137, 'ap': 0.17368}\ntest: {'epoch': 6, 'time_epoch': 27.3279, 'loss': 0.05283834, 'lr': 0, 'params': 11224213, 'time_iter': 0.31777, 'accuracy': 0.97763, 'auc': 0.83944, 'ap': 0.17112}\n> Epoch 6: took 543.6s (avg 551.2s) | Best so far: epoch 6\ttrain_loss: 0.0409 train_ap: 0.1588\tval_loss: 0.0504 val_ap: 0.1737\ttest_loss: 0.0528 test_ap: 0.1711\ntrain: {'epoch': 7, 'time_epoch': 473.63521, 'eta': 44255.68402, 'eta_hours': 12.29325, 'loss': 0.03978864, 'lr': 0.00049945, 'params': 11224213, 'time_iter': 0.69144, 'accuracy': 0.98367, 'auc': 0.85585, 'ap': 0.17699}\nval: {'epoch': 7, 'time_epoch': 25.541, 'loss': 0.04847825, 'lr': 0, 'params': 11224213, 'time_iter': 0.29699, 'accuracy': 0.97919, 'auc': 0.85146, 'ap': 0.19353}\ntest: {'epoch': 7, 'time_epoch': 27.19274, 'loss': 0.05114404, 'lr': 0, 'params': 11224213, 'time_iter': 0.31619, 'accuracy': 0.97764, 'auc': 0.8454, 'ap': 0.18517}\n> Epoch 7: took 540.0s (avg 549.8s) | Best so far: epoch 7\ttrain_loss: 0.0398 train_ap: 0.1770\tval_loss: 0.0485 val_ap: 0.1935\ttest_loss: 0.0511 test_ap: 0.1852\ntrain: {'epoch': 8, 'time_epoch': 477.23947, 'eta': 43736.21592, 'eta_hours': 12.14895, 'loss': 0.03876985, 'lr': 0.00049877, 'params': 11224213, 'time_iter': 0.6967, 'accuracy': 0.98409, 'auc': 0.86513, 'ap': 0.195}\nval: {'epoch': 8, 'time_epoch': 25.58096, 'loss': 0.04872778, 'lr': 0, 'params': 11224213, 'time_iter': 0.29745, 'accuracy': 0.97843, 'auc': 0.85551, 'ap': 0.19855}\ntest: {'epoch': 8, 'time_epoch': 27.18493, 'loss': 0.05136498, 'lr': 0, 'params': 11224213, 'time_iter': 0.3161, 'accuracy': 0.97707, 'auc': 0.85051, 'ap': 0.1933}\n> Epoch 8: took 543.6s (avg 549.1s) | Best so far: epoch 8\ttrain_loss: 0.0388 train_ap: 0.1950\tval_loss: 0.0487 val_ap: 0.1986\ttest_loss: 0.0514 test_ap: 0.1933\ntrain: {'epoch': 9, 'time_epoch': 474.74864, 'eta': 43202.77607, 'eta_hours': 12.00077, 'loss': 0.03783367, 'lr': 0.00049782, 'params': 11224213, 'time_iter': 0.69306, 'accuracy': 0.98437, 'auc': 0.8735, 'ap': 0.21174}\nval: {'epoch': 9, 'time_epoch': 25.57361, 'loss': 0.04722296, 'lr': 0, 'params': 11224213, 'time_iter': 0.29737, 'accuracy': 0.97942, 'auc': 0.86736, 'ap': 0.2168}\ntest: {'epoch': 9, 'time_epoch': 27.2562, 'loss': 0.04955973, 'lr': 0, 'params': 11224213, 'time_iter': 0.31693, 'accuracy': 0.97794, 'auc': 0.85996, 'ap': 0.20876}\n> Epoch 9: took 541.6s (avg 548.4s) | Best so far: epoch 9\ttrain_loss: 0.0378 train_ap: 0.2117\tval_loss: 0.0472 val_ap: 0.2168\ttest_loss: 0.0496 test_ap: 0.2088\ntrain: {'epoch': 10, 'time_epoch': 476.46423, 'eta': 42693.88805, 'eta_hours': 11.85941, 'loss': 0.03709323, 'lr': 0.00049659, 'params': 11224213, 'time_iter': 0.69557, 'accuracy': 0.98471, 'auc': 0.8802, 'ap': 0.22783}\nval: {'epoch': 10, 'time_epoch': 25.53176, 'loss': 0.0471329, 'lr': 0, 'params': 11224213, 'time_iter': 0.29688, 'accuracy': 0.97954, 'auc': 0.86759, 'ap': 0.22494}\ntest: {'epoch': 10, 'time_epoch': 27.21421, 'loss': 0.0494716, 'lr': 0, 'params': 11224213, 'time_iter': 0.31644, 'accuracy': 0.97801, 'auc': 0.86211, 'ap': 0.21772}\n> Epoch 10: took 543.0s (avg 547.9s) | Best so far: epoch 10\ttrain_loss: 0.0371 train_ap: 0.2278\tval_loss: 0.0471 val_ap: 0.2249\ttest_loss: 0.0495 test_ap: 0.2177\ntrain: {'epoch': 11, 'time_epoch': 473.58842, 'eta': 42169.31471, 'eta_hours': 11.7137, 'loss': 0.03633938, 'lr': 0.00049509, 'params': 11224213, 'time_iter': 0.69137, 'accuracy': 0.98501, 'auc': 0.88416, 'ap': 0.24377}\nval: {'epoch': 11, 'time_epoch': 25.56972, 'loss': 0.04576645, 'lr': 0, 'params': 11224213, 'time_iter': 0.29732, 'accuracy': 0.98008, 'auc': 0.87329, 'ap': 0.23569}\ntest: {'epoch': 11, 'time_epoch': 27.18741, 'loss': 0.04804942, 'lr': 0, 'params': 11224213, 'time_iter': 0.31613, 'accuracy': 0.97874, 'auc': 0.86381, 'ap': 0.22809}\n> Epoch 11: took 539.9s (avg 547.2s) | Best so far: epoch 11\ttrain_loss: 0.0363 train_ap: 0.2438\tval_loss: 0.0458 val_ap: 0.2357\ttest_loss: 0.0480 test_ap: 0.2281\ntrain: {'epoch': 12, 'time_epoch': 477.87058, 'eta': 41681.24275, 'eta_hours': 11.57812, 'loss': 0.03570767, 'lr': 0.00049333, 'params': 11224213, 'time_iter': 0.69762, 'accuracy': 0.98522, 'auc': 0.88967, 'ap': 0.25828}\nval: {'epoch': 12, 'time_epoch': 25.58239, 'loss': 0.04579325, 'lr': 0, 'params': 11224213, 'time_iter': 0.29747, 'accuracy': 0.98032, 'auc': 0.87622, 'ap': 0.23457}\ntest: {'epoch': 12, 'time_epoch': 27.14127, 'loss': 0.04792707, 'lr': 0, 'params': 11224213, 'time_iter': 0.3156, 'accuracy': 0.9792, 'auc': 0.86999, 'ap': 0.23461}\n> Epoch 12: took 544.1s (avg 547.0s) | Best so far: epoch 11\ttrain_loss: 0.0363 train_ap: 0.2438\tval_loss: 0.0458 val_ap: 0.2357\ttest_loss: 0.0480 test_ap: 0.2281\ntrain: {'epoch': 13, 'time_epoch': 473.66271, 'eta': 41168.77979, 'eta_hours': 11.43577, 'loss': 0.0351465, 'lr': 0.0004913, 'params': 11224213, 'time_iter': 0.69148, 'accuracy': 0.98536, 'auc': 0.89284, 'ap': 0.27251}\nval: {'epoch': 13, 'time_epoch': 25.50307, 'loss': 0.04565052, 'lr': 0, 'params': 11224213, 'time_iter': 0.29655, 'accuracy': 0.98014, 'auc': 0.87817, 'ap': 0.24513}\ntest: {'epoch': 13, 'time_epoch': 27.10274, 'loss': 0.04796544, 'lr': 0, 'params': 11224213, 'time_iter': 0.31515, 'accuracy': 0.97883, 'auc': 0.87276, 'ap': 0.23924}\n> Epoch 13: took 539.8s (avg 546.5s) | Best so far: epoch 13\ttrain_loss: 0.0351 train_ap: 0.2725\tval_loss: 0.0457 val_ap: 0.2451\ttest_loss: 0.0480 test_ap: 0.2392\ntrain: {'epoch': 14, 'time_epoch': 475.07002, 'eta': 40669.46496, 'eta_hours': 11.29707, 'loss': 0.03459303, 'lr': 0.00048901, 'params': 11224213, 'time_iter': 0.69353, 'accuracy': 0.98569, 'auc': 0.89792, 'ap': 0.28158}\nval: {'epoch': 14, 'time_epoch': 25.55894, 'loss': 0.04523325, 'lr': 0, 'params': 11224213, 'time_iter': 0.2972, 'accuracy': 0.97999, 'auc': 0.878, 'ap': 0.24853}\ntest: {'epoch': 14, 'time_epoch': 27.22049, 'loss': 0.04760358, 'lr': 0, 'params': 11224213, 'time_iter': 0.31652, 'accuracy': 0.97875, 'auc': 0.87342, 'ap': 0.23976}\n> Epoch 14: took 541.7s (avg 546.1s) | Best so far: epoch 14\ttrain_loss: 0.0346 train_ap: 0.2816\tval_loss: 0.0452 val_ap: 0.2485\ttest_loss: 0.0476 test_ap: 0.2398\ntrain: {'epoch': 15, 'time_epoch': 473.44919, 'eta': 40164.67139, 'eta_hours': 11.15685, 'loss': 0.03407908, 'lr': 0.00048645, 'params': 11224213, 'time_iter': 0.69117, 'accuracy': 0.98591, 'auc': 0.90164, 'ap': 0.29652}\nval: {'epoch': 15, 'time_epoch': 25.49004, 'loss': 0.04512301, 'lr': 0, 'params': 11224213, 'time_iter': 0.2964, 'accuracy': 0.98034, 'auc': 0.87796, 'ap': 0.25335}\ntest: {'epoch': 15, 'time_epoch': 27.21931, 'loss': 0.04725131, 'lr': 0, 'params': 11224213, 'time_iter': 0.3165, 'accuracy': 0.97915, 'auc': 0.87339, 'ap': 0.25242}\n> Epoch 15: took 539.8s (avg 545.7s) | Best so far: epoch 15\ttrain_loss: 0.0341 train_ap: 0.2965\tval_loss: 0.0451 val_ap: 0.2534\ttest_loss: 0.0473 test_ap: 0.2524\ntrain: {'epoch': 16, 'time_epoch': 475.10665, 'eta': 39671.65769, 'eta_hours': 11.0199, 'loss': 0.03359224, 'lr': 0.00048364, 'params': 11224213, 'time_iter': 0.69359, 'accuracy': 0.98604, 'auc': 0.90572, 'ap': 0.30742}\nval: {'epoch': 16, 'time_epoch': 25.58402, 'loss': 0.0450847, 'lr': 0, 'params': 11224213, 'time_iter': 0.29749, 'accuracy': 0.98011, 'auc': 0.88014, 'ap': 0.25782}\ntest: {'epoch': 16, 'time_epoch': 27.16331, 'loss': 0.0474157, 'lr': 0, 'params': 11224213, 'time_iter': 0.31585, 'accuracy': 0.97867, 'auc': 0.87522, 'ap': 0.25598}\n> Epoch 16: took 541.7s (avg 545.5s) | Best so far: epoch 16\ttrain_loss: 0.0336 train_ap: 0.3074\tval_loss: 0.0451 val_ap: 0.2578\ttest_loss: 0.0474 test_ap: 0.2560\ntrain: {'epoch': 17, 'time_epoch': 473.24082, 'eta': 39172.13375, 'eta_hours': 10.88115, 'loss': 0.03313321, 'lr': 0.00048057, 'params': 11224213, 'time_iter': 0.69086, 'accuracy': 0.9863, 'auc': 0.90924, 'ap': 0.32007}\nval: {'epoch': 17, 'time_epoch': 25.60126, 'loss': 0.04513328, 'lr': 0, 'params': 11224213, 'time_iter': 0.29769, 'accuracy': 0.97996, 'auc': 0.87866, 'ap': 0.26173}\ntest: {'epoch': 17, 'time_epoch': 28.91991, 'loss': 0.04733503, 'lr': 0, 'params': 11224213, 'time_iter': 0.33628, 'accuracy': 0.97877, 'auc': 0.87357, 'ap': 0.25513}\n> Epoch 17: took 541.3s (avg 545.3s) | Best so far: epoch 17\ttrain_loss: 0.0331 train_ap: 0.3201\tval_loss: 0.0451 val_ap: 0.2617\ttest_loss: 0.0473 test_ap: 0.2551\ntrain: {'epoch': 18, 'time_epoch': 471.80943, 'eta': 38669.27425, 'eta_hours': 10.74147, 'loss': 0.03270203, 'lr': 0.00047725, 'params': 11224213, 'time_iter': 0.68877, 'accuracy': 0.98642, 'auc': 0.91167, 'ap': 0.333}\nval: {'epoch': 18, 'time_epoch': 25.45072, 'loss': 0.04470595, 'lr': 0, 'params': 11224213, 'time_iter': 0.29594, 'accuracy': 0.98061, 'auc': 0.88112, 'ap': 0.26612}\ntest: {'epoch': 18, 'time_epoch': 27.14593, 'loss': 0.04718128, 'lr': 0, 'params': 11224213, 'time_iter': 0.31565, 'accuracy': 0.97962, 'auc': 0.87649, 'ap': 0.25738}\n> Epoch 18: took 537.9s (avg 544.9s) | Best so far: epoch 18\ttrain_loss: 0.0327 train_ap: 0.3330\tval_loss: 0.0447 val_ap: 0.2661\ttest_loss: 0.0472 test_ap: 0.2574\ntrain: {'epoch': 19, 'time_epoch': 471.13078, 'eta': 38166.80515, 'eta_hours': 10.60189, 'loss': 0.0323567, 'lr': 0.00047368, 'params': 11224213, 'time_iter': 0.68778, 'accuracy': 0.9866, 'auc': 0.91426, 'ap': 0.33985}\nval: {'epoch': 19, 'time_epoch': 27.49137, 'loss': 0.04502662, 'lr': 0, 'params': 11224213, 'time_iter': 0.31967, 'accuracy': 0.98015, 'auc': 0.88029, 'ap': 0.26799}\ntest: {'epoch': 19, 'time_epoch': 27.18671, 'loss': 0.0473252, 'lr': 0, 'params': 11224213, 'time_iter': 0.31612, 'accuracy': 0.97909, 'auc': 0.87582, 'ap': 0.26367}\n> Epoch 19: took 539.4s (avg 544.6s) | Best so far: epoch 19\ttrain_loss: 0.0324 train_ap: 0.3398\tval_loss: 0.0450 val_ap: 0.2680\ttest_loss: 0.0473 test_ap: 0.2637\ntrain: {'epoch': 20, 'time_epoch': 472.58817, 'eta': 37672.8032, 'eta_hours': 10.46467, 'loss': 0.03196527, 'lr': 0.00046987, 'params': 11224213, 'time_iter': 0.68991, 'accuracy': 0.98677, 'auc': 0.91745, 'ap': 0.3501}\nval: {'epoch': 20, 'time_epoch': 25.54499, 'loss': 0.04457164, 'lr': 0, 'params': 11224213, 'time_iter': 0.29703, 'accuracy': 0.98072, 'auc': 0.88348, 'ap': 0.27274}\ntest: {'epoch': 20, 'time_epoch': 27.21227, 'loss': 0.04670618, 'lr': 0, 'params': 11224213, 'time_iter': 0.31642, 'accuracy': 0.97958, 'auc': 0.87761, 'ap': 0.26384}\n> Epoch 20: took 539.0s (avg 544.3s) | Best so far: epoch 20\ttrain_loss: 0.0320 train_ap: 0.3501\tval_loss: 0.0446 val_ap: 0.2727\ttest_loss: 0.0467 test_ap: 0.2638\ntrain: {'epoch': 21, 'time_epoch': 474.71678, 'eta': 37188.29486, 'eta_hours': 10.33008, 'loss': 0.03155671, 'lr': 0.00046581, 'params': 11224213, 'time_iter': 0.69302, 'accuracy': 0.98688, 'auc': 0.9201, 'ap': 0.35882}\nval: {'epoch': 21, 'time_epoch': 25.52557, 'loss': 0.04434832, 'lr': 0, 'params': 11224213, 'time_iter': 0.29681, 'accuracy': 0.98107, 'auc': 0.88264, 'ap': 0.27306}\ntest: {'epoch': 21, 'time_epoch': 27.15228, 'loss': 0.04673265, 'lr': 0, 'params': 11224213, 'time_iter': 0.31572, 'accuracy': 0.9797, 'auc': 0.87617, 'ap': 0.26428}\n> Epoch 21: took 541.1s (avg 544.2s) | Best so far: epoch 21\ttrain_loss: 0.0316 train_ap: 0.3588\tval_loss: 0.0443 val_ap: 0.2731\ttest_loss: 0.0467 test_ap: 0.2643\ntrain: {'epoch': 22, 'time_epoch': 472.49345, 'eta': 36697.19463, 'eta_hours': 10.19367, 'loss': 0.03122659, 'lr': 0.00046152, 'params': 11224213, 'time_iter': 0.68977, 'accuracy': 0.98701, 'auc': 0.92234, 'ap': 0.36749}\nval: {'epoch': 22, 'time_epoch': 25.51761, 'loss': 0.04416049, 'lr': 0, 'params': 11224213, 'time_iter': 0.29672, 'accuracy': 0.98104, 'auc': 0.88151, 'ap': 0.27828}\ntest: {'epoch': 22, 'time_epoch': 27.17115, 'loss': 0.04647533, 'lr': 0, 'params': 11224213, 'time_iter': 0.31594, 'accuracy': 0.97981, 'auc': 0.87685, 'ap': 0.27174}\n> Epoch 22: took 539.0s (avg 544.0s) | Best so far: epoch 22\ttrain_loss: 0.0312 train_ap: 0.3675\tval_loss: 0.0442 val_ap: 0.2783\ttest_loss: 0.0465 test_ap: 0.2717\ntrain: {'epoch': 23, 'time_epoch': 475.05906, 'eta': 36215.7694, 'eta_hours': 10.05994, 'loss': 0.03089142, 'lr': 0.000457, 'params': 11224213, 'time_iter': 0.69352, 'accuracy': 0.98712, 'auc': 0.92441, 'ap': 0.3777}\nval: {'epoch': 23, 'time_epoch': 25.46691, 'loss': 0.04435316, 'lr': 0, 'params': 11224213, 'time_iter': 0.29613, 'accuracy': 0.98057, 'auc': 0.88303, 'ap': 0.27575}\ntest: {'epoch': 23, 'time_epoch': 27.18338, 'loss': 0.04676916, 'lr': 0, 'params': 11224213, 'time_iter': 0.31609, 'accuracy': 0.97933, 'auc': 0.87814, 'ap': 0.26783}\n> Epoch 23: took 541.3s (avg 543.9s) | Best so far: epoch 22\ttrain_loss: 0.0312 train_ap: 0.3675\tval_loss: 0.0442 val_ap: 0.2783\ttest_loss: 0.0465 test_ap: 0.2717\ntrain: {'epoch': 24, 'time_epoch': 471.07363, 'eta': 35722.89717, 'eta_hours': 9.92303, 'loss': 0.03059169, 'lr': 0.00045225, 'params': 11224213, 'time_iter': 0.6877, 'accuracy': 0.98723, 'auc': 0.92708, 'ap': 0.38617}\nval: {'epoch': 24, 'time_epoch': 25.5275, 'loss': 0.04475083, 'lr': 0, 'params': 11224213, 'time_iter': 0.29683, 'accuracy': 0.98062, 'auc': 0.88226, 'ap': 0.27532}\ntest: {'epoch': 24, 'time_epoch': 27.17545, 'loss': 0.04701352, 'lr': 0, 'params': 11224213, 'time_iter': 0.31599, 'accuracy': 0.97941, 'auc': 0.87655, 'ap': 0.26653}\n> Epoch 24: took 537.3s (avg 543.6s) | Best so far: epoch 22\ttrain_loss: 0.0312 train_ap: 0.3675\tval_loss: 0.0442 val_ap: 0.2783\ttest_loss: 0.0465 test_ap: 0.2717\ntrain: {'epoch': 25, 'time_epoch': 474.48559, 'eta': 35241.41271, 'eta_hours': 9.78928, 'loss': 0.03026247, 'lr': 0.00044729, 'params': 11224213, 'time_iter': 0.69268, 'accuracy': 0.98743, 'auc': 0.9287, 'ap': 0.39244}\nval: {'epoch': 25, 'time_epoch': 25.44885, 'loss': 0.04390307, 'lr': 0, 'params': 11224213, 'time_iter': 0.29592, 'accuracy': 0.98131, 'auc': 0.88346, 'ap': 0.28581}\ntest: {'epoch': 25, 'time_epoch': 27.19116, 'loss': 0.04626265, 'lr': 0, 'params': 11224213, 'time_iter': 0.31618, 'accuracy': 0.97996, 'auc': 0.87895, 'ap': 0.27532}\n> Epoch 25: took 540.6s (avg 543.5s) | Best so far: epoch 25\ttrain_loss: 0.0303 train_ap: 0.3924\tval_loss: 0.0439 val_ap: 0.2858\ttest_loss: 0.0463 test_ap: 0.2753\ntrain: {'epoch': 26, 'time_epoch': 471.49047, 'eta': 34752.34876, 'eta_hours': 9.65343, 'loss': 0.02995206, 'lr': 0.0004421, 'params': 11224213, 'time_iter': 0.68831, 'accuracy': 0.98755, 'auc': 0.9309, 'ap': 0.40022}\nval: {'epoch': 26, 'time_epoch': 25.55145, 'loss': 0.04417332, 'lr': 0, 'params': 11224213, 'time_iter': 0.29711, 'accuracy': 0.98093, 'auc': 0.88107, 'ap': 0.27938}\ntest: {'epoch': 26, 'time_epoch': 27.1892, 'loss': 0.0465485, 'lr': 0, 'params': 11224213, 'time_iter': 0.31615, 'accuracy': 0.97999, 'auc': 0.87579, 'ap': 0.27088}\n> Epoch 26: took 537.8s (avg 543.3s) | Best so far: epoch 25\ttrain_loss: 0.0303 train_ap: 0.3924\tval_loss: 0.0439 val_ap: 0.2858\ttest_loss: 0.0463 test_ap: 0.2753\ntrain: {'epoch': 27, 'time_epoch': 473.10102, 'eta': 34268.68147, 'eta_hours': 9.51908, 'loss': 0.02969354, 'lr': 0.00043671, 'params': 11224213, 'time_iter': 0.69066, 'accuracy': 0.98771, 'auc': 0.93308, 'ap': 0.41025}\nval: {'epoch': 27, 'time_epoch': 25.53086, 'loss': 0.04403977, 'lr': 0, 'params': 11224213, 'time_iter': 0.29687, 'accuracy': 0.98094, 'auc': 0.88253, 'ap': 0.28308}\ntest: {'epoch': 27, 'time_epoch': 27.15767, 'loss': 0.04664419, 'lr': 0, 'params': 11224213, 'time_iter': 0.31579, 'accuracy': 0.97964, 'auc': 0.87742, 'ap': 0.27209}\n> Epoch 27: took 539.3s (avg 543.1s) | Best so far: epoch 25\ttrain_loss: 0.0303 train_ap: 0.3924\tval_loss: 0.0439 val_ap: 0.2858\ttest_loss: 0.0463 test_ap: 0.2753\ntrain: {'epoch': 28, 'time_epoch': 472.18658, 'eta': 33783.50411, 'eta_hours': 9.38431, 'loss': 0.02942479, 'lr': 0.00043111, 'params': 11224213, 'time_iter': 0.68932, 'accuracy': 0.98777, 'auc': 0.9347, 'ap': 0.41728}\nval: {'epoch': 28, 'time_epoch': 25.46124, 'loss': 0.04445224, 'lr': 0, 'params': 11224213, 'time_iter': 0.29606, 'accuracy': 0.98068, 'auc': 0.88005, 'ap': 0.28633}\ntest: {'epoch': 28, 'time_epoch': 27.06339, 'loss': 0.0467399, 'lr': 0, 'params': 11224213, 'time_iter': 0.31469, 'accuracy': 0.9797, 'auc': 0.87383, 'ap': 0.2727}\n> Epoch 28: took 538.2s (avg 543.0s) | Best so far: epoch 28\ttrain_loss: 0.0294 train_ap: 0.4173\tval_loss: 0.0445 val_ap: 0.2863\ttest_loss: 0.0467 test_ap: 0.2727\ntrain: {'epoch': 29, 'time_epoch': 475.88868, 'eta': 33307.83103, 'eta_hours': 9.25218, 'loss': 0.02912698, 'lr': 0.00042531, 'params': 11224213, 'time_iter': 0.69473, 'accuracy': 0.9879, 'auc': 0.9363, 'ap': 0.42196}\nval: {'epoch': 29, 'time_epoch': 25.52175, 'loss': 0.04481421, 'lr': 0, 'params': 11224213, 'time_iter': 0.29676, 'accuracy': 0.9808, 'auc': 0.88029, 'ap': 0.28134}\ntest: {'epoch': 29, 'time_epoch': 27.12385, 'loss': 0.04720314, 'lr': 0, 'params': 11224213, 'time_iter': 0.31539, 'accuracy': 0.97967, 'auc': 0.87509, 'ap': 0.27227}\n> Epoch 29: took 542.2s (avg 542.9s) | Best so far: epoch 28\ttrain_loss: 0.0294 train_ap: 0.4173\tval_loss: 0.0445 val_ap: 0.2863\ttest_loss: 0.0467 test_ap: 0.2727\ntrain: {'epoch': 30, 'time_epoch': 470.57094, 'eta': 32820.30778, 'eta_hours': 9.11675, 'loss': 0.02886903, 'lr': 0.00041932, 'params': 11224213, 'time_iter': 0.68696, 'accuracy': 0.988, 'auc': 0.93769, 'ap': 0.43294}\nval: {'epoch': 30, 'time_epoch': 25.5164, 'loss': 0.04483503, 'lr': 0, 'params': 11224213, 'time_iter': 0.2967, 'accuracy': 0.98015, 'auc': 0.88107, 'ap': 0.28678}\ntest: {'epoch': 30, 'time_epoch': 28.93351, 'loss': 0.04730936, 'lr': 0, 'params': 11224213, 'time_iter': 0.33644, 'accuracy': 0.97906, 'auc': 0.87605, 'ap': 0.28103}\n> Epoch 30: took 538.6s (avg 542.8s) | Best so far: epoch 30\ttrain_loss: 0.0289 train_ap: 0.4329\tval_loss: 0.0448 val_ap: 0.2868\ttest_loss: 0.0473 test_ap: 0.2810\ntrain: {'epoch': 31, 'time_epoch': 470.65617, 'eta': 32334.02515, 'eta_hours': 8.98167, 'loss': 0.02863098, 'lr': 0.00041315, 'params': 11224213, 'time_iter': 0.68709, 'accuracy': 0.98808, 'auc': 0.93903, 'ap': 0.43678}\nval: {'epoch': 31, 'time_epoch': 25.55119, 'loss': 0.04476568, 'lr': 0, 'params': 11224213, 'time_iter': 0.29711, 'accuracy': 0.98072, 'auc': 0.88104, 'ap': 0.2886}\ntest: {'epoch': 31, 'time_epoch': 27.1791, 'loss': 0.04708965, 'lr': 0, 'params': 11224213, 'time_iter': 0.31604, 'accuracy': 0.9794, 'auc': 0.87651, 'ap': 0.28205}\n> Epoch 31: took 537.0s (avg 542.6s) | Best so far: epoch 31\ttrain_loss: 0.0286 train_ap: 0.4368\tval_loss: 0.0448 val_ap: 0.2886\ttest_loss: 0.0471 test_ap: 0.2821\ntrain: {'epoch': 32, 'time_epoch': 471.84387, 'eta': 31851.10098, 'eta_hours': 8.84753, 'loss': 0.02836636, 'lr': 0.00040679, 'params': 11224213, 'time_iter': 0.68882, 'accuracy': 0.98828, 'auc': 0.94099, 'ap': 0.44274}\nval: {'epoch': 32, 'time_epoch': 27.47736, 'loss': 0.04430868, 'lr': 0, 'params': 11224213, 'time_iter': 0.3195, 'accuracy': 0.98098, 'auc': 0.88115, 'ap': 0.29409}\ntest: {'epoch': 32, 'time_epoch': 27.21503, 'loss': 0.04690678, 'lr': 0, 'params': 11224213, 'time_iter': 0.31645, 'accuracy': 0.97963, 'auc': 0.87558, 'ap': 0.28096}\n> Epoch 32: took 540.1s (avg 542.5s) | Best so far: epoch 32\ttrain_loss: 0.0284 train_ap: 0.4427\tval_loss: 0.0443 val_ap: 0.2941\ttest_loss: 0.0469 test_ap: 0.2810\ntrain: {'epoch': 33, 'time_epoch': 471.71084, 'eta': 31368.57034, 'eta_hours': 8.71349, 'loss': 0.02812457, 'lr': 0.00040027, 'params': 11224213, 'time_iter': 0.68863, 'accuracy': 0.98837, 'auc': 0.94257, 'ap': 0.45163}\nval: {'epoch': 33, 'time_epoch': 25.49262, 'loss': 0.04458332, 'lr': 0, 'params': 11224213, 'time_iter': 0.29643, 'accuracy': 0.9813, 'auc': 0.87892, 'ap': 0.29005}\ntest: {'epoch': 33, 'time_epoch': 27.1675, 'loss': 0.04707967, 'lr': 0, 'params': 11224213, 'time_iter': 0.3159, 'accuracy': 0.98011, 'auc': 0.87506, 'ap': 0.27766}\n> Epoch 33: took 537.9s (avg 542.4s) | Best so far: epoch 32\ttrain_loss: 0.0284 train_ap: 0.4427\tval_loss: 0.0443 val_ap: 0.2941\ttest_loss: 0.0469 test_ap: 0.2810\ntrain: {'epoch': 34, 'time_epoch': 474.20742, 'eta': 30891.29451, 'eta_hours': 8.58092, 'loss': 0.0278856, 'lr': 0.00039358, 'params': 11224213, 'time_iter': 0.69227, 'accuracy': 0.98846, 'auc': 0.94378, 'ap': 0.45787}\nval: {'epoch': 34, 'time_epoch': 25.57605, 'loss': 0.04523218, 'lr': 0, 'params': 11224213, 'time_iter': 0.2974, 'accuracy': 0.98047, 'auc': 0.87748, 'ap': 0.28774}\ntest: {'epoch': 34, 'time_epoch': 27.1817, 'loss': 0.04775681, 'lr': 0, 'params': 11224213, 'time_iter': 0.31607, 'accuracy': 0.97944, 'auc': 0.87187, 'ap': 0.27569}\n> Epoch 34: took 540.5s (avg 542.4s) | Best so far: epoch 32\ttrain_loss: 0.0284 train_ap: 0.4427\tval_loss: 0.0443 val_ap: 0.2941\ttest_loss: 0.0469 test_ap: 0.2810\ntrain: {'epoch': 35, 'time_epoch': 470.34789, 'eta': 30407.32775, 'eta_hours': 8.44648, 'loss': 0.02768099, 'lr': 0.00038674, 'params': 11224213, 'time_iter': 0.68664, 'accuracy': 0.98846, 'auc': 0.94539, 'ap': 0.4629}\nval: {'epoch': 35, 'time_epoch': 25.50382, 'loss': 0.04469738, 'lr': 0, 'params': 11224213, 'time_iter': 0.29656, 'accuracy': 0.98086, 'auc': 0.88095, 'ap': 0.29184}\ntest: {'epoch': 35, 'time_epoch': 27.16328, 'loss': 0.04733824, 'lr': 0, 'params': 11224213, 'time_iter': 0.31585, 'accuracy': 0.97968, 'auc': 0.87529, 'ap': 0.28041}\n> Epoch 35: took 536.6s (avg 542.2s) | Best so far: epoch 32\ttrain_loss: 0.0284 train_ap: 0.4427\tval_loss: 0.0443 val_ap: 0.2941\ttest_loss: 0.0469 test_ap: 0.2810\ntrain: {'epoch': 36, 'time_epoch': 473.43984, 'eta': 29929.36181, 'eta_hours': 8.31371, 'loss': 0.02744738, 'lr': 0.00037974, 'params': 11224213, 'time_iter': 0.69115, 'accuracy': 0.98864, 'auc': 0.94652, 'ap': 0.46899}\nval: {'epoch': 36, 'time_epoch': 25.5357, 'loss': 0.04480161, 'lr': 0, 'params': 11224213, 'time_iter': 0.29693, 'accuracy': 0.9815, 'auc': 0.88119, 'ap': 0.29368}\ntest: {'epoch': 36, 'time_epoch': 27.16798, 'loss': 0.04739075, 'lr': 0, 'params': 11224213, 'time_iter': 0.31591, 'accuracy': 0.98012, 'auc': 0.87508, 'ap': 0.27991}\n> Epoch 36: took 539.8s (avg 542.1s) | Best so far: epoch 32\ttrain_loss: 0.0284 train_ap: 0.4427\tval_loss: 0.0443 val_ap: 0.2941\ttest_loss: 0.0469 test_ap: 0.2810\ntrain: {'epoch': 37, 'time_epoch': 473.74741, 'eta': 29452.13592, 'eta_hours': 8.18115, 'loss': 0.02721727, 'lr': 0.00037261, 'params': 11224213, 'time_iter': 0.6916, 'accuracy': 0.98866, 'auc': 0.94844, 'ap': 0.4732}\nval: {'epoch': 37, 'time_epoch': 25.4568, 'loss': 0.04468576, 'lr': 0, 'params': 11224213, 'time_iter': 0.29601, 'accuracy': 0.98095, 'auc': 0.88101, 'ap': 0.29087}\ntest: {'epoch': 37, 'time_epoch': 27.17368, 'loss': 0.04725154, 'lr': 0, 'params': 11224213, 'time_iter': 0.31597, 'accuracy': 0.97984, 'auc': 0.87419, 'ap': 0.2805}\n> Epoch 37: took 540.0s (avg 542.1s) | Best so far: epoch 32\ttrain_loss: 0.0284 train_ap: 0.4427\tval_loss: 0.0443 val_ap: 0.2941\ttest_loss: 0.0469 test_ap: 0.2810\ntrain: {'epoch': 38, 'time_epoch': 473.20771, 'eta': 28974.24425, 'eta_hours': 8.0484, 'loss': 0.02699263, 'lr': 0.00036534, 'params': 11224213, 'time_iter': 0.69081, 'accuracy': 0.98878, 'auc': 0.94971, 'ap': 0.4807}\nval: {'epoch': 38, 'time_epoch': 25.4378, 'loss': 0.0448215, 'lr': 0, 'params': 11224213, 'time_iter': 0.29579, 'accuracy': 0.98138, 'auc': 0.87933, 'ap': 0.29325}\ntest: {'epoch': 38, 'time_epoch': 27.07012, 'loss': 0.04777439, 'lr': 0, 'params': 11224213, 'time_iter': 0.31477, 'accuracy': 0.98009, 'auc': 0.87175, 'ap': 0.27881}\n> Epoch 38: took 539.2s (avg 542.0s) | Best so far: epoch 32\ttrain_loss: 0.0284 train_ap: 0.4427\tval_loss: 0.0443 val_ap: 0.2941\ttest_loss: 0.0469 test_ap: 0.2810\ntrain: {'epoch': 39, 'time_epoch': 470.65, 'eta': 28492.75023, 'eta_hours': 7.91465, 'loss': 0.02676653, 'lr': 0.00035794, 'params': 11224213, 'time_iter': 0.68708, 'accuracy': 0.98895, 'auc': 0.95073, 'ap': 0.4873}\nval: {'epoch': 39, 'time_epoch': 25.52176, 'loss': 0.04459449, 'lr': 0, 'params': 11224213, 'time_iter': 0.29676, 'accuracy': 0.98134, 'auc': 0.88173, 'ap': 0.2918}\ntest: {'epoch': 39, 'time_epoch': 27.21819, 'loss': 0.0474134, 'lr': 0, 'params': 11224213, 'time_iter': 0.31649, 'accuracy': 0.98001, 'auc': 0.87417, 'ap': 0.28008}\n> Epoch 39: took 537.1s (avg 541.9s) | Best so far: epoch 32\ttrain_loss: 0.0284 train_ap: 0.4427\tval_loss: 0.0443 val_ap: 0.2941\ttest_loss: 0.0469 test_ap: 0.2810\ntrain: {'epoch': 40, 'time_epoch': 474.00218, 'eta': 28016.60904, 'eta_hours': 7.78239, 'loss': 0.02654946, 'lr': 0.00035042, 'params': 11224213, 'time_iter': 0.69197, 'accuracy': 0.98905, 'auc': 0.95208, 'ap': 0.49598}\nval: {'epoch': 40, 'time_epoch': 25.51169, 'loss': 0.04579384, 'lr': 0, 'params': 11224213, 'time_iter': 0.29665, 'accuracy': 0.98066, 'auc': 0.87785, 'ap': 0.29079}\ntest: {'epoch': 40, 'time_epoch': 27.12929, 'loss': 0.04842067, 'lr': 0, 'params': 11224213, 'time_iter': 0.31546, 'accuracy': 0.97943, 'auc': 0.87152, 'ap': 0.27898}\n> Epoch 40: took 540.1s (avg 541.8s) | Best so far: epoch 32\ttrain_loss: 0.0284 train_ap: 0.4427\tval_loss: 0.0443 val_ap: 0.2941\ttest_loss: 0.0469 test_ap: 0.2810\ntrain: {'epoch': 41, 'time_epoch': 471.80552, 'eta': 27537.53623, 'eta_hours': 7.64932, 'loss': 0.02627938, 'lr': 0.0003428, 'params': 11224213, 'time_iter': 0.68877, 'accuracy': 0.98914, 'auc': 0.95316, 'ap': 0.50146}\nval: {'epoch': 41, 'time_epoch': 25.58658, 'loss': 0.04527001, 'lr': 0, 'params': 11224213, 'time_iter': 0.29752, 'accuracy': 0.98108, 'auc': 0.87824, 'ap': 0.29294}\ntest: {'epoch': 41, 'time_epoch': 27.14548, 'loss': 0.0479916, 'lr': 0, 'params': 11224213, 'time_iter': 0.31565, 'accuracy': 0.97989, 'auc': 0.87457, 'ap': 0.28151}\n> Epoch 41: took 538.2s (avg 541.7s) | Best so far: epoch 32\ttrain_loss: 0.0284 train_ap: 0.4427\tval_loss: 0.0443 val_ap: 0.2941\ttest_loss: 0.0469 test_ap: 0.2810\ntrain: {'epoch': 42, 'time_epoch': 475.01183, 'eta': 27063.05166, 'eta_hours': 7.51751, 'loss': 0.0261154, 'lr': 0.00033507, 'params': 11224213, 'time_iter': 0.69345, 'accuracy': 0.98919, 'auc': 0.95506, 'ap': 0.50395}\nval: {'epoch': 42, 'time_epoch': 25.583, 'loss': 0.04512553, 'lr': 0, 'params': 11224213, 'time_iter': 0.29748, 'accuracy': 0.98085, 'auc': 0.87913, 'ap': 0.29533}\ntest: {'epoch': 42, 'time_epoch': 27.1516, 'loss': 0.04789984, 'lr': 0, 'params': 11224213, 'time_iter': 0.31572, 'accuracy': 0.97941, 'auc': 0.87369, 'ap': 0.2815}\n> Epoch 42: took 541.4s (avg 541.7s) | Best so far: epoch 42\ttrain_loss: 0.0261 train_ap: 0.5040\tval_loss: 0.0451 val_ap: 0.2953\ttest_loss: 0.0479 test_ap: 0.2815\ntrain: {'epoch': 43, 'time_epoch': 471.25494, 'eta': 26583.76163, 'eta_hours': 7.38438, 'loss': 0.02586438, 'lr': 0.00032725, 'params': 11224213, 'time_iter': 0.68796, 'accuracy': 0.98928, 'auc': 0.95569, 'ap': 0.5146}\nval: {'epoch': 43, 'time_epoch': 25.53465, 'loss': 0.04585492, 'lr': 0, 'params': 11224213, 'time_iter': 0.29691, 'accuracy': 0.98113, 'auc': 0.87806, 'ap': 0.29667}\ntest: {'epoch': 43, 'time_epoch': 29.0177, 'loss': 0.04858525, 'lr': 0, 'params': 11224213, 'time_iter': 0.33742, 'accuracy': 0.97971, 'auc': 0.87056, 'ap': 0.28135}\n> Epoch 43: took 539.4s (avg 541.7s) | Best so far: epoch 43\ttrain_loss: 0.0259 train_ap: 0.5146\tval_loss: 0.0459 val_ap: 0.2967\ttest_loss: 0.0486 test_ap: 0.2813\ntrain: {'epoch': 44, 'time_epoch': 470.10402, 'eta': 26103.42203, 'eta_hours': 7.25095, 'loss': 0.02568838, 'lr': 0.00031935, 'params': 11224213, 'time_iter': 0.68628, 'accuracy': 0.9893, 'auc': 0.9564, 'ap': 0.51738}\nval: {'epoch': 44, 'time_epoch': 25.49942, 'loss': 0.04602115, 'lr': 0, 'params': 11224213, 'time_iter': 0.2965, 'accuracy': 0.98077, 'auc': 0.87602, 'ap': 0.29528}\ntest: {'epoch': 44, 'time_epoch': 27.05614, 'loss': 0.04867449, 'lr': 0, 'params': 11224213, 'time_iter': 0.31461, 'accuracy': 0.97947, 'auc': 0.8702, 'ap': 0.28094}\n> Epoch 44: took 536.2s (avg 541.6s) | Best so far: epoch 43\ttrain_loss: 0.0259 train_ap: 0.5146\tval_loss: 0.0459 val_ap: 0.2967\ttest_loss: 0.0486 test_ap: 0.2813\ntrain: {'epoch': 45, 'time_epoch': 469.30251, 'eta': 25622.58655, 'eta_hours': 7.11739, 'loss': 0.02542161, 'lr': 0.00031137, 'params': 11224213, 'time_iter': 0.68511, 'accuracy': 0.98942, 'auc': 0.95819, 'ap': 0.52477}\nval: {'epoch': 45, 'time_epoch': 27.37104, 'loss': 0.04599819, 'lr': 0, 'params': 11224213, 'time_iter': 0.31827, 'accuracy': 0.98099, 'auc': 0.87879, 'ap': 0.29555}\ntest: {'epoch': 45, 'time_epoch': 27.16032, 'loss': 0.04887656, 'lr': 0, 'params': 11224213, 'time_iter': 0.31582, 'accuracy': 0.9796, 'auc': 0.87027, 'ap': 0.2794}\n> Epoch 45: took 537.4s (avg 541.5s) | Best so far: epoch 43\ttrain_loss: 0.0259 train_ap: 0.5146\tval_loss: 0.0459 val_ap: 0.2967\ttest_loss: 0.0486 test_ap: 0.2813\ntrain: {'epoch': 46, 'time_epoch': 471.74307, 'eta': 25144.99396, 'eta_hours': 6.98472, 'loss': 0.02519553, 'lr': 0.00030332, 'params': 11224213, 'time_iter': 0.68868, 'accuracy': 0.98959, 'auc': 0.95942, 'ap': 0.53073}\nval: {'epoch': 46, 'time_epoch': 25.55713, 'loss': 0.04596876, 'lr': 0, 'params': 11224213, 'time_iter': 0.29718, 'accuracy': 0.9811, 'auc': 0.87862, 'ap': 0.29679}\ntest: {'epoch': 46, 'time_epoch': 27.14107, 'loss': 0.04878529, 'lr': 0, 'params': 11224213, 'time_iter': 0.31559, 'accuracy': 0.97987, 'auc': 0.8705, 'ap': 0.27942}\n> Epoch 46: took 538.1s (avg 541.4s) | Best so far: epoch 46\ttrain_loss: 0.0252 train_ap: 0.5307\tval_loss: 0.0460 val_ap: 0.2968\ttest_loss: 0.0488 test_ap: 0.2794\ntrain: {'epoch': 47, 'time_epoch': 471.6709, 'eta': 24667.56692, 'eta_hours': 6.8521, 'loss': 0.02503657, 'lr': 0.00029522, 'params': 11224213, 'time_iter': 0.68857, 'accuracy': 0.98961, 'auc': 0.96055, 'ap': 0.53348}\nval: {'epoch': 47, 'time_epoch': 25.53145, 'loss': 0.04637694, 'lr': 0, 'params': 11224213, 'time_iter': 0.29688, 'accuracy': 0.98088, 'auc': 0.87369, 'ap': 0.29311}\ntest: {'epoch': 47, 'time_epoch': 27.13231, 'loss': 0.04920176, 'lr': 0, 'params': 11224213, 'time_iter': 0.31549, 'accuracy': 0.97934, 'auc': 0.86654, 'ap': 0.27527}\n> Epoch 47: took 537.9s (avg 541.3s) | Best so far: epoch 46\ttrain_loss: 0.0252 train_ap: 0.5307\tval_loss: 0.0460 val_ap: 0.2968\ttest_loss: 0.0488 test_ap: 0.2794\ntrain: {'epoch': 48, 'time_epoch': 471.77591, 'eta': 24190.48411, 'eta_hours': 6.71958, 'loss': 0.02480346, 'lr': 0.00028707, 'params': 11224213, 'time_iter': 0.68872, 'accuracy': 0.98974, 'auc': 0.96139, 'ap': 0.53966}\nval: {'epoch': 48, 'time_epoch': 25.48234, 'loss': 0.04628612, 'lr': 0, 'params': 11224213, 'time_iter': 0.29631, 'accuracy': 0.98095, 'auc': 0.87501, 'ap': 0.29786}\ntest: {'epoch': 48, 'time_epoch': 27.09223, 'loss': 0.04923813, 'lr': 0, 'params': 11224213, 'time_iter': 0.31503, 'accuracy': 0.97949, 'auc': 0.86713, 'ap': 0.28013}\n> Epoch 48: took 538.0s (avg 541.3s) | Best so far: epoch 48\ttrain_loss: 0.0248 train_ap: 0.5397\tval_loss: 0.0463 val_ap: 0.2979\ttest_loss: 0.0492 test_ap: 0.2801\ntrain: {'epoch': 49, 'time_epoch': 473.14962, 'eta': 23714.9873, 'eta_hours': 6.5875, 'loss': 0.02467017, 'lr': 0.00027887, 'params': 11224213, 'time_iter': 0.69073, 'accuracy': 0.9898, 'auc': 0.96233, 'ap': 0.54565}\nval: {'epoch': 49, 'time_epoch': 25.47803, 'loss': 0.04615588, 'lr': 0, 'params': 11224213, 'time_iter': 0.29626, 'accuracy': 0.98098, 'auc': 0.87523, 'ap': 0.29602}\ntest: {'epoch': 49, 'time_epoch': 27.18126, 'loss': 0.04901261, 'lr': 0, 'params': 11224213, 'time_iter': 0.31606, 'accuracy': 0.97977, 'auc': 0.86786, 'ap': 0.27824}\n> Epoch 49: took 539.5s (avg 541.2s) | Best so far: epoch 48\ttrain_loss: 0.0248 train_ap: 0.5397\tval_loss: 0.0463 val_ap: 0.2979\ttest_loss: 0.0492 test_ap: 0.2801\ntrain: {'epoch': 50, 'time_epoch': 470.4689, 'eta': 23237.00694, 'eta_hours': 6.45472, 'loss': 0.02436888, 'lr': 0.00027064, 'params': 11224213, 'time_iter': 0.68682, 'accuracy': 0.9899, 'auc': 0.96326, 'ap': 0.55261}\nval: {'epoch': 50, 'time_epoch': 25.55179, 'loss': 0.04679184, 'lr': 0, 'params': 11224213, 'time_iter': 0.29711, 'accuracy': 0.98094, 'auc': 0.87477, 'ap': 0.29431}\ntest: {'epoch': 50, 'time_epoch': 27.21263, 'loss': 0.04957152, 'lr': 0, 'params': 11224213, 'time_iter': 0.31643, 'accuracy': 0.97959, 'auc': 0.8686, 'ap': 0.28009}\n> Epoch 50: took 536.7s (avg 541.1s) | Best so far: epoch 48\ttrain_loss: 0.0248 train_ap: 0.5397\tval_loss: 0.0463 val_ap: 0.2979\ttest_loss: 0.0492 test_ap: 0.2801\ntrain: {'epoch': 51, 'time_epoch': 475.13183, 'eta': 22763.61972, 'eta_hours': 6.32323, 'loss': 0.02422456, 'lr': 0.0002624, 'params': 11224213, 'time_iter': 0.69362, 'accuracy': 0.98996, 'auc': 0.96434, 'ap': 0.55652}\nval: {'epoch': 51, 'time_epoch': 25.56265, 'loss': 0.04675906, 'lr': 0, 'params': 11224213, 'time_iter': 0.29724, 'accuracy': 0.98091, 'auc': 0.87438, 'ap': 0.29453}\ntest: {'epoch': 51, 'time_epoch': 27.17522, 'loss': 0.04960727, 'lr': 0, 'params': 11224213, 'time_iter': 0.31599, 'accuracy': 0.97967, 'auc': 0.86586, 'ap': 0.27869}\n> Epoch 51: took 541.4s (avg 541.1s) | Best so far: epoch 48\ttrain_loss: 0.0248 train_ap: 0.5397\tval_loss: 0.0463 val_ap: 0.2979\ttest_loss: 0.0492 test_ap: 0.2801\ntrain: {'epoch': 52, 'time_epoch': 471.47668, 'eta': 22286.92531, 'eta_hours': 6.19081, 'loss': 0.02400603, 'lr': 0.00025413, 'params': 11224213, 'time_iter': 0.68829, 'accuracy': 0.99005, 'auc': 0.96545, 'ap': 0.56175}\nval: {'epoch': 52, 'time_epoch': 25.52694, 'loss': 0.04695219, 'lr': 0, 'params': 11224213, 'time_iter': 0.29682, 'accuracy': 0.98092, 'auc': 0.87217, 'ap': 0.2926}\ntest: {'epoch': 52, 'time_epoch': 27.10994, 'loss': 0.04992567, 'lr': 0, 'params': 11224213, 'time_iter': 0.31523, 'accuracy': 0.97942, 'auc': 0.86457, 'ap': 0.27703}\n> Epoch 52: took 537.6s (avg 541.1s) | Best so far: epoch 48\ttrain_loss: 0.0248 train_ap: 0.5397\tval_loss: 0.0463 val_ap: 0.2979\ttest_loss: 0.0492 test_ap: 0.2801\ntrain: {'epoch': 53, 'time_epoch': 472.54872, 'eta': 21811.33737, 'eta_hours': 6.0587, 'loss': 0.02383347, 'lr': 0.00024587, 'params': 11224213, 'time_iter': 0.68985, 'accuracy': 0.99014, 'auc': 0.96632, 'ap': 0.56745}\nval: {'epoch': 53, 'time_epoch': 25.48186, 'loss': 0.04732859, 'lr': 0, 'params': 11224213, 'time_iter': 0.2963, 'accuracy': 0.9806, 'auc': 0.87257, 'ap': 0.29126}\ntest: {'epoch': 53, 'time_epoch': 27.09051, 'loss': 0.05024273, 'lr': 0, 'params': 11224213, 'time_iter': 0.31501, 'accuracy': 0.97928, 'auc': 0.86451, 'ap': 0.27649}\n> Epoch 53: took 538.7s (avg 541.0s) | Best so far: epoch 48\ttrain_loss: 0.0248 train_ap: 0.5397\tval_loss: 0.0463 val_ap: 0.2979\ttest_loss: 0.0492 test_ap: 0.2801\ntrain: {'epoch': 54, 'time_epoch': 472.61838, 'eta': 21335.91694, 'eta_hours': 5.92664, 'loss': 0.02361246, 'lr': 0.0002376, 'params': 11224213, 'time_iter': 0.68995, 'accuracy': 0.99021, 'auc': 0.96699, 'ap': 0.57178}\nval: {'epoch': 54, 'time_epoch': 25.51138, 'loss': 0.04730593, 'lr': 0, 'params': 11224213, 'time_iter': 0.29664, 'accuracy': 0.98079, 'auc': 0.87219, 'ap': 0.29069}\ntest: {'epoch': 54, 'time_epoch': 27.16453, 'loss': 0.05028185, 'lr': 0, 'params': 11224213, 'time_iter': 0.31587, 'accuracy': 0.97931, 'auc': 0.86342, 'ap': 0.27383}\n> Epoch 54: took 538.9s (avg 541.0s) | Best so far: epoch 48\ttrain_loss: 0.0248 train_ap: 0.5397\tval_loss: 0.0463 val_ap: 0.2979\ttest_loss: 0.0492 test_ap: 0.2801\ntrain: {'epoch': 55, 'time_epoch': 474.52336, 'eta': 20862.09335, 'eta_hours': 5.79503, 'loss': 0.02341734, 'lr': 0.00022936, 'params': 11224213, 'time_iter': 0.69273, 'accuracy': 0.99036, 'auc': 0.96823, 'ap': 0.57812}\nval: {'epoch': 55, 'time_epoch': 25.49056, 'loss': 0.0474866, 'lr': 0, 'params': 11224213, 'time_iter': 0.2964, 'accuracy': 0.98133, 'auc': 0.87107, 'ap': 0.29495}\ntest: {'epoch': 55, 'time_epoch': 27.04379, 'loss': 0.0504302, 'lr': 0, 'params': 11224213, 'time_iter': 0.31446, 'accuracy': 0.97974, 'auc': 0.86313, 'ap': 0.27755}\n> Epoch 55: took 540.6s (avg 541.0s) | Best so far: epoch 48\ttrain_loss: 0.0248 train_ap: 0.5397\tval_loss: 0.0463 val_ap: 0.2979\ttest_loss: 0.0492 test_ap: 0.2801\ntrain: {'epoch': 56, 'time_epoch': 472.12409, 'eta': 20386.43523, 'eta_hours': 5.6629, 'loss': 0.0232558, 'lr': 0.00022113, 'params': 11224213, 'time_iter': 0.68923, 'accuracy': 0.99035, 'auc': 0.96907, 'ap': 0.58307}\nval: {'epoch': 56, 'time_epoch': 25.44792, 'loss': 0.04778177, 'lr': 0, 'params': 11224213, 'time_iter': 0.29591, 'accuracy': 0.98071, 'auc': 0.87092, 'ap': 0.29809}\ntest: {'epoch': 56, 'time_epoch': 27.09939, 'loss': 0.05068942, 'lr': 0, 'params': 11224213, 'time_iter': 0.31511, 'accuracy': 0.97928, 'auc': 0.86306, 'ap': 0.27705}\n> Epoch 56: took 538.1s (avg 540.9s) | Best so far: epoch 56\ttrain_loss: 0.0233 train_ap: 0.5831\tval_loss: 0.0478 val_ap: 0.2981\ttest_loss: 0.0507 test_ap: 0.2771\ntrain: {'epoch': 57, 'time_epoch': 471.49486, 'eta': 19910.44332, 'eta_hours': 5.53068, 'loss': 0.02308638, 'lr': 0.00021293, 'params': 11224213, 'time_iter': 0.68831, 'accuracy': 0.99038, 'auc': 0.9697, 'ap': 0.58371}\nval: {'epoch': 57, 'time_epoch': 25.48701, 'loss': 0.04769707, 'lr': 0, 'params': 11224213, 'time_iter': 0.29636, 'accuracy': 0.98072, 'auc': 0.86941, 'ap': 0.2959}\ntest: {'epoch': 57, 'time_epoch': 27.17847, 'loss': 0.05061876, 'lr': 0, 'params': 11224213, 'time_iter': 0.31603, 'accuracy': 0.97928, 'auc': 0.86387, 'ap': 0.27795}\n> Epoch 57: took 537.7s (avg 540.9s) | Best so far: epoch 56\ttrain_loss: 0.0233 train_ap: 0.5831\tval_loss: 0.0478 val_ap: 0.2981\ttest_loss: 0.0507 test_ap: 0.2771\ntrain: {'epoch': 58, 'time_epoch': 471.59268, 'eta': 19434.67184, 'eta_hours': 5.39852, 'loss': 0.02290864, 'lr': 0.00020478, 'params': 11224213, 'time_iter': 0.68846, 'accuracy': 0.99051, 'auc': 0.97029, 'ap': 0.58925}\nval: {'epoch': 58, 'time_epoch': 27.40305, 'loss': 0.04818245, 'lr': 0, 'params': 11224213, 'time_iter': 0.31864, 'accuracy': 0.98058, 'auc': 0.86955, 'ap': 0.29123}\ntest: {'epoch': 58, 'time_epoch': 27.10103, 'loss': 0.05122745, 'lr': 0, 'params': 11224213, 'time_iter': 0.31513, 'accuracy': 0.97895, 'auc': 0.86306, 'ap': 0.27386}\n> Epoch 58: took 539.5s (avg 540.9s) | Best so far: epoch 56\ttrain_loss: 0.0233 train_ap: 0.5831\tval_loss: 0.0478 val_ap: 0.2981\ttest_loss: 0.0507 test_ap: 0.2771\ntrain: {'epoch': 59, 'time_epoch': 473.06992, 'eta': 18960.02448, 'eta_hours': 5.26667, 'loss': 0.02269841, 'lr': 0.00019668, 'params': 11224213, 'time_iter': 0.69061, 'accuracy': 0.99066, 'auc': 0.97115, 'ap': 0.59766}\nval: {'epoch': 59, 'time_epoch': 25.47293, 'loss': 0.04820102, 'lr': 0, 'params': 11224213, 'time_iter': 0.2962, 'accuracy': 0.9807, 'auc': 0.86985, 'ap': 0.2924}\ntest: {'epoch': 59, 'time_epoch': 27.13115, 'loss': 0.05130857, 'lr': 0, 'params': 11224213, 'time_iter': 0.31548, 'accuracy': 0.97905, 'auc': 0.86274, 'ap': 0.27579}\n> Epoch 59: took 539.2s (avg 540.8s) | Best so far: epoch 56\ttrain_loss: 0.0233 train_ap: 0.5831\tval_loss: 0.0478 val_ap: 0.2981\ttest_loss: 0.0507 test_ap: 0.2771\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}